%%[phdFutureWork
Plain level:

Completion into full Haskell + extensions, well documented, open source project, useful for teaching, useful/usable for third-party experimentation and contribution

Meta level:

Ruler:
\begin{Itemize}
\item
Strategy: combining known (programmer supplied, inferred/proven for sure) information with to be inferred information.
Abstraction then is: representation of mixture of known/unknown, API for incorporating info (like substitution).
Multipass algorithm(s) for (1) extracting known, (2) inferring unknown.
\item
Low-level: mapping to AG may use additional attr's, hidden from the type rule view
\end{Itemize}

Build system: nix, ...

Editing system: proxima, ...

%%]

%%[phdEvaluation
In the introduction (\secPageRef{sec-intro-holygrail}) we listed the following research goals:

\def\ehConclGoalA{EH, explanation and presentation}
\def\ehConclGoalB{EH, use of explicit and implicit type information}
\def\ehConclGoalC{Partitioning and complexity}
\def\ehConclGoalD{Consistency}
\def\ehConclGoalE{EH, formal properties}
\def\ehConclGoalF{EH, relation to Haskell}
\def\ehConclGoalG{AG experience}

\begin{Itemize}
\item \textbf{\ehConclGoalA:}
A compiler for Haskell, plus extensions, fully described for use in education and research.
\item \textbf{\ehConclGoalB:}
A better cooperation between (type) information specified by a (EH) programmer and (type) information
inferred by the system.
\item \textbf{\ehConclGoalC:}
A stepwise approach to explaining a compiler (or similar large programs).
\item \textbf{\ehConclGoalD:}
An organisation of material which guarantees consistency by construction (a priori),
instead of by comparison (a posteriori).
\end{Itemize}

We discuss these goals in the following sections.
Although not listed as a goal, we also look into the following list of topics,
as these constitute a large part of this thesis,
or are related otherwise:

\begin{Itemize}
\item \textbf{\ehConclGoalE:}
EH (formal) properties.
\item \textbf{\ehConclGoalF:}
EH and Haskell.
\item \textbf{\ehConclGoalG:}
Our experience with the AG system.
\end{Itemize}

A high level conclusion is that the key to an understandable and maintainable description of
a working compiler lies in the use of domain specific languages for dealing with the complexity of such a compiler.
Although traditionally many compilers are built without the use of tools like the AG system,
we feel that traditional techniques have reached their limit,
especially in dealing with mature implementations (like GHC (Glasgow Haskell Compiler))
of complex languages such as Haskell.
In the following sections we detail this conclusion to more specific aspects.

\subsection{\ehConclGoalA}
\Paragraph{Discussion}
Our approach to the explanation of EH shares the focus on implementation
with the approach taken by Jones \cite{jones00thih},
and shares the focus on the combination of formalities and implementation with Pierce
\cite{typing:types-prog-lang:pierce}.

However, there are also differences:
\begin{Itemize}
\item
Our focus is on the implementation, which is described by means of
various formalisms like type rules and Attribute Grammar notation.
\item
Our compilers are complete, in the sense that all aspects of a compiler are implemented.
For example, parsing, error reporting and code generation (in later EH versions) are included.
\item
We use type rules as a specification language for the implementation of a compiler;
type rules directly specify the implementation.
\end{Itemize}

Conciseness suffers when features are combined,
because
a short, to the point, presentation of an issue can only be constructed thanks to a simplification
of that issue,
and the reduction to its bare essentials.
On the other hand, a treatment of feature interaction usually suffers,
because of the simplifications required to understand a feature.

The problem is that the understanding of a language feature requires simplicity and isolation,
whereas the practical application of that feature requires its co-existence with other features:
these are contradictory constraints.
This is a problem that will never be resolved, because language features are to be used
as a solution to a real programmer's need (that is, a wider context);
a need that can only be understood if stripped of unrelated issues (that is, a restricted context).

Although the problem of understanding language features, both in isolation and context,
cannot be resolved,
we can make it more manageable:

\begin{Itemize}
\item
Specify and explain in terms of differences.
This is the approach taken by this thesis.
The AG system allows separation into attributes,
the |Ruler| system and fragment manipulation tool |Shuffle| specify in terms of views.
All tools provide a simple mechanism to combine the separated parts:
redefinition of values associated with an attribute (or a similar identification mechanism).
\item
If separated parts influence each other in combination,
then we specify their combination.
This is the place where an ordering of separate parts becomes important.
Our tools can be parameterized with an ordering (of views).
If separate parts do not influence each other, they can co-exist as (conceptually)
separate computations.
\item
Emphasize differences in their context.
Visual clues as to what has changed and what remained the same,
help to focus on the issue at hand,
while not forgetting its context.
The |Ruler| tool uses colors to accomplish this.
However, this has its limits, for example \figPageRef{rules3.I2.expr.eh4B2}
and \figPageRef{rules3.I2.decl.base} are likely to require a bit of study.
\item
Hide `irrelevant' material.
This thesis hides a fair amount of code, making it only indirectly available.
This is not a satisfactory solution, because the need to expose hidden information depends
on the reader's frame of reference, understanding and knowledge.
\item
Render non-linearly.
A paper version of an explanation is one-dimensional,
understanding often is non-linear and associative.
\end{Itemize}

Most of the abovementioned measures facilitate understanding.
However,
their realisation requires alternate ways of browsing through material,
supported by proper tool(s).
Existing mechanisms for abstraction and encapsulation, like module systems,
can help a great deal,
but in our experience a single feature often requires changes crossing traditional
encapsulation boundaries.
Crucial to the success of such a tool would be,
in our opinion,
the handling of feature isolation and feature combination.

\Paragraph{This thesis and future work}
We summarize our contribution and future work:

\begin{Itemize}
\item
This thesis: type system specification and implementation are jointly presented.
\item
Future work: the type system is algorithmically specified;
a declarative specification would benefit understanding but its relationship with the implementation would be less clear.
Our intended approach is to incorporate strategies into |Ruler| to be able to automate the translation from declarative to algorithmic specification
(see also \secRef{ehConclGoalD}).
\item
This thesis: a stepwise description and implementation.
\item
Future work: a step is described relative to a previous step;
splitting such a step into (1) an isolated feature description which is (2) explicitly combined with a particular step
would allow for further modularisation of features.
\end{Itemize}


\subsection{\ehConclGoalB}

\Paragraph{Discussion}
EH supports a flexible use of explicitly specified type information:
\begin{Itemize}
\item
Type signatures are exploited as much as possible by employing several strategies
for the propagation to the place (in the AST) where they needed.
\item
Type signatures may be partially specified.
This gives the ``all or nothing'' type specification requirement of Haskell
a more (user-)friendly face,
by a allowing a gradual shift between explicit and implicit.
\end{Itemize}

Furthermore, EH supports a flexible use of implicit parameters
by allowing explicit parameter passing for implicit parameters and
by allowing explicitly introduced program values to be used
implicitly for an implicit parameter.

Our approach to the interaction between explicit (higher-ranked) types and type inference is algorithmic.
This is similar to the algorithmic approach taken by R\'emy \cite{remy05sysf-tycont}, but also lacks a clear type theoretical characterisation of
the types which we can infer.
However, our informal claim is that the notion of ``touched by'' a quantified type provides an intuitive invariant of our algorithm.
In practice, we expect this to be sufficiently predictive for a programmer.

The central idea is to allow a programmer to gradually shift between full explicit type signature
specification to implicitly inferred types.
We can apply the `gradual shift' design starting point to other parts of the language as well.
For example, the instantiation of type variables with types now is done implicitly or indirectly.
We may well allow explicit notation a la System F, instead of the more indirect means by specifying
(separate) type signatures and/or type annotations.

We have also implemented explicit kind signatures,
but did not discuss these
in this thesis.

\Paragraph{This thesis and future work}
We summarize our contribution and future work:

\begin{Itemize}
\item
This thesis: allow a more flexible use and integration (into the type inferencer) of explicit (type) information.
\item
Future work: allow explicit specification of other implicit elements of the language (e.g. System F like notation).
\item
This thesis: exploitation of type annotations by means of (global) quantifier propagation.
\item
Future work: formalise quantifier propagation.
\end{Itemize}


\subsection{\ehConclGoalC}

\Paragraph{Discussion}
In this thesis we have chosen a particular partitioning of the full EH system into smaller and
ordered steps.
Although we feel that the chosen partitioning serves the explanation well,
others are possible and perhaps better within different contexts. 

\FigurePDF{t}{1}{ag-ast-asp}{AST and aspects}{fig-ag-ast-asp}

\figRef{fig-ag-ast-asp} intuitively illustrates where we have to make choices when we
partition.
We can partition along two dimensions, one for the language constructs (horizontally),
and one for the semantic aspects (vertically).
For example, in \chapterRef{ehc1} we fill in the light grey zone, ending in \chapterRef{ehc9}
by filling in the dark grey zone.
The horizontal dimension corresponds to AST extensions, the vertical dimension corresponds to
attributes and their (re)definition.
Each step describes one or more polygon shaped areas from the two-dimensional space.
Partitioning means selecting which squares are picked for extension and description.

%if False
We have chosen our partitioning based on the the following considerations:
\begin{Itemize}
\item
Start with requiring the programmer to be explicit and then relax this to allow type inference.
\item

\end{Itemize}
%endif

\figRef{fig-ag-ast-asp} also shows that complexity increases with the addition of features.
Not only do we have to describe new features, but we also have to
adapt the semantics of previous features as well,
hopefully minimizing their interdependencies.
Most likely the partitioning has the least interdependencies;
this is similar to proper module design.

Extending EH is not simply a matter of adding a new module.
An EH extension requires changes to the AST, attributes, semantics for attributes,
supporting Haskell functions and type rules.
It is unclear how to capture such a group of changes by a module-like mechanism.

\Paragraph{This thesis and future work}
We summarize our contribution and future work:

\begin{Itemize}
\item
This thesis: a partitioning of EH into steps, with tool support (either already existing (AG) or new (|Shuffle|, |Ruler|)).
\item
Future work: what is a good partitioning, and which mechanisms support the partitioning required for the description and construction of a compiler.
\end{Itemize}


\subsection{\ehConclGoalD}
\label{ehConclGoalD}

\Paragraph{Discussion}
Our approach to maintaining consistency between material used
for explanation and implementation,
is to avoid inconsistencies in the first place.
Inconsistencies are introduced when two (or more) artefacts represent derived
information, but are treated as independent, non-related pieces of information.
A change in such an artefact, or the artefact derived from, not accompanied by a corresponding (correct)
change in the remaining related artefacts, results in an inconsistency.

The dangers of inconsistency are therefore twofold:
\begin{Itemize}
\item
Changes are not propagated.
\item
Changes are made in related artefacts, but the changes are incorrect,
that is, inconsistent.
\end{Itemize}

This is a general problem.
In our particular case we consider it a problem for:

\begin{Itemize}
\item
Implementation (program code) and its incorporation into explanation.
\item
Formal representation of type rules, and their implementation (AG code).
\end{Itemize}

The first combination often is handled by ``copy and paste'' of code.
For a one-shot product this usually is not a large problem,
but for long-lived products it is.
A solution to this problem consists of mechanisms for sharing text fragments.
Both our tool |Shuffle| and similar literate programming tools
\cite{www05litprog}
have in common that a shared text fragment is associated with some identification
by which the shared text fragment can be included at different places.

The second combination suffers the fate of many specifications: they are forgotten after
being implemented.
This is also a general problem,
because non-automated translation between artefacts is involved:
human translations are seldom flawless.
This, of course, also holds for the artefacts themselves,
but translation at least can be automated,
provided a well-defined semantics of the artefacts and their relation.

Type rules and AG implementation correspond to such a degree that
it is possible to translate from a common type rule description to an
implementation and a visual rendering,
which can be included in text dealing with the formal aspects of type rules.
This is the responsibility of the |Ruler| tool,
which already in its first version turned out to be indispensable for the construction of this thesis.
Actually, the |Ruler| tool started out to reduce the amount of work involved
in typesetting type rules,
soon to be extended to generate AG code when we were confronted with the amount of work
required to keep those type rules consistent with their AG implementation.

The strong point of a tool like |Ruler| is that it acts,
like any compiler,
as a substitute for the proof
that implementation and type rules describe the same semantics.
And, like any compiler,
optimisations can be performed.
We foresee that a tool like |Ruler| can deal with aspects of the translation
from type rule specification to implementation,
some of which are done manually in this thesis:

\begin{Itemize}
\item
%%[[futWorkRulerDifferentStrategies
In \thispaper,
equational type rules are implemented by algorithmic ones,
which easily map to AG rules.
The transition from equation to algorithm involves a certain strategy.
In \thispaper\ we use HM inference, a greedy resolution of constraints.
Alternate strategies exist
\cite{heeren02hm-constr,heeren05class-direct};
|Ruler| (or similar tools) can provide abstractions of such strategies.
%%]
\item
Strategies can be user defined.
This thesis uses a representation of yet unknown information (type variable),
a representation of found information (constraints) and combinatorial behavior (application of
constraints as a substitution, type matching, AST top-down/bottom-up propagation).
These, and similar aspects, may well form the building blocks of strategies.
\item
This thesis often uses multiple passes over an AST.
For example, \chapterRef{ehc4B} describes a two-step inference for finding
polymorphic type information.
A more general purpose variant of this strategy would allow categorisation of
found information, where each pass would find information from a particular category.
\item
%%[[futWorkRulerSyntaxOrOtherDirected
|Ruler| exploits the syntax-directed nature of type rules.
This implies that the structure of an AST determines which rule has to be used.
The choice of the right rule may also depend on other conditions (than the structure of the AST),
or a choice may be non-deterministic.
The consequence of this observation is that |Ruler| has to deal with multiple levels of rules,
transformed into eachother,
with the lowest level corresponding to an AST based target language.
%%]
\item
|Ruler| uses AG as its target language.
In thesis, the rules for type matching (e.g. \figPageRef{rules3.K.match.onlyK}),
are also syntax-directed, but base their choice on two AST's (for types), instead of one.
This is a special, but useful, case of the previous item.
\item
|Ruler| extends views on type rules by adding new computations,
expressed in terms of holes in a judgement.
The final version combines all descriptions;
this easily becomes too complex
(\figPageRef{rules3.I2.expr.eh4B1}, \figPageRef{rules3.I2.decl.base})
to be helpful.
Instead,
mechanisms for separation of feature description (say, a feature module or trait)
and feature combination would better serve
understanding.
\item
%%[[futWorkRulerTargets
|Ruler| compiles to target languages (AG, \TeX), but does not prove anything about
the described rules.
A plugin architecture would allow the translation to different targets,
and in particular, into a description suitable for further use by theorem provers etc..
%%]
\end{Itemize}

\Paragraph{This thesis and future work}
We summarize our contribution and future work:

\begin{Itemize}
\item
This thesis: consistency between parts of this thesis and EH implementation by generating material from shared sources.
This is done for (1) type rules and their AG implementation (|Ruler|), and (2) all source code used for the construction of EH compilers and their inclusion in this thesis.
\item
Future work: other guarantees and derived information related to consistency,
for example ``explain before use'' of (program) identifiers occuring in this thesis, or (automatically generated) indices of used (type system) symbols.
\item
This thesis: automic generation of type rule implementation.
\item
Future work: high level (declarative) type rule specification with various mechanisms to automate the translation to an implementation for multiple targets.
\end{Itemize}


\subsection{\ehConclGoalE}
\label{ehConclGoalE}
\Paragraph{Discussion}
In this thesis we do not make claims about the usual formal properties of a type system:
soundness, completeness, and principality (of type inference).
However, we still can make the following observations:
\begin{Itemize}
\item
EH3 implements Hindley-Milner type inference,
which is standard \cite{hindley69princ-type,damas82principal-type},
combined with the use of explicit type signatures,
which also has been explored \cite{pierce00local-type-inference,odersky01col-loc-infer}.
\item
From EH4 onwards we allow the same expressiveness as System F by means of type annotations which
allow quantifiers at arbitrary positions (in the type annotation).
The key question is what kind of types can be inferred when type annotations are omitted.
We informally argue the following:
\begin{Itemize}
\item
We rely on (classical) Hindley-Milner type inference,
hence we inherit its properties in case no type signatures are specified.
\item
We propagate known type signatures (specified or generalised by means of HM inference) to
wherever these signatures are needed,
so these signatures can be used as if specified by type annotations.
\item
We allow polymorphic types to propagate impredicatively,
but we do not invent polymorphism other than via HM generalisation.
Propagation is based on the relatively simple notion of
``touched by another polymorphic type'' (\chapterRef{ehc4B})).
This notion can be seen as a characterisation of what we can infer.
\end{Itemize}
\end{Itemize}

These observations are formulated more precisely in \secRef{ehc4B-design} (\thRef{eh4-th-hm-sound} and \thRef{eh4-th-sysf-sound}).

\Paragraph{This thesis and future work}
We summarize our contribution and future work:

\begin{Itemize}
\item
This thesis: we have engineered (but not proven) a type system for exploiting type annotations.
\item
Future work: further investigate the formal properties of our type inference,
in particular the quantifier propagation.
\end{Itemize}

%if False
\Paragraph{EH: future work}
In the near future we intend to construct and/or investigate the following:

\begin{Itemize}
\item
Co- and contravariance analysis, along the lines of Steffen \cite{steffen97polar-abs,steffen97polar-phd}.
This is required for |<=| to determine the direction of |<=| for components of a data type.
\item
The integration of global quantifier propagation into all compilers.
In particular the presence of implicit parameters adds complexity.
\end{Itemize}
%endif

\subsection{\ehConclGoalF}

\Paragraph{Discussion}
EH includes essential features of Haskell;
EH is closer to the core language internally used by GHC \cite{tolmach01ghc-core,www04ghc},
in that it allows System F expressiveness \cite{girard72system-f,reynolds74type-struct-sysF}.
EH also resembles the core
used for the description of the static semantics of Haskell \cite{faxen02semantics-haskell}
or used by the language definition \cite{peytonjones03has98-rev-rep},
in the assumption that syntactic desugaring and dependency analysis (of identifiers) has been done.

The strong point, however, of EH, is the lifting of restrictions with respect to explicitly specified
information and implicitly inferred information.
With a Haskell frontend for
syntactic sugar, and a module mechanism, these strong points are
easily made available as
Haskell with extensions.

\Paragraph{This thesis and future work}
We summarize our contribution and future work:

\begin{Itemize}
\item
This thesis: Haskell extensions on top of EH, a simplified Haskell, or, in the form of acronym: \textsf{Haskell-{}-++}.
\item
Future work: make EH available via a Haskell frontend.
This requires additional preprocessing with respect to dependency analysis and desugaring,
a module system and completion of code generation (and much more).
\end{Itemize}

\subsection{\ehConclGoalG}

\Paragraph{Discussion}
The AG system is heavily used for the description of all EH implementations.
For the description of the EH compilers, the following features of the AG system proved to be essential:

\begin{Itemize}
\item
The AG notation (in essence) offers a domain specific language for the specification of tree based computations (catamorphisms).
\item
The AG system offers mechanisms to split a description into smaller fragments, and later combine those fragments.
\item
The AG system allows focussing on the places where something unusual needs to be done, similar to other approaches
\cite{laemmel03boilerplate}.
In particular, copy rules allow us to forget about a large amount of plumbing.
\item
A collection of attribute computations can be wrapped into a Haskell function.
Although the AG system does not provide notation for higher-order AG \cite{swierstra91hag},
this mechanism can be used to simulate higher-order AG's as well as use AG for describing transformations on an AST.
\end{Itemize}

Although the AG system is a simple system it turned out to be a surprisingly useful system,
of which some of the features found their way into the |Ruler| system as well.
However, the simplicity of our AG system also has its drawbacks:

\begin{Itemize}
\item
Type checking is delegated to AG's target language: Haskell.
As a consequence errors are difficult to read because AG's translation is exposed.
\item
Performance is expected to give problems for large systems.
This seems to be primarily caused by the simple translation scheme in which all attributes together live in a tuple just until the program
completes.
This inhibits garbage collection of intermediate attributes that are no longer required.
It also stops GHC from performing optimizations.
Work to improve this is in progress, based on AG dependency analysis \cite{saraiva99phd-funcimpl-ag}.
\end{Itemize}

Attribute grammar systems have been around for a while \cite{knuth68ag}.
We refer to Parigot's AG www page \cite{parigot98ag-www-home} and Saraiva's work \cite{saraiva99phd-funcimpl-ag}
for further reading.
Alternative systems are FNC-2 \cite{parigot98ag-fnc2},
Eli \cite{www05eli,gray92eli},
and JastAdd (Java based, with some rewriting like Stratego \cite{visser05stratego-www,visser01stratego-system05})

%if False
---------------------------

Before we look further how the hunt for these goals did work out,
we ask ourselves a `what if' question:
could this thesis have been constructed without the AG system, |Ruler|,
and |Shuffle|?
In our opinion: No!
Not using these tools would have had the following consequences:

\begin{Itemize}
\item
AST definition would have been defined in terms of (Haskell) data types;
the logistics of tree traversals would have to be handcoded.
\item
Separation into steps, with duplication of shared material,
would have led to a nightmare of inconsistencies.
\item
A description without separation into steps would have been very difficult to explain;
separation also helps the developer.
\item
Type rules would have been typeset in \LaTeX,
already unpleasant by itself,
without any guarantee about being consistent
with the implementation.
\end{Itemize}

Traditionally, many compilers are built without the use of attribute grammar based tools,
or any of the other tools used by this thesis.
So, indeed, why do we need such tools?
In our opinion Haskell is one of the most complex languages available today.
The GHC (Glasgow Haskell Compiler),
the maturest Haskell implementation,
is complex.
The extension with even more features will
push the edge of the envelope even further,
necessitating the use of additional tools for managing complexity,
like those used in this thesis.
It may well be that until now programming languages were too simple to warrant
the use of such tools.

The key to an understandable and maintainable description of
a working compiler lies in the use of domain specific languages for these problems.
In the remainder of this chapter we will discuss the goals we set out for,
discuss how the used domain specific languages AG and |Ruler| did help,
and propose directions for future work.
%endif





%%]

%%[afpConclusion
\Paragraph{AG system}
At the start of \thispaper\ we did make a claim that our ``describe separately'' approach contributes
to a better understood implementation of a compiler, in particular a Haskell compiler.
Is this true?
We feel that this is the case, and thus the benefits outweigh the drawbacks, based on some observations made during this project:

The AG system provides mechanisms to split a description into smaller fragments, combine those fragments and redefine part
of those fragments.
An additional fragment management system did allow us to do the same with Haskell fragments.
Both are essential in the sense that the simultaneous `existence' of a sequence of compiler versions,
all in working order when compiled, with all aspects described with the least amount of duplication,
presentable in a consistent form in \thispaper\ could not have been achieved without these mechanisms and supporting tools.

The AG system allows focusing on the places where something unusual needs to be done, similar to other approaches
\cite{laemmel03boilerplate}.
In particular, copy rules allow us to forget about a large amount of plumbing.

The complexity of the language Haskell, its semantics, and the interaction between features is not reduced.
However, it becomes manageable and explainable when divided into small fragments.
Features which are indeed independent can also be described independently of each other by different attributes.
Features which evolve through different versions, like the type system, can also be described separately,
but can still be looked upon as a group of fragments.
This makes the variation in the solutions explicit and hence increases the understanding of what really makes the difference
between two subsequent versions.

On the downside, fragments for one aspect but for different compiler versions end up in different sections of \thispaper.
This makes their understanding more difficult because one now has to jump between pages.
This is a consequence of the multiple dimensions we describe: variation in language elements (new AST), additional semantics (new attributes) and
variation in the implementation.
Paper, on the other hand, provides by definition a linear, one dimensional rendering of this multidimensional view.
We can only expect this to be remedied by the use of proper tool support (like a fragment editor or browser).
On paper, proper cross referencing, colors, indexing or accumulative merging of text are most likely to be helpful.

The AG system, though in its simplicity surprisingly usable and helpful, could be improved in many areas.
For example, no type checking related to Haskell code for attribute definitions is performed,
nor will the generated Haskell code when compiled by a Haskell compiler produce sensible error messages in terms of the
original AG code.
The AG system also lacks features necessary for programming in the large.
For example, all attributes for a node live in a global namespace for that node instead of being packaged in some form of module.

Performance is expected to give problems for large systems.
This seems to be primarily caused by the simple translation scheme in which all attributes together live in a tuple just until the program
completes.
This inhibits garbage collection of intermediate attributes that are no longer required.
It also stops GHC from performing optimizations;
informal experimentation with a large AG program resulted in GHC taking approximately 10 times more time with optimization flags on.
The resulting program only ran approximately 15\% faster.
The next version of the AG system will be improved in this area \cite{saraiva99phd-funcimpl-ag}.

\Paragraph{AG vs Haskell}
Is the AG system a better way to do Haskell programming? In general, no, but for Haskell programs
which can be described by a catamorphism the answer is yes (see also \secRef{ag-primer}).
In general, if the choices made by a function are mainly driven by some datastructure,
it is likely that this datastructure can be described by an AST and the function can be described by the AG's attribution.
This is the case for an abstract syntax tree or analysis of a single type.
It is not the case for a function like |fitsIn| (\secPageRef{EHTyFitsIn.1.fitsIn.Base}) in which
decisions are made based on the combination of two (instead of just one) type.

\Paragraph{About \thispaper\, EH and its code}
The linear presentation of code and explanation might suggest that this is also
the order in which the code and \thispaper\ came into existence.
This is not the case.
A starting point was created by programming a final version (at that time EH version 6, not included in \thispaper).
From this version the earlier versions were constructed.
After that, later versions were added.
However, these later versions usually needed some tweaking of earlier versions.
The consequence of this approach is that the rationale for design decisions in earlier versions become clear only
in later versions.
For example, an attribute is introduced only so later versions only need to redefine the rule for this single attribute.
However, the initial rule for such an attribute often just is the value of another attribute.
At such a place the reader is left wondering.
This problem could be remedied by completely redefining larger program fragments.
This in turn decreases code reuse.
Reuse, that is, sharing of common code turned out to be beneficial for the development process as the
use of different contexts provides more opportunities to test for correctness.
No conclusion is attached to this observation, other than being another example of the tension between clarity
of explanation and the logistics of compiler code management.

\Paragraph{Combining theory and practice}
Others have described type systems in a practical setting as well.
For example, Jones \cite{jones00thih} describes the core of Haskell98 by a monadic style type inferencer.
Pierce \cite{typing:types-prog-lang:pierce} explains type theory and provides many small implementations performing
(mainly) type checking for the described type systems in his book.
On the other hand, only recently the static semantics of Haskell has been described formally \cite{faxen02semantics-haskell}.
Extensions to Haskell usually are formally described but once they find their way into a production compiler the interaction
with other parts of Haskell is left in the open or is at best described in the manual.

The conclusion of these observations might be that a combined description of a language, its semantics,
its formal analysis (like the type system),
and its implementation is not feasible.
Whatever the cause of this is, certainly one contributing factor is the sheer size of all these
aspects in combination.
We feel that our approach contributes towards a completer description of Haskell,
or any other language if described by the AG system.
Our angle of approach is to keep the implementation and its explanation consistent and understandable
at the same time.
However, this document clearly is not complete either.
Formal aspects are present, let alone a proof that the implementation is sound and complete
with respect to the formal semantics.
Of course one may wonder if this is at all possible; in that case our approach may well
be a feasible second best way of describing a compiler implementation.

\Paragraph{EH vs Haskell}
The claim of our title also is that we provide an implementation of Haskell,
thereby implying recent versions of Haskell, or at least Haskell98.
However, \thispaper\ does not include the description of (e.g.) a class system;
the full version of EH however does.
%%]

%%[thesisContrib
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

