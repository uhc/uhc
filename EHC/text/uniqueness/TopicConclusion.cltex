%%[main

\chapter{Conclusion}
\label{chapt.Conclusion}

Chapter~\ref{chapt.NoBindings} defines a constraint-based type system for uniqueness typing. The language of
this chapter does not even come close to that of a real function programming language, but provides sufficient
features to show which constraints are generated and how they are interpreted to check a uniqueness typing
of a program. Chapter~\ref{chapt.Polyvariant} extends on this approach by showing how uniqueness \emph{inference}
can be performed on the constraints. In the chapters that follow, we gradually showed how language features
interact with the type system. We did not shy away from implementation-level problems (such as supporting
data types) and present problems that we encountered and discuss solutions. Finally, in Chapter~\ref{chapt.DataTypes}
we ended up with a uniqueness type inferencer that supports sufficient language features to use in the
front-end of a compiler. Chapter~\ref{chap.InspectingResults} shows that the analysis of the inferencer in the front-end
of the compiler, is important to communicate demands and results with the programmer.

Key advantages of the type system are that it is constraint-based, fully polyvariant, accurate and scalable. It is constraint-based
so the implementation is split up in multiple phases, separating concerns and isolating changes. It is fully polyvariant (Chapter~\ref{chapt.Polyvariant}),
meaning that each use-site can have different uniqueness types (if the constraints allow it), also in the presence of
recursion (Chapter~\ref{chapt.Recursion}). By means of special |beta| annotations, we accomplish a form of higher-rankness in
uniqueness types. We show in Section~\ref{sect.FullRecursion} that polyvariant recursion for uniqueness types is decidable, if
polymorphic recursion is taken care of. The type system is accurate in the sense that it allows components of a value to
be typed differently from the spine of a value, such that functions that only touch the spine, do not influence the
components. This is our main difference with other approaches, as it requires us to assume that the presence of some
identifier does not guarantee that the identifier is actually used. Finally, with scalable, we mean that there are several
possibilities to improve the performance drastically by giving up some accuracy. Performance is scaled by influencing two
factors, the number of annotations (Section~\ref{sect.Defaulting}, Section~\ref{sect.ExposedAnnotations}) and the number of instantiation-operations on the constraint graph (Section~\ref{sect.FullRecursion}).

Unfortunately, there are some disadvantages. During prototyping, we discovered that despite our ways to
isolate changes, it still complicates the compiler from an engineering point of view. Since uniqueness
types can occur everywhere, any change to the compiler requires a verification that the uniqueness types
are preserved by the change. Since Haskell does not rely on uniqueness typing in order to perform IO~\cite{DBLP:journals/jfp/Jones03t}, and
the rise of smart data-type implementations, such as versioned arrays (@Data.Array.Diff@), it remains a
question if the additional complications of uniqueness typing are worth it.

\section{Related work}

  Philip Wadler~\cite{wadler90linear}, presented a linear type system for functional languages. This system
  assumes that each occurrence of an identifier occurs sequential and is always used. As a consequence, the
  type system is virtually the same as a conventional type system, except with restrictions on the
  environment.
  
  The Clean compiler~\cite{barendsen96uniqueness} analyzes the abstract syntax tree how different uses of an identifier occur, and
  marks a type of an identifier shared if it occurs more than once in sequence. The type system then
  propagates these values accordingly. The type system of Clean gives a polyvariant uniqueness typing. There
  is support for data types, but data types cannot be parameterized over uniqueness annotations occurring in
  types of a data-type definition (Section~\ref{sect.ExposedAnnotations}).
  
  Clean has a restriction that if a component of a value is unique, then the spine must be unique as well. Or
  in other words, that the components are assumed to be used at least as much as the spine. This ensures that
  Clean does not have the graph-duplication problems for instantiation in the presence of polymorphism
  (Section~\ref{sect.ConstrDup}). A consequence is that in curry-style function application, the order in which
  the parameters are defined matters. There can be no unique parameter to the left of a shared parameter.
  
  Another restriction is that Clean assumes that each occurrence of an identifier means that it is also
  used. The elements of the list |xs| cannot be marked as unique in Clean, despite the fact that the elements are
  used only once, since the |length| function does not touch the elements:

%%[[wrap=code
  (\xs -> map (+ length (Sub(xs)(1))) (Sub(xs)(2))) [1,2,3,4]
%%]

  Our analysis discovers that length does not use values of |xs|; it derives |(Sub(xs)(1)) :: (Sup(List)(Card(1)(*))) (Sup(Int)(Card(0)(1)))|
  and |(Sub(xs)(2)) :: (Sup(List)(Card(1)(*))) (Sup(Int)(Card(1)(1)))|, which results in a linear type for the
  integers in the list.

  A consequence of these restrictions is that the type system of Clean performs less work. Uniqueness
  signatures are rather trivial to implement in the system of Clean, as well as the interaction with a
  class system, contrary to signatures in our system (Section~\ref{sect.Helping}) and (Section~\ref{sect.OverloadedAdvanced}).
  
  In a recent PhD thesis, Keith Wansbrough~\cite{Wansbrough:PhDThesis}, gives an overview of different usability-analysis,
  and discusses an implementation of usage-analysis in the back-end of the GHC compiler. His implementation
  is polyvariant, and uses a similar way of dealing with data types as our exposed annotations in Section~\ref{sect.ExposedAnnotations}.
  However, this type system also makes the assumption that spine of a value is as unique as the elements.

\section{Complexity comparison to Clean}

  Using information about values that are not used, has consequences. Uniqueness typing for Clean, for instance, assumes that all values are initially
  used exactly once. The reference counting phase then amounts to marking identifiers unique if there is only one occurrence in the abstract syntax
  tree or only parallel occurrences, and shared if that is not the case. The uniqueness propagation phase then ensures that the values that were
  incorrectly assumed to be used at most once (these are the values that are passed as argument to a shared parameter of a function), get a shared
  type and propagates this sharing to all places where the value occurs. So, the Clean compiler performs a local reference count analysis and a global
  uniqueness propagation.

  We do not make the assumption during reference counting that values are used at most once. We just assume that a value is used
  |delta| times, where |delta| is determined at another time. This means that in situations where there are multiple (sequential) occurrences of an
  identifier, we do not yet know if the identifier is shared because the occurrences may be unused. So, our reference count analysis is a
  global analysis and we pay for that in complexity. Consequences are that we need constraints that combine multiple occurrences (the
  aggregation constraint), which makes constraint sets complicated. For example, the notion of entailment is difficult to express. As a result,
  dealing with constraint duplication in the presence of polyvariance is hard (graphs with hyper edges, graph reduction). On the other hand,
  our uniqueness propagation phase is rather trivial, because all the work is done by the reference counting phase.

\section{Future work}
\label{sect.FutureWork}

  There are fourth directions of future work. In the first place, the lessons learned by the prototype and this
  thesis, can be used to create a faster performing prototype, where more care is taken to speed up the
  graph algorithms, and has more options to choose between accuracy and performance.

  Secondly, the relation between uniqueness typing and overloading requires more thorough investigation. For example, is it
  possible to implement overloading resolving that take cardinality variables in account.

  Thirdly, the question is if the interaction with the user can be improved, by means of signatures,
  the identification of problem sites, or other mechanisms to inspect the result of the analysis.
  
  Finally, there is the topic of using the uniqueness types to improve code. At the time of writing, a
  code generator is added to EHC~\cite{ExceptionalGRIN}, and other approaches of code generation for EHC
  are under investigation. There is already a lot of work done in this field, but it remains a question which
  of that work is applicable for EHC, and how to treat the problem of polyvariance.
  
  So, this master's thesis opens up several topics of research for other master's thesis projects.

%%]
