%%[main

\chapter{Polyvariant analysis with monomorphic bindings}
\label{chapt.Polyvariant}

In the previous chapter, we generate constraints from an annotated program, and call the annotations valid if and only if
the constraints are satisfied. In this chapter, we take a closer look at the constraints. We take a validly typed, but not
annotated, program and decorate the type constructors with \emph{cardinality variables} |delta| instead of concrete
annotations. Then we generate the set of constraints, which now contain variables instead of concrete values. A solution
is an assignment of concrete cardinality values to the annotations such that the constraints are satisfied. The constraints thus specify
a whole family of solutions. In this chapter, we discuss a technique that infers the \emph{minimal} solution to the
constraints (Section~\ref{sect.TheMinimalSolution}).

To make this chapter slightly more challenging, we add a monomorphic and non-recursive |let| to the expression language defined
in Chapter (Section~\ref{chapt.NoBindings}). With the existence of cardinality variables, we turn the analysis into a polyvariant analysis, which
means that we infer a most general uniqueness typing for a binding, and allow different instantiations for each use of an
identifier.

This chapter is organized as follows. We illustrate the concept of cardinality inference by an example.
Then we go into the concept of the type inference process (Section~\ref{sect.TheInferencer}). We follow this up with a discussion about
the addition of bindings to the expression language, and consequently the concept of binding groups and
instantiation(Section~\ref{sect.Instantiation}). The constraints play an important role here, but we go to yet another representation:
the constraints are rewritten to a graph-representation on which we can perform graph reduction (Section~\ref{sect.Rewriting}) to
simplify the constraints.


\section{Example}
\label{sect.ExamplePoly}

A type now only has cardinality variables as annotations. Upper and lower-bound cardinality annotations do not exist anymore,
but are used internally by the inferencer. The following example demonstrates some annotated types and constraint
sets:

%%[[wrap=code
f  ::  forall (Delta(1))                      .  (Annot(Int)(1))
g  ::  forall (Delta(1))                      .  (Annot(Int)(1)), ((Card(onemin)(*))) =>= (Delta(1))
h  ::  forall (Delta(2))                      .  (Annot(Int)(2)) (Annot(->)(2)) (Annot(Int)(2))
j  ::  forall (Delta(3))(Delta(4))(Delta(5))  .  (Annot(Int)(4)) (Annot(->)(3)) (Annot(Int)(5)), (Delta(5)) =>= (Delta(4)), (Delta(5)) (Sub(=>=)(s)) (Delta(3))
%%]

All cardinality variables are universally quantified, but are constrained by a constraint set. For example, |f| represents
a value with a type that has an unconstrained annotation |(Delta(1))|. This annotation can be instantiated with any consistent
cardinality value. The annotation on the type of |g| can only be |(Card(onemin)(*))|, or |(Card(*)(*))|, since the
constraint disallows other combinations (Section~\ref{Sect.CheckingConstraints}). The type of |h| corresponds to a function
that may have any of the five cardinality values as long as the function, the argument, and the result type have the same cardinality value.
The three annotations on the type of |j| can have any cardinality value as long as the two coercions are satisfied.

We annotate a program with cardinality variables, perform the constraint gathering of the previous chapter, and
perform cardinality inference on the constraints. We remark that a cardinality annotation is a cardinality
variable (i.e. |delta|), and a concrete cardinality annotation is a cardinality value (i.e. |(Card(1)(*))|).

The first step in this process is to take a typed program and annotate it with fresh annotations:

%%[[wrap=code
(\x -> x) 3
((\(x :: Int) -> x :: Int) :: Int -> Int) (3 :: Int) :: Int
%%]

Each type constructor in the types is annotated with a fresh cardinality variable (the |x| parameter gets its annotation from
the type of the function).

%%[[wrap=code
(\x -> x) 3
((\(x :: (Annot(Int)(1))) -> x :: (Annot(Int)(2))) :: (Annot(Int)(3)) (Annot(->)(4)) (Annot(Int)(5))) (3 :: (Annot(Int)(6))) :: (Annot(Int)(7))
%%]

Constraint gathering of Section~\ref{Sect.ConstraintGathering} results in the following constraints:

%%[[wrap=code
(Delta(0)) (Sub(=>=)(s)) (Delta(7))           -- root of the expression
(Delta(7)) =>= (Delta(5))                     -- function application to function result
(Delta(7)) (Sub(=>=)(s)) (Delta(4))           -- function application to function spine
(Delta(3)) =>= (Delta(6))                     -- function parameter to function type
(Delta(5)) =>= (Delta(2))                     -- function result to function body
(Delta(2)) <= (Delta(1))                      -- aggregation of use-sites of |x|
(Delta(1)) =>= (Delta(3))                     -- parameter to function
%%]

With a fixed cardinality variable |(Delta(0))| of which we set the initial substitution for the upper bound to |0| and the
lower bound to |0|.

These constraints specify a whole family of solutions. A solution is a substitution that maps each cardinality
variable to a cardinality value and satisfies the constraints. The substitution that maps each
cardinality variable to |(Card(*)(*))| always satisfies the constraint. But in this case, a substitution that maps
each cardinality variable to |(Card(1)(1))|, |(Card(1)(oneplus))|, or |(Card(oneplus)(oneplus))|, also satisfies
the constraints. Annotate the program with corresponding upper and lower bound annotations and apply the constraint
checking of the previous chapter if in doubt.

The above constraints are passed to the inferencer (we call it the constraint solver), which infers the least solution to
the constraints. With a least solution we mean a solution that is as specific as possible for each cardinality annotation
(see the ordering |leqS| in Section~\ref{sect.TpAnnConstr}). The |Usage| values |1| and
|0| are the most specific (and mutual exclusive), the annotations |onemin| and |oneplus| annotations are in between (also
mutual exclusive) and |*| is the least specific |Usage| value. There is exactly one least solution to the constraints (see
Section~\ref{sect.TheMinimalSolution} for a proof). Keep in mind that this solution is still a conservative approximation
of the actual usage of a value.

We obtain the solution as follows. Based upon the constraints, a lower and an upper bound is inferred for each cardinality
annotation. After the lower and upper bounds are inferred, we choose the most specific |Usage| value for both |Usage|
values of the cardinality annotations. We then relax (make them less specific according to |leqS|) the left-hand side and
right-hand side of the cardinality values independently until they satisfy the constraints. A relaxation of the
left-hand side possibly leads to a relaxation of the right-hand side to keep the cardinality value consistent
(see |leqB| in Section~\ref{sect.partialorderings}). Satisfying one constraint by relaxing one of the |Usage| values of
a cardinality annotation, can cause a need for relaxation of the cardinalities of another constraint. This process is repeated
until all cardinality values satisfies the constraints. This process terminates since we cannot relax |Usage| values
beyond |*|.

The upper and lower bound are obtained in a similar way. An initial substitution is created that maps the lower bound of
each cardinality variable to |1|. Each coercion constraint that is not satisfied has a higher lower bound in the substitution
for the right-hand side than the left-hand side. The constraint becomes satisfied by lowering the lower bound of the
right-hand side to the lower-bound of the left-hand side. An aggregation constraint that is not satisfied, becomes satisfied
by changing the right-hand side to the result of the computation of the left-hand side. This process is repeated until
all constraints are satisfied. The same process is performed for the right-hand side, except that we start with the
value |0|. Performing this approach on the constraints of the example results in lower-bound values of all |1| (none
of the constraints are satisfied to start with), and upper bound values of |1| (some work had to be done for this one).

The most specific cardinality value that fits (Figure~\ref{code.FitsInBounds}) an upper bound and lower bound of |1|, is
the value |1| (linear). A cardinality of |1| satisfies all the above constraints. The substitution |S (Delta(i)) = 1|, with |1 <= i <= 7|
is a least solution to the constraints of the above example.


\section{Cardinality variables and type rules}
\label{sect.UniquenessVariables}

In contrast to the previous chapter where the type constructors are annotated by a triple of annotations, we now only
annotate the type constructors with a fresh cardinality variable. The example
in Section~\ref{sect.ExamplePoly} showed several types annotated with such cardinality variables.

Consider the following expression with types of every identifier and subexpression:

%%[[wrap=code
let  (f :: Int -> Int) = (\x :: Int -> (x :: Int) + (1 :: Int) :: Int) :: Int -> Int
in   (f :: Int -> Int) (1 :: Int) :: Int
%%]

There are several ways in which we can annotate a program. One annotation strategy is to annotate all of these types
with fresh cardinality variables. For example:

%%[[wrap=code
let  (f :: (Annot(Int)(1)) (Annot(->)(2)) (Annot(Int)(3))) = (\(x :: (Annot(Int)(4))) -> (x :: (Annot(Int)(5))) + (1 :: (Annot(Int)(6))) :: (Annot(Int)(7))) :: (Annot(Int)(8)) (Annot(->)(9)) (Annot(Int)(10))
in   (f :: (Annot(Int)(11)) (Annot(->)(12)) (Annot(Int)(13))) (1 :: (Annot(Int)(14))) :: (Annot(Int)(15))
%%]

As demonstrated in Chapter~\ref{chapt.NoBindings}, the constraints between annotations make sure that the cardinality
variables are properly connected to each other.

The above approach allows cardinality coercions everywhere in the abstract syntax tree. A cardinality coercion allows
a relaxation of an established cardinality result to a weaker variant. For example, we can coerce a value with a total
|usage| value of |onemin| to a value with a total usage of |*| by forgetting that the value is used at most once in total,
if optimisations do not conflict with this coercion and possibly requires a coercion to be applied to the value at the
moment of relaxation. There is some redundancy between annotations on some types. For example, a parameter of a function can use the cardinality
variables of the enclosing function for its annotated type. It does not need fresh cardinality variables. Since we allow a
coercion from argument to parameter, there is no need for a coercion from parameter to use-site of the parameter in the body
of the function. Similarly, for a function application like such as |f 3|, it is possible to coerce the result of |f| at the
function application. However, the result of |f 3| can be used at three places: as argument of a function application, as
body of a lambda, or as right-hand side of a |let| binding (later in this chapter). In case it is used as argument to another function application (i.e. |g (f 3)|), then the function
application of |g| already allows an coercion. In case it is used as the body of lambda, we already allow a coercion from the type of
the body to the type of the result of the lambda. Finally, we also already allow a coercion of the right-hand side of a
|let|. So, a coercion of the result of a function at a function application is redundant. Coercions possibly lead to additional
code depending on the code generator. We therefore want to limit the number
of coercions that can be inserted, and restrict it to clearly defined places. Therefore, we restrict coercion to the three
places mentioned above. This has no influence on the quality of the inference process. It leads to
fewer coercion constraints and fewer annotations, which is beneficial for the performance of the compiler and possibly the
performance of the program itself.

Consider the difference between the following example and the previous example:

%%[[wrap=code
let  (f :: (Annot(Int)(1)) (Annot(->)(2)) (Annot(Int)(3))) = (\(x :: (Annot(Int)(1))) -> (x :: (Annot(Int)(4))) + (1 :: (Annot(Int)(5))) :: (Annot(Int)(6))) :: (Annot(Int)(1)) (Annot(->)(2)) (Annot(Int)(3))
in   (f :: (Annot(Int)(7)) (Annot(->)(8)) (Annot(Int)(9))) (1 :: (Annot(Int)(10))) :: (Annot(Int)(11))
%%]

With this strategy, fewer different cardinality annotations are used compared with the annotation strategy above.
The difference is that we allow coercions only at restricted places. The result is less constraints and less
coercion variables, which results in better performance of the inferencer. To scale up to typing big real-life
programs, a cardinality inferencer can decide to reduce accuracy for certain unimportant pieces of a program,
by limiting the coercions and reusing cardinality variables even more, to reduce the number of
annotations severely, thus preventing choking on the number of annotations (Section~\ref{sect.FutureWork}).
Limiting coercions can also positively impact runtime performance, depending on the additional code that is inserted for it,
and what the code-improvements are, up to the point of the coercion.

Take a look at the example again. The types at the use-site of an identifier are freshly annotated, but this is unrelated
to coercions. The reason for freshly annotating the use-sites of an identifier is because we treat each use of an
identifier independently, and combine the results at the definition site with the aggregation constraint. For similar
reasons, integer expressions and the spine of a lambda are freshly annotated: we can produce integers and functions
for any cardinality annotation.

Figure~\ref{RulerUniquenessExamples.UX.expr.base} lists the type rules for the uniqueness type system with cardinality inference. The constraint
generation remained virtually the same (see Section~\ref{Sect.HigherGathering}). The notable changes are the places where identifiers are freshly
annotated, and a reduction in the number of coercion constraints. Furthermore, we take a copy of the constraint set of an identifier with the
annotations of the definition site replaced by annotations of the use-site (the relation |rename| in the type rules), which is needed to ensure
that we still know which types we are talking about.

\rulerCmdUse{RulerUniquenessExamples.UX.expr.base}

The generated constraints are passed to the inferencer, which results in annotated types for each expression,
and a substitution. The substitution maps each cardinality variable to a cardinality value.


\section{The inferencer}
\label{sect.TheInferencer}

The question that now remains is: how to obtain the substitution that assigns a cardinality value to a cardinality
variable? Since we have a finite number of cardinality variables and a finite number of cardinality values, an obvious
approach is to iterate through all combinations, and pick a least solution from the combinations that satisfy the constraints.
A least solution is a lowest value in the ordering |leqM| such that the cardinality values satisfy the constraints:

%%[[wrap=code
((Sub(c)(1)), ..., (Sub(c)(n))) leqM ((Sub(d)(1)), ..., (Sub(d)(n)))
  =   (Sub(c)(1)) leqC (Sub(d)(1))
  &&  ...
  &&  (Sub(c)(n)) leqC (Sub(d)(n))
%%]

Although it is not obvious from the definition |leqM|, we show in Section~\ref{sect.TheMinimalSolution} that there is only one least solution
to a set of constrains. Unfortunately, the above approach is not feasible in practice. The problem is for |n| annotations, there are
$5^n$ combinations, which is not feasible to calculate in practice.

Fortunately, there is no need to compute all possible combinations. Due to the orderings defined on the cardinalities,
upper bound and lower bounds, we can use a fixpoint computation to obtain a least fixpoint. Actually, we need three
of these computations: one for the lower bound, one for the upper bound, and one for the cardinalities. We first
compute the lower and upper bounds. Then choose for each cardinality the most specific cardinality that fits with
the two bounds. The third iteration process relaxes the cardinalities according to |leqC|, until all constraints
are satisfied. This means that there are three fixpoint computations on the same constraint set, each time with a
different solving function. Figure~\ref{code.fixSolve} gives an overview of the implementation.

\begin{CodeFigure}{}{Pseudo code of the inferencer}{code.fixSolve}

%%[[wrap=code
infer :: ConstrSet -> Subst Cardinality
infer cs
  =  let  lowerSubst = worklistFix lowerSolveF cs [ delta :-> 1 | delta `elem` cs ]
          upperSubst = worklistFix upperSolveF cs [ delta :-> 0 | delta `elem` cs ]
     in   worklistFix cardSolveF cs [ delta :-> upperSubst delta `bestFit` lowerSubst delta | delta `elem` cs ]
%%]
\\
%%[[wrap=code
bestFit :: LowerBound -> UpperBound -> c
bestFit l u
  =  let  z = l `bestFitAux` u
     in   (Card(z)(z))
  where
    bestFitAux :: LowerBound -> UpperBound -> Usage
    bestFitAux  0    0  =  0
    bestFitAux  1    1  =  1
    bestFitAux  1    _  =  oneplus
    bestFitAux  _    1  =  onemin
    bestFitAux  _    _  =  *
%%]
\\
%%[[wrap=code
worklistFix :: (Constr a -> Constr a) -> ConstrSet -> Subst a -> Subst a
worklistFix solveF cs initial
  = iter (toList cs) initial
  where
    deps = [ (c, [d `elem` cs, c `shareVar` d, c /= d]) | c `elem` cs ]

    iter [] subst = subst
    iter (c : cs) subst
      =  let  c1      = subst `apply` c
              c2      = solveF c1
              subst'  = c2 `update` subst
         in   if subst' == subst
              then  iter cs subst'
              else  iter ((c `lookup` deps) ++ cs) subst'
%%]
\end{CodeFigure}

The fixpoint iteration proceeds as follows. We start with the best possible substitution, which is the
lowest value in the ordering. If one particular value |v| does not satisfy a constraint, then |v| is
too low in the ordering and we select a value v', with |v leq v'| and v' as low as possible in the
ordering and satisfying the constraint. We relaxed |v| to |v'|, since |v'| can only be higher in the
ordering. Such a value can always be found since the orderings are finite join-semilattices, which
implies there is a value |t| such that |v leq t| for any |x|. It is possible that satisfying one constraint
makes another constraint not-satisfied. Processing the constraints once is called an iteration, and we
repeat iterations until the substitution does not change anymore. If a substitution does not change during such an interation, then
no relaxations took place, which implies that all constraints are satisfied. Furthermore, the number of
relaxations is limited, since we have a finite substitution of which the value for each variable can
be relaxed a finite number of times. A property of the fixpoint procedure is that a least fixpoint is
reached after the process terminates.

For our constraints and their interpretation, there is only one such least fixpoint, and it
is the optimal solution to the constraint set (Section~\ref{sect.TheMinimalSolution}). This  approach is fast:
when the constraints are topologically ordered, with the left-hand side(s) as dependency on the right-hand-side, and a special treatment
for constraints that form a cycle (a change of the substitution of a variable in a cycle is directly propagated to the substitution of the
other variables in the cycle), at most four iterations are required for the upper bound computation: three iterations (in the worst case) to get |*| everywhere in the substitution, and one
iteration to discover that the substitution did not change. With an implementation by means of a worklist algorithm, it takes even less
time in practice, due to several reasons, including that the constraints represented as a graph (Section~\ref{sect.Rewriting})
are sparse. The worklist version takes a single constraint, ensures that it is satisfied, and
if it changes the substitution, adds all dependent constraints on the stack of constraints to
process. See Figure~\ref{code.fixSolve} for the worklist implementation.

The remaining question is what the solve functions are that make the substitution satisfy constraints.
Figure~\ref{code.iterate} lists the solve functions. The worklist function applies the current
substitution to a constraint, such that the solve function gets a constraint with concrete
values filled in for the variables. The solve function returns the constraint with possibly
increased values for the variables. The worklist function takes these values out of the constraint
again and updates the substitution. This happens over and over again until the substitution does not
change anymore. Compare these functions with the constraint checking functions of Section~\ref{Sect.CheckingConstraints}. The
solve functions are similar to the constraint checking functions, which is not surprising since the iteration functions
just increase values until they fit.

Figure~\ref{code.iterate} lists the solve functions. The solve functions for the lower bound and the upper bound are fairly straightforward.
The |beta| in the coercion constraint means that we do not differentiate between soft and hard coercion constraints for the lower and
upper bounds. The solve function for cardinality values requires some explanation. The |increase| function minimally relaxes both |Usage| parameters
until they coerce into each other. The |split| function relaxes |Usage| values minimally until usages are properly split over the individual
occurrences. This means for example that if we know that the combined occurrences are used in total exactly once, then once of the occurrences must
be used exactly once and the others not. If we know that the total is used more than once, then one of the individual occurrences must be used
more than once and the others can be arbitrarily used.


\begin{CodeFigure}{}{Solve functions}{code.iterate}

%%[[wrap=code
lowerSolveF (a (Sub(=>=)(beta)) b)
  =  a (Sub(=>=)(beta)) (a `min` b)
lowerSolveF ((Sub(a)(1)) \*/ ... \*/ (Sub(a)(n)) <=  a)
  =  let  z = (Sub(a)(1)) `max` ... `max` (Sub(a)(n))  -- 0 if n == 0
     in   ((Sub(a)(1)) \*/ ... \*/ (Sub(a)(n)) <=  (a `min` z))
%%]
\\
%%[[wrap=code
upperSolveF (a (Sub(=>=)(beta)) b)
  =  a (Sub(=>=)(beta)) (a `max` b)
upperSolveF ((Sub(a)(1)) \*/ ... \*/ (Sub(a)(n)) <=  a)
  =  let  z = (Sub(a)(1)) + ... + (Sub(a)(n))          -- 0 if n == 0
     in   ((Sub(a)(1)) \*/ ... \*/ (Sub(a)(n)) <=  (a `max` z))
%%]
\\
%%[[wrap=code
cardSolveF c@(_ (Sub(=>=)(s)) _)  -- soft constraint ignores cardinality
  = c
cardSolveF ((Card(a)(b)) (Sub(=>=)(h)) (Card(c)(d)))
  = let  (a1, c1)  =  increase (leqP) (leqS) a c
         (b1, d1)  =  increase (leqQ) (leqZ) b d
         b2 = a1 `(Sub(join)(leqX))` b1
         d2 = c1 `(Sub(join)(leqX))` d1
    in   ((Card(a1)(b2)) (Sub(=>=)(h)) (Card(c1)(d2)))
cardSolveF ((Card(Sub(a)(1))(Sub(b)(1))) \*/ ... \*/ (Card(Sub(a)(n))(Sub(b)(1))) <= (Card(a)(b)))
  = let  ((Sub(a1)(1)), ..., (Sub(a1)(n)), a1) = split ((Sub(a)(1)), ..., (Sub(a)(b)), a)
         (Sub(b1)(1))  =  (Sub(a1)(1))  `(Sub(join)(leqX))`  (Sub(b)(1))
         ...
         (Sub(b1)(n))  =  (Sub(a1)(n))  `(Sub(join)(leqX))`  (Sub(b)(n))
         b1            =  a1            `(Sub(join)(leqX))`  b
         b2 = (Sub(b1)(1)) `(Sub(join)(leqQ))` ... `(Sub(join)(leqQ))` (Sub(b1)(n))
    in   ((Card(Sub(a1)(1))(b2)) \*/ ... \*/ (Card(Sub(a1)(n))(b2)) <= (Card(a1)(b2)))
%%]
\\
%%[[wrap=code
split c@(0, ..., 0, 0)                      -- all exactly 0
  = c
split c@((Sub(a)(1)), ..., (Sub(a)(n)), 1)  -- one exactly 1, the others exactly 0
  | length neq0 == 1 && head neq0 == 1
      = c
  where
    (_, neq0) = partition (== 0) [(Sub(a)(1)), ..., (Sub(a)(n))]
split ((Sub(a)(1)), ..., (Sub(a)(n)), a)    -- all 1+ or all 1- or all *
  =  let  z = (Sub(a)(1)) `(Sub(join)(leqP))` ... `(Sub(join)(leqP))` (Sub(a)(n)) `(Sub(join)(leqP))` a
     in   (z, ..., z, z)
%%]
\\
%%[[wrap=code
increase relaxR coercionR a b
  = (Sub(min)((leqQ, leqQ)))  [ (a', b')  |  a' <- Usage, b' <- Usage
                                          ,  a `relaxR` a'
                                          ,  b `relaxR` b'
                                          ,  a `coercionR` b' ]
%%]
\end{CodeFigure}


\section{Minimal solution}
\label{sect.TheMinimalSolution}

  We mentioned in the introduction that the inferencer infers the minimal solution to the constraints.
  This minimal solution is computed by obtaining a least solution by means of a fixpoint iteration~\cite{nnh99}.
  However, is this really a single least solution? We verify that this is the case. To simplify matters, only the
  upper bound annotations are considered. Proofs for the lower bound and the cardinality annotations are similar.

  A solution in the context of this section is an upper bound substitution for the cardinality variables, such
  that the upper bound values satisfy the constraints.
  
  \begin{theorem}\label{theorem.one}
  There is exactly one least solution |S| to constraints |Cs|.
  \end{theorem}
  
  Note that there is always a solution |top| to the constraints. We therefore have to prove that there is at
  most one least solution. For that, we take a few sidesteps first:
  
  \begin{lemma}\label{lemma.infer}
  Substitution |S| is a solution of the constraints |Cs| if and only if |S = solve Cs S|, with |S0 <= S|, where
  |S0| is the default initial substitution.
  \end{lemma}
  
  \begin{proof}
  The fixpoint iteration process does not terminate until all constraints in |Cs| are satisfied. If |S| is a
  solution, then all constraints are satisfied, and the fixpoint iteration results in |S|.
  \end{proof}
  
  \begin{collorary}\label{lemma.inferInd}
  Substitution |S| is a solution of constraint set |Cs| if the substitution during any fixpoint iteration step
  of |solve Cs S| remains invariant.
  \end{collorary}
  
  \begin{proof}
  By induction on the structure of the |solve| function, using Lemma~\ref{lemma.infer}.
  \end{proof}
  
  With Lemma~\ref{lemma.infer} we can prove that a substitution is a solution by showing that a substitution remains
  invariant during a fixpoint iteration step. We use this lemma to prove the following lemma:
  
  \begin{lemma}
  Given solutions |S| and |R|, the minimum |Z| of these solutions, defined as
  |Z === S `min` R === { S(delta) `min` R(delta) || delta `elem` annotations }|, is also a solution.
  \end{lemma}
  
  \begin{proof}
  Take |S|, |R|, and |Z| as specified above. Now, consider a solve step with constraint |C| for
  |infer Cs Z|.
  
  Suppose |C === (Sub(delta)(1)) =>= (Sub(delta)(2))|. Since fixpoint iteration is completed
  for |S| and |R|, |S((Sub(delta)(1))) <= S((Sub(delta)(2)))| and |R((Sub(delta)(1))) <= R((Sub(delta)(2)))|.
  Since |Z((Sub(delta)(1))) === S((Sub(delta)(1))) `min` R((Sub(delta)(1)))|, we conclude that by
  transitivity |Z((Sub(delta)(1))) <= S((Sub(delta)(2)))| and |Z((Sub(delta)(1))) <= R((Sub(delta)(2)))|,
  which implies that |Z((Sub(delta)(1))) <= S((Sub(delta)(2))) `min` S((Sub(delta)(2))) = Z((Sub(delta)(2)))|. This
  means that |Z((Sub(delta)(1))) `join` Z((Sub(delta)(2))) = Z((Sub(delta)(2)))|, and we
  conclude that |Z| does not change.
  
  Suppose that |C === sumi (Sub(delta)(i)) <= delta|. Since fixpoint iteration is completed
  for |S| and |R|, |sumi S((Sub(delta)(i))) <= S(delta)| and |sumi R((Sub(delta)(i))) <= R(delta)|.
  Analogous to the previous section, we conclude that |sumi Z((Sub(delta)(i))) <= S(delta)| and |sumi Z((Sub(delta)(i))) <= R(delta)|,
  which means that |sumi Z((Sub(delta)(i))) <= S(delta) `min` R(delta) = Z(delta)|, and we
  conclude that |Z| does not change.
  
  We therefore conclude that |Z| remains invariant during the fixpoint iteration process and conclude by
  Lemma~\ref{lemma.infer} that |Z| is a solution.
  \end{proof}

  So, taking the minimum of two solutions gives another solution. With this fact, we prove Theorem~\ref{theorem.one}:
  
  \begin{proof}
  Assume that the opposite is the case, such that |P| and |Q| are different least solutions. In order for |P| and |Q|
  to be different, there must be some annotation |(Sub(delta)(0))| for which |Q((Sub(delta)(0))) < P((Sub(delta)(0)))| (the
  other way around as well, but we do not need that here). Consider the
  solution obtained by taking the minimum from |P| and |Q|: |M === P `min` Q|. For each annotation |delta|,
  |M(delta) <= P(delta)|. But for |(Sub(delta)(0))|, |M((Sub(delta)(0))) = Q((Sub(delta)(0))) < P((Sub(delta)(0)))|.
  This means that |M| is a smaller solution that |P|. Thus |P| cannot be a least solution, and that there is only
  one least solution.
  \end{proof}

  Proofs for the lower bound, and even the cardinality values, are analogous to this proof. This means that we compute the
  best solution or `most general' solution to the constraints. The constraints can be considered part of the type language,
  which means that our analysis gives a principal uniqueness typing.

\section{Adding a non-recursive let}

In Chapter~\ref{chapt.NoBindings} we showed how to gather the constraints. In this chapter, we showed how to infer the
uniqueness types given the constraints, using a fixpoint iteration approach. We are now going to make life a bit interesting
by supporting a non-recursive |let|. This will complicate the inferencer, especially since we are paving the way to support a polymorphic and recursive |let| for
Chapter~\ref{chapt.Recursion} in Chapter~\ref{chapt.Polymorphic}. A gentle warning: make sure you have a feeling
of the inference process up to this point, otherwise subsequent sections and chapters will be difficult to understand!

The remainder of this chapter describes how to deal with a non-recursive |let|. We show that we can deal with
this language construct by means of constraint duplication. We can handle this constraint duplication
during the constraint gathering phase, but also during the inference phase. There are several
reasons to deal with constraint duplication during the inference phase. Some of the reasons will only
become clear in Chapter~\ref{chapt.Recursion} and Chapter~\ref{chapt.Polymorphic}, but we can
already mention an advantage in this chapter. Constraint duplication leads to an explosion in the
number of constraints, but we keep this constraint set manageable by applying several simplification
strategies (Section~\ref{sect.Strategies}). Some of these strategies can benefit from upper and
lower bounds that are incrementally computed during the inference phase.

Before we go into the details and complications of the uniqueness inference process for the |let|, we
first discuss how to deal with it during the constraint gathering phase. Assume we have the following |let|-construct:

%%[[wrap=code
let  id = def_expr
in   body_expr
%%]

Technically speaking, there is no use for allowing more than one binding to occur in a |let|, since the
|let| is non-recursive and monomorphic as well. However, in preparation for subsequent chapters, we assume there
can be many bindings inside the same |let|, although we often only use one binding in each |let| for type rules and
examples.

For the conventional type system, a non-recursive, monomorphic |let| is a trivial addition to the type system: infer a type |tau|
for |def_expr| with the same environment |Gamma| as the entire expression, then use |tau| for each occurrence of |id|
by adding it to |Gamma| for |body_expr|. Since the |let| is non-recursive, the environment for |def_expr| does not need knowledge
about |id|, but that will change once we want to support a recursive |let| in Chapter~\ref{chapt.Recursion}. This gives the
type rule:

\begin{center}
\rulerCmdUse{RulerUniquenessExamples.EL.expr.base.e.let}
\end{center}

We can apply the same strategy for the uniqueness type system. A set of constraints $S$ is inferred for |expr_def|. For each use
of |id|, $S$ is copied and becomes the constraint set of the identifier. To show why this approach works, remember that beta-reduction
allows us to substitute an identifier by its definition.

Suppose the occurrences |1..n| of |id| in |body_expr| are replaced by |def_expr| to |(Sub(inst_expr)(1))..(Sub(inst_expr)(n))|.
In other words, in the expression:

%%[[wrap=code
let  id = def_expr
 in  ... (Sub(id)(1)) ... (Sub(id)(n)) ...
%%]

The definition for |id| is inlined to (beta-reduction):

%%[[wrap=code
... (Sub(inst_expr)(1)) ... (Sub(inst_expr)(n)) ...
%%]

We now compare the constraint sets resulting from constraint gathering on |def_expr| and each |(Sub(inst_expr)(i))|. The constraint
sets of such an |(Sub(inst_expr)(i))| is a copy of |S|, except that the cardinality variables are different. The reason for this difference
is due to the fresh annotations on the types occurring in |(Sub(inst_expr)(1))..(Sub(inst_expr)(n))| (Section~\ref{sect.UniquenessVariables}).

There is one other difference in the constraint set of the whole expression. If |def_expr| uses some identifier |x| from an outer
binding group, then the |x| occurs |n| times more often in the inlined version. The corresponding aggregation constraint will be |n|
times bigger. We solve this difference with a trick: when instantiating the constraint set of |def_expr|, the cardinality variables
occurring on the use-site-types of identifiers are not given fresh variables (unless they are an argument or result of a function, but that
is not important for this explanation). This gives us |n| of the smaller aggregation constraints, slightly different, but gives a
conservative result towards the |n| times as big aggregation constraint, as the following example shows:

%%[[wrap=code
let  x = ...
     y = x ... x
in   y ... y
%%]
%%[[wrap=code
let  x = ...
in   (x ... x) ... (x ... x)
%%]

In the version where |y| is inlined, we compute the bounds of |x| directly. In the inlined version, the bound of |x| are
computed from the bounds of |y|. This potentially leads to a less accurate, but conservative result, since we already
approximate the bounds of |y|. This means that the constraint set that we obtain by separately analysing |y| and
inserting this constraint set at each use-site of |y|, gives solutions that are equal, or conservatively less
accurate, compared to solutions obtained from analysing an expression without a |let|.

There is a caveat here. The above assumption only holds in the presence of functions if we assume that all free
variables of a function are at least as often used as the function itself. Consider the following expression:

%%[[wrap=code
let  y  =  1
     f  =  \x -> x + y
in   f 1 + f 2
%%]

The free variable |y| is hidden by |f|, but a use of |f| indirectly leads to a use of |y|. Normally, the aggregation
constraint deals with this situation, but the aggregation constraint does not go deeper than the spine of the function;
it does not aggregate the parameters or result of a function. By putting a soft coercion constraint between |y| and
|f| (outermost annotation of |y| and the annotation on the function arrow of the lambda abstraction), we ensure that
the aggregation constraint does aggregate uses of |y|. Later in this chapter we introduce constraint graphs, which
can be used to improve this approximation. For example, if there is no path between the result of a function |g| and a
free variable |v| (for an instantiation of |g|), then the constraint between |g| and |v| may be omitted. This is similar
to an approach in Section~\ref{sect.lowerite}. However, we leave this particular topic as future work.

There is an alternative way to discover how to deal with a |let|. Consider the function |(\f -> f 3 + f 3) (\x -> x)| from
Chapter~\ref{Sect.HigherGathering}. There is an equivalent way to write this function:

%%[[wrap=code
let  f = \x -> x
in  f 3 + f 3
%%]

This expression is exactly the same because the let is monomorphic. Since we use a full polyvariant analysis, we expect
the constraint set of the let-expression to equal the constraint set of the function-expression. The function expression
copied the constraint set of |\x -> x| to the use-sites of |f|. In our description of how to deal with a |let|, we
copy the constraint set of |f|---which is the constraint set of |\x -> x|---to the use-sites of |f|. So, technically
speaking, this is nothing new!

We now present a type system that uses the above approach to type |let|s. The type scheme has an additional result |omega|,
which represents the constraints of a binding group. Note that we consider an argument expression to be a (pseudo) nested
binding group of the binding group of the function application, since conceptually that is the same. The type scheme is now:

\rulerCmdUse{RulerUniquenessExamples.UL.expr.base.scheme}

The inferencer performs the constraint duplication. In the inferred constraint set, we insert a placeholder to represent the
constraints that are to be inserted, by means of the following constraint:

%%[[wrap=code
Inst <binding_group_id>                                                         -- Identifier of binding group
     [(replace_binding_group_id, by_binding_goup_id)]                           -- How to map Inst constraints
     [(<definition_site_uniqueness_variable>, <use_site_uniqueness_variable>)]  -- How to map the annotations
%%]

This constraint is inserted at the use-site of an identifier and at a function application. We use the relation
|genInst| in the type rule to convert a source and destination type into an |Inst| constraint. This |Inst| constraint
specifies that we take the constraint set for the identifier (or argument expression) and insert a fresh copy at the
use-site. This constraint sets can contain |Inst| constraints. These |Inst| constraint point to some binding
group |beta|, which is known at the use site. Therefore, the |Inst| constraint contains a mapping how to rename
the |Inst| constraints of the definition site, such that they refer to the binding groups of the use site. These
|beta| annotations can be considered universally quantified phantom variables on type constructors, which are
instantiated at a use site. We have to keep track of where they are instantiated too; this is why we add a
list of pairs to the |Inst| constraint. We already mentioned that the constraint set is the same, but the cardinality annotations
differ. The list of pairs in the constraint specify which cardinality variables in the constraint set of the definition-site are
mapped to which cardinality variables of the use-site. The other cardinality variables occurring in the definition-site constraint set
are replaced by fresh variables in the inferencer (Section~\ref{sect.TheInferencer}). This gives us the following rule for identifiers:

\begin{center}
\rulerCmdUse{RulerUniquenessExamples.UL.expr.base.e.var}
\rulerCmdUse{RulerUniquenessExamples.UL.expr.base.e.app}
\end{center}

A nice result is that the type rules do not have the seemingly complex propagation of constraint sets: the inferencer takes care
of it. 

There is a slight optimization possible. If a parameter of a function is not a function itself, then we do not need to generate an
|Inst| constraint for it. The reason is that the aggregation constraint properly relates the annotations of such a variable.

For the |let|, we have the following type rule:

\begin{center}
\rulerCmdUse{RulerUniquenessExamples.UL.expr.base.e.let}
\end{center}

The term \emph{binding group} refers to the bindings of the |let|. We assume that each |let| is numbered. This number
identifies the binding group. In EH, each |let| only contains a single binding group, but technically speaking, the |let|
can be partitioned in multiple binding groups (such as in Haskell). Binding groups are nested. Binding group |A| is a
parent of binding group |B| if and only if |B| occurs in an expression of |A|. The body of the |let| belongs to the same
binding group of the enclosing expression. An identifier in binding group |C| is also in binding group |D| if and only if
|D| equals |C|, or |D| is a parent of |C|. Two identifiers are in the same binding group if
and only if the binding group of one identifier is the parent of the binding group of the other
identifier. A binding group is outermost if it has no parent.

The type rule for the |let| specifies that a compiler performs two tasks for a |let|: generate a constraint to
aggregate the cardinalities of individual occurrences of the identifier bound to the |let|, and collect constraints
of the binding group.

In the end, a result of the constraint gathering phase is a set of constraints for each binding group. These constraints
are passed to the inferencer. The inferencer combines the individual results and computes a least substitution.


\section{The inferencer revisited}
\label{sect.Instantiation}

The inferencer processes the constraint sets of binding groups in the order such that if $A$ is a constraint set
containing an |Inst| constraint referencing binding group $B$, then $B$ is processed before $A$. In other
words: the binding groups are processed in the order specified by a topological sort on the dependency graph
between constraint sets. Since the binding groups are non-recursive, it is guaranteed that when the inferencer
processes a binding group, that all the binding groups it depends on have already been processed.

The processing of a binding group consists essentially of getting rid of the |Inst| constraints, and
replacing them by normal constraints by making a fresh copy of the originating constraint set. Not every
cardinality variable may be replaced by a fresh version when making a copy: the originating and destination
binding group may share some annotation variables of which the relations need to be preserved. These annotations
we call the $\emph{sacred}$ variables.

The sacred variables, or sacred annotations, are those cardinality variables that may not be replaced by a fresh
cardinality variable. These sacred variables are the annotations on the definition-site types of identifiers and the
annotations on use-site types of identifiers that are in scope for a binding group. Basically, the sacred annotations
are the annotations on identifiers that are directly influenced by and visible to the binding group. The number of sacred
annotation variables is linear in the size of the abstract syntax tree. The sacred variables are collected and
passed to the inferencer. We omitted this detail in the type rules.

After copying the constraint sets, we end up with a normal constraint set again, and can continue with
the cardinality inference as specified in the first part of this chapter. However, replacing the |Inst|
constraint by the constraint set of the corresponding binding group, tends to make the number of constraints
explode. We take a different approach, by converting each constraint set to an equivalent
graph representation: the \emph{constraint graph}. By simplifying the graph, it becomes small enough for practical
use (Section~\ref{sect.Rewriting}).

The graph representation of a constraint set |Cs| is obtained as follows. Let $G = (V, E, H)$ be a directed graph with
vertices $V$, edges $E$, and hyper edges $H$. The vertices of $G$ are the cardinality annotations of |Cs|: |delta `elem` V| if and only
if |delta `isAnnotation` Cs|. The labeled edges of $G$ are the coercion constraints of |Cs|:
|E((Delta(1)), (Delta(2))) = beta| if and only if |((Delta(1)) (Sub(=>=)(beta)) (Delta(2))) `elem` Cs|. The labeled hyper edges of $G$ are aggregation
constraints of |Cs|: |H [(Delta(1)), ..., (Delta(n))] = (delta, Nothing)| if and only if
|((Delta(1)) \*/ ... \*/ (Delta(n)) <= delta) `elem` Cs|. This |Nothing| value is a label on the block
symbol of a hyper edge. We come back to this value later.

As an example of the above approach, take the following constraint set (we make an explicit difference between the
soft and hard versions of the coercion constraints):

%%[[wrap=code
a (Sub(=>=)(s)) b
b (Sub(=>=)(h)) c
b (Sub(=>=)(h)) d
c \*/ d <= e
e (Sub(=>=)(h)) b
%%]

The variables |a..e| become vertices in the graph. The coercion constraints become the edges and the aggregation
constraints become hyper edges. Suppose that the variables |a|, |c|, and |e| are sacred cardinality
variables. We visually represent sacred vertices with a double circle, and normal vertices with just a single circle.
This results in the graph:

\begin{center}
\includegraphics{unq-graph}
\end{center}

To calculate an upper bound substitution from initial substitution |S(a) = 1, S(delta) = 0, delta `elem` [b..e]|,
we iterate over the (hyper) edges of the graph. The edge between |a| and |b|, combined with the substitution |S(a) = 1|, force |S(b)| to |1|. |S(b) = 1|
forces both |S(c)| and |S(d)| to |1|. The hyper edge combines |S(c) = 1| and |S(d) = 1|, resulting in |*|, and forces |S(b)| to |*|. Repeating the
above procedure forces |S(c)| and |S(d)| to |*|. The hyper edge combines |S(c) = *| and |S(d) = *| in |*| for |S(b)|, which is already the case. None of the edges
in the graph change the substitution anymore, and the iteration stops with a final substitution S', with |S'(a) = 1, S'(delta) = *, delta `elem` [b..e]|.

Figure~\ref{fig.graphinferencer} gives an overview of inferencer. The inferencer constructs the constraint graph of the outermost binding group and
performs a fixpoint substitution calculation on it. To obtain this graph, the inferencer takes the constraint set without |Inst| constraints and
creates a constraint graph out of it. Then we fill these missing holes by constructing the constraint graphs belonging to the |Inst| constraints. Note that we make
a difference between constraint graphs and binding groups here. A binding group can have multiple constraint graphs if it contains higher-order functions. In
these cases, there is a separate constraint graph for each instantiation of an identifier of the binding group occurring in the program.

\begin{CodeFigure}{}{Overview of the inferencer}{fig.graphinferencer}

%%[[wrap=code
inferSubst constrSets
  =  infer (construct (outermostBndgId, []))
  where
    construct key@(bndgId, idMap)
      | inMemo key  =  key `lookup` memo
      | otherwise   =  let  cs   = bndgId `lookup` constrSets
                            cs'  = rename idMap cs
                            gr   = csToGraph cs'
                            gr'  = foldr (instantiate bndgId) gr (insts cs')
                       in   returnAndStoreInMemo gr'

    instantiate to (Inst from idMap tups) gr
      =  let  key   = (from, idMap)
              src   = construct key
              src'  = fresh (sacred to) tups src
              dst   = src `union` gr
              dst'  = reduce dst
         in  dst'
%%]
\end{CodeFigure}
%\end{figure}

The last step results in a cardinality substitution for each cardinality variable occurring in types
of the outermost binding group of the program. It does not directly give a substitution for another
binding group, since the substitution is potentially different for each use of an identifier occurring in such a
binding group. We can construct such a definition-site substitution from a use-site substitution. Suppose
we know the cardinality substitution $R$ for binding group |k|, and |k| contains the use-site of an identifier |x|
defined in binding group |j|. We can construct the substitution $S$ for the upper bound and the substitution $W$ for the lower bound
from $R$ by translating the guarantees that left-hand side |Usage| values of cardinality values of $R$ give to upper and lower bounds.
Consider an upper bound substitution $S$ for example. Take $S$ such that it maps to |0| for each cardinality variable of |j|,
except for the cardinality variables mentioned by the |Inst| constraint. These we set to a value determined by the
cardinality of the use-site counterpart in $R$. If the left-hand side cardinality value is |0|, the upper bound is |0|, and
for |onemin| and |1| the upper bound is |1|, and otherwise |*|. By feeding $S$ as initial substitution to the inferencer for the fixpoint
computations on the constraint graph of |j|, we get a final substitution for the upper bound of each cardinality variable of |j|.
This allows us to get a use-site dependent typing for a binding-group, which can be exploited by a code generator that performs
code specialization.


\section{Graph reduction}
\label{sect.Rewriting}
\label{sect.Rewrite}
\label{sect.Strategies}

The performance of an implementation of the uniqueness type system is directly connected to the size of the
uniqueness graphs. Without special treatment, these graphs explode in size. For example, consider the program:

%%[[wrap=code
let  inc1 = \x -> x + 1
in   let  inc4 = \x -> inc1 (inc1 (inc1 (inc1 x))
     in   let  inc16 = \x -> inc4 (inc4 (inc4 (inc4 x))
          in   inc16 4
%%]

With constraint duplication, the size of the graph corresponding to |inc16| is at least four times as big as the graph of |inc4|, and at least
sixteen times as big as the graph of |inc1|. However, we can reduce this graph, because only the sacred variables matter. These are all use-site
and definition-site cardinality variables in scope of the binding group. The other cardinality variables, for example, those taken freshly when
instantiating a constraint graph, are intermediates. The vertices corresponding to sacred cardinality variables are sacred vertices. The other
vertices are intermediates. We present a reduction strategy that removes all intermediates from the graph, and preserves the substitution that
the graph represents.

The theoretical maximum number of constraints between these sacred vertices is only dependent on the
number of sacred vertices, which are in turn only dependent on the size of the abstract syntax tree. Without reduction, the number of
constraints depends on the amount of duplication, which can easily
cause the number of constraints to explode, as the example shows. With the reduction of intermediate vertices, and $n$
sacred vertices, the theoretical maximum number of normal edges is $O(n^2)$, but the theoretical number
of hyper edges is still high: $O(n!)$. In practice, these numbers are not reached. The constraint graphs are
sparse, with the number of edges proportional (on average) to the number of vertices. The reason is that the
sacred annotations corresponds to values, and evaluation of some value depends typically on only a few
other values. Furthermore, there are only a few hyper edges with many sources (those related to functions that
are used a lot, such as prelude functions), but most identifiers in a program have only a handful of
occurrences.


\subsection{Preprocessing reductions}

We present several relatively simple reduction strategies. It will turn out that these reduction strategies are
insufficient. However, these strategies already reduce the number of paths in the graph, which makes the real
reduction strategy (Section~\ref{sect.RedInstGraphs}) performs better in practice.

Again, some cardinality variables may not be eliminated. The sacred cardinality variables will have counterparts
in the destination graph during instantiation and should not be eliminated. The
other variables are allowed to be eliminated, since their values can be reconstructed once we have a substitution for the
sacred variables, by filling in the values in the original graph, and performing the
fixpoint computation again, as we discussed earlier in this chapter.

A simple elimination strategy is splitting the constraint graph in connected components,
and dropping those components without a sacred cardinality variable. These components cannot influence
the substitution on sacred cardinality variables, since there is no path to them. This strategy
is only effective when there are multiple, independent (mutual exclusive), bindings in a binding group.
A possible use for this strategy arises in the context of a module system, where bindings from another
module can be considered to be in the same binding group.

Another elimination strategy is cycle reduction. Thus far, however, cycles in the constraint graph cannot occur,
since each binding is non-recursive in this chapter. In Chapter~\ref{chapt.Recursion}, recursive |let|s
are discussed, which can cause cycles in the constraint graph. A cycle in the constraint graph can be
reduced to all sacred nodes in the cycle, or to an arbitrary node if there are no sacred nodes in the
cycle. The edges of a singleton cycle, \emph{self edges}, can be removed entirely.

There is an effective reduction strategy that can be applied in this chapter. Each non-sacred node |u| that
is not connected by hyper edges, can be eliminated. For each pair of nodes |a| and |b|, such that there
exists only an edge |a| to |u|, and only an edge from |u| to |b|, create an edge from |a| to |b|. Then remove
|u| and the corresponding edges from the graph. Unfortunately, this strategy removes not all of the intermediate
nodes; intermediate vertices with an in-degree greater than one are not reduced for example.

A final reduction strategy that we discuss makes use of upper and lower bound information. By applying the
fixpoint substitution computations for the lower and upper bound on the constraint graph (while the graph
is being constructed), we can get bounds for the cardinality variables. Take the upper bound values for example. Most
of these values will still be |0|, since these values depend largely on the usage of the binding group. But constraints
from annotations of the user (Section~\ref{sect.Signatures}), or ways to deal with a module system (Chapter~\ref{chapt.BeyondEH}),
can give some information that allow |*| to be inferred as an upper bound. If we also get a lower bound value of |0|, we
know that the two substitutions do not change anymore, and that the only possible cardinality annotation is |(Card(*)(*))|.
Suppose that this is the case for vertex |n|. Then for each vertex |m| with an edge from |m| to |n|, we know that
|m| also has a lower bound substitution of |0| and an upper bound substitution of |*|, and a corresponding
cardinality value of |(Card(*)(*))|, because that is the result of one solve step on this the edge from |m| to |n|. In
that case, we can remove the edge from |m| to |n| from the constraint graph. This approach disconnects nodes
from the graph for which we already know that we cannot optimize the corresponding values.

All the reduction strategies discussed in this subsection have the property that they can be implemented in an efficient way.
Unfortunately, not all intermediate vertices are removed. Therefore we need a more complicated approach, which we present
in the next section (Section~\ref{sect.RedInstGraphs}). Still, these reductions are important, because the performance of
the reduction of the next section depends on the size of the graph that it gets as input.

\section{Reduction of intermediates}
\label{sect.RedInstGraphs}

Figure~\ref{fig.graphinferencer} mentioned the function |reduce|. We describe the implementation of this function
in this section. This is an important function: it ensures that graphs do not explode due to constraint duplication.

We take the graph |G = ((Sub(N)(G)), (Sub(E)(G)))| (encountered above) as example:

\begin{center}
\includegraphics{unq-graph}
\end{center}

From this graph, we construct a reduced graph |R| that does not have the vertices |b| and |d|. We take a two step approach:
first we create a reduced graph |K| that contains only normal edges, and then a reduced graph |L| that contains
only hyper edges. The reduced graph |R| is the union of |K| and |L|.

We start with the reduced graph |K=((Sub(V)(K)),(Sub(E)(K)),emptyset)|. This reduced graph only contains the
sacred vertices of |G|: |(Sub(V)(K)) = [ v `elem` (Sub(V)(G)) || v sacred ]|. The set of edges |(Sub(E)(K))|
is defined as follows:

%%[[wrap=code
[ ((Delta(1)),(Delta(2)), h) `elem` (Sub(E)(K)) || (Delta(1)) `hardPath` (Delta(2)) ]
[ ((Delta(1)),(Delta(2)), s) `elem` (Sub(E)(K)) || (Delta(1)) `softPath` (Delta(2)) && not ((Delta(1)) `hardPath` (Delta(2))) ]
%%]

There is a hard path between |x| and |y| (|x /= y|) if there is a simple path between |x| and |y| that:
\begin{itemize}
\item Contains no sacred vertices except |x| and |y|.
\item All edges are labeled with |h|.
\end{itemize}

Similarly, a soft path between |x| and |y| (|x /= y|) if there is a simple path between |x| and |y| that:
\begin{itemize}
\item Contains no sacred vertices except |x| and |y|.
\item Has an edge labeled with |s|.
\end{itemize}

The idea behind these two definitions is that a soft edge is overruled by a hard edge.

When we look at the example, we see that there is only a soft path between |a| and |c|. There is
a hard path between |e| and |c| (via |b|). The result is:

\begin{center}
\includegraphics{unq-graph2}
\end{center}

Now the graph |L=((Sub(V)(L)),emptyset,(Sub(H)(L)))| containing only hyper edges. This reduced graph only contains the
sacred vertices of |G|: |(Sub(V)(L)) = [ v `elem` (Sub(V)(G)) || v sacred ]|. The set of hyper edges |(Sub(H)(L))|
is defined as follows:

|(Sub(H)(L)) [(Delta(1)), ..., (Delta(n))] = (delta, eta)|, if:

\begin{itemize}
\item |delta| is a sacred vertex.
\item Each |(Delta(i))| is a sacred vertex.
\item There is a path P between |(Delta(i))| and |delta| in |G|, not containing any other sacred vertices.
\item P traverses a hyper edge (possibly more). The hyper edge |HG| is the hyper edge closest to |delta|.
\item The value |eta| is the value |eta| taken from |HG| if all the vertices |(Delta(i))| are the sources of |HG|. Otherwise |eta| is
      the value |Just X| where |X| is a constraint graph that describes how |(Delta(1)) ... (Delta(n))|
      are combined into |delta|.
\end{itemize}

Before we talk about the graph |X|, we consider the example again. There is a path from both |a| end |c| to
|e| traversing a hyper edge. We therefore expect |L| to be:

\begin{center}
\includegraphics{unq-graph3}
\end{center}

Unfortunately, there is a coercion (actually two) between |a| and the source vertex |d| on
the hyper edge in |G|. The problem is that the hyper edge can force some value for the substitution of
|d| which is relaxed by the coercion before it reaches |a|. If we blindly connect |a| to the hyper edge
(as in the example), then the hyper edge can force the value directly on |a|, which is not the case
with the original graph |G|. Or formulated in terms of constraints, there is a difference between
|a (Sub(=>=)(s)) d, d \+/ c <= e| and |a \+/ c <= e|.

Even worse, consider a slightly different graph:

\begin{center}
\includegraphics{unq-graph5}
\end{center}

A reduction from the left graph to the right graph gives a constraint graph that results in
inconsistent solutions! This is where |eta = Just X| enters the picture. We store a graph on a hyper
edge (|HL|) in |L| that describes how the sources of |HL| are combined. This is basically a subgraph
of |G| representing |HL|. When performing a solve step for |HL|, the inferencer
either applies the solve step for an aggregation constraint (|eta = Nothing|), or recurses on the
graph |X| (|eta = Just X|). In that case, the inferencer temporarily extends the substitution with
initial values for the vertices in |X| that are not on |HL|.

We construct the constraint graph |X = ((Sub(V)(X)),(Sub(E)(X)))| as follows. We first construct
a graph |X1| which is a subgraph of |G| such that it contains only vertices and edges on simple
paths from |(Delta(i))| to |delta| that traverse hyper edge |HG|. In other words, it is just
the part of |G| that is related to the hyper edge that we are inserting into |L|. The second step
is that we clone |X1| to |X2| with a fresh variable on each vertex of |X2|, except for the
source and destination vertices of |HL|. Now comes the trick: we mark all source
and destination vertices of hyper edges in |X2| sacred and recursively reduce |X2| to |X|. Since
all sources and destinations of the hyper edges are marked as sacred, reduction of this graph does
not recurse further, but creates a small graph that connects the sources of |HL| properly to the destination of |HL|.

Applying this approach on the running example of this section, gives us the following graphs for |L| and |X|:

\begin{center}
\includegraphics{unq-graph5}
\end{center}

Unfortunately, this reduction is insufficient. It saves us from an explosion of coercion constraints,
but does not safe us from an explosion of hyper edges. Consider the example at the beginning of
Section~\ref{sect.Rewrite}. The resulting graph has a hyper edge containing four hyper edges who
each in turn contain four hyper edges. This problem can be solved by merging hyper edges in
a constraint graph to a single hyper edge.

Merging hyper edges basically corresponds to simplifying an expression consisting of
additions and maximums (or minimums), if we assume that coercions between destinations of a
hyper edge and a source of another hyper edge are equalities (which is a conservative
approximation). Expressions with additions and maximums can be rewritten to a
canonical form, called \emph{max-min-plus canonical form}~\cite{DeSvan:02-002}. This is
achieved by applying the following rewrite rules:

%%[[wrap=code
max(alpha,beta) + gamma === max(alpha+gamma,beta+gamma)
max(alpha,beta) + max(gamma,delta) === max(alpha+gamma,alpha+delta,beta+gamma,beta+delta)
%%]

Note that the rewrite rules for plus/max and max/min are equivalent, such that there is no
difference between the graph for upper and lower bound.

This leads to a possible future work: we generated constraint graph in such a way that the
edges correspond to constraints. But what if we store constraints as computations in
the vertices, such that the edges connect inputs and outputs of these computations. A
constraint graph is then a \emph{Max-Min-Plus System} which we can reduce to a canonical
form of a fixed size in the number of annotations. This should be possible for the
upper and lower bounds, although it is not clear if this graph can be used for the
cardinality propagation.

The last step is to take the union between |K| and |L| to obtain:

\begin{center}
\includegraphics{unq-graph4}
\end{center}

\subsection{Remarks about graph reduction}

The concept of sacred cardinality variables, or sacred vertices, is important. The number of sacred
cardinality variables is a subset of all cardinality variables involved, especially where
instantiations occur (which creates intermediates). The reduction strategies
that we discussed, reduce the graphs such that the size is determined by the number of sacred
vertices instead of the amount of constraint duplication (which would be a severe problem otherwise).
This helps to improve the performance of the compiler on real-life programs. The number of sacred cardinality
variables is a good indicator of the required processing time of the type system. To take this a step further, by
controlling the number of sacred cardinality variables, a compiler could trade accuracy conservatively
for compilation speed. For example, the use-site types can be replaced by definition-site types,
reducing the number of cardinality annotations severely, but also reducing the quality of the
inference for these types. In Chapter~\ref{chapt.DataTypes} we introduce data types,
which leads to an explosion of sacred cardinality variables, and we discuss several ways to allow control
over the number of cardinality variables, and thus to control the size of the constraint graphs, and
the performance of the compiler.


\section{Conclusion}

The inferencing of cardinality annotations on types is rather indirect. The typing problem is translated into a constraint problem, which
in turn is converted to a graph problem. This may seem to be overkill at first thought, but it allows a certain separation
of concerns and abstraction, which is further exploited in subsequent chapters. However, in this chapter, we already use
this separation to allow non-recursive and monomorphic |let|-bindings in the expression language.

The translation of the constraints to a graph representation allows another way of looking at the
typing problem. Paths in the constraint graph tell how cardinality variables are related to each
other. Furthermore, we are only interested in the cardinality values of a small subset of the nodes
of the constraint graph. By looking at the paths between these special, sacred, nodes,
we eliminate the other, intermediate, nodes, so that the resulting constraint graphs are small,
which is important for the performance of the compiler.

In the next chapter, Chapter~\ref{chapt.Recursion}, the notion of a constraint graph will be exploited further,
to allow polymorphic and recursive |let| bindings. The reduction work on graphs will become even more
important, as the recursion will cause a fixpoint construction of a constraint set of a binding group,
which would explode beyond imagination without the reduction work of this chapter.

%%]
