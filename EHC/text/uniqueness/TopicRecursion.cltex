%%[main

\chapter{Recursion}
\label{chapt.Recursion}

In Chapter~\ref{chapt.Polyvariant}, we discussed the inferencing of cardinalities for a non-recursive
|let|. We go one step further in this chapter, in order to support a (mutual) recursive Let. It is important
to fully understand Chapter~\ref{chapt.Polyvariant} before proceeding with this chapter, especially
the part about the inferencer (Section~\ref{sect.TheInferencer} and Section~\ref{sect.Instantiation}), since we set up
the type system by means of the |Inst| constraint in such a way that only the inferencer is affected to
support recursion.


\section{Example}

  Consider the following, incorrect\footnote{We discuss how to make expressions conditional in Chapter~\ref{chapt.DataTypes} about data types},
  implementation of the faculty function:

%%[[wrap=code
let  (Sub(fac)(a)) :: Int -> Int = \n -> n * (Sub(fac)(b)) (n - 1)
in   fac 3
%%]

  So, what is the challenge of the recursion? The problem is that when gathering the constraint set of (the binding group of) |(Sub(fac)(a))|,
  we encounter the variable |(Sub(fac)(b))|, which results in an |Inst| constraint that refers to |(Sub(fac)(a))|. So, the constraint
  graph of |(Sub(fac)(a)| is needed to construct the constraint graph of |(Sub(fac)(a)|, which is a problem, since it is not yet there.

  This is a familiar problem. This problem already occurs with a conventional type system. Hindley-Milner type inference solves this
  problem by treating an identifier, that occurs recursively, monomorphically. That is also a solution to our problem. By
  giving up a polyvariant typing for an identifier occurring in an expression of its own binding group, we do not need the constraint
  graph when dealing with the corresponding |Inst| constraint (Section~\ref{Sect.MonovariantRecursion}). A downside is that we loose some
  accuracy this way~\cite{DBLP:conf/sas/DussartHM95}. In a conventional type system, this problem can
  be solved when a programmer provides a polymorphic type signature~\cite{jones04practical}. Similarly, we can bypass
  this problem, when the programmer supplies a polyvariant constraint set (Section~\ref{sect.Helping}).

  Contrary to the conventional typing problem, full polyvariant cardinality inference is possible in the presence of
  recursion. We perform Kleene-Mycroft Iteration~\cite{DussartHengleinMossin:PolynomialTimeBTA:95}, which amounts to
  a fixpoint iteration on the constraint graph, which assumes initially an empty constraint graph for
  a binding group and keeps expanding the graph, until a fixpoint is reached. We come to this in Section~\ref{sect.FullRecursion}.


\section{Monovariant recursion}
\label{Sect.MonovariantRecursion}

  A solution is to forget instantiation for recursive identifiers, and choose the definition-site annotated of an identifier
  as use-site annotated type instead of a freshly annotated type. For example, for the |fac| function:

%%[[wrap=code
let  fac :: (Annot(Int)(2)) (Annot(->)(1)) (Annot(Int)(3)) = \n -> n * (fac :: (Annot(Int)(2)) (Annot(->)(1)) (Annot(Int)(3))) (n - 1)
 in  fac :: (Annot(Int)(5)) (Annot(->)(4)) (Annot(Int)(6))
%%]

  In our implementation, we always use fresh identifiers for the use-site annotated type. An |Inst| constraint contains a
  mapping from annotations on the definition-site type to the (fresh) annotations on the use-site type. Then it is just a matter
  of equating the annotations on the definition-site type to those on each use-site type. So, with this approach we interpret
  |Inst| constraints belonging to recursive identifiers differently, and instead of inserting the constraint set of the
  corresponding binding group, we just insert the constraints |a =>= b| and |b =>= a| for each pair |(a, b)| mentioned in the
  |Inst| constraint. An |Inst| constraint corresponds to a recursive identifier if it occurs in a right-hand side expression
  of a binding-group.

  To support recursion, only a minor change of the inferencer is required. But, in this approach, individual recursive identifiers
  are treated in exactly the same way. This gives suboptimal typings for some, perhaps contrived, programs~\cite{jones04practical}, such as:

%%[[wrap=code
let  f = \n ->  let  x = x
                in   f x + f (n-1)
in   f 1
%%]

  What happens here is that the |x| is discovered to be shared. It is passed as argument to |f|, which makes the parameter of |f|
  shared. The |n| is also an argument of |f|, and since the parameter of |f| is considered to be shared, it makes |n| shared as
  well, although |n| can be unique.

  This approach is the easiest way to deal with recursion and corresponds closely to how recursion is treated with conventional
  type inference.


\section{Helping the inferencer with manual constraint sets}
\label{sect.Helping}

  In Section~\ref{Sect.MonovariantRecursion}, we by-passed the problem of needing a constraint graph while building it, by removing the
  requirement on a constraint graph. This approach has limitations, although it is questionable if these limitations
  are ever a problem in practice. However, we can consider what the impact on the type system is when circumventing this limitation.

  Instead of dropping the requirement on constraint graph |A| while building |A|, we can change the requirement on |A| to some
  other graph |B|, which is at least as restrictive as |A|. This means that any valid substitution of |B| is also
  a valid substitution of |A|, such that it can replace |A|. When constructing the graph of |A|, the type system can just insert a copy
  of |B| where |A| is expected. This basically shifts the problem to another place, since we still need this graph |B|, but
  we come to that in a second.

  How can we tell the type system to use another graph for a certain identifier? The |Inst| constraint helps us again. Suppose
  that for some identifier |f :: utau|, we have a constraint set |S|, where |S| does not contain any |Inst| constraints. We then
  create some new binding group |n| for |S|, and for each use |f :: (Sub(utau)(2))|, generate an |Inst| constraint that refers to
  |n| and maps uniqueness variables of |utau| to |(Sub(utau)(2))|. Then the constraint graph obtained from |S| is used as
  replacement for the constraint graph of the binding group of |f|.

\subsection{Programmer specified constraint sets}
\label{sect.ProgSpecified}

  The constraint set is specified by the programmer. The programmer gives annotated types of the recursive identifiers in the binding group,
  and a constraint set. For example:

%%[[wrap=code
  f :: a:Int -> (1,1):Int, (1,1) =>= a
%%]

  Each type constructor is prefixed with a cardinality variable chosen by the programmer, a concrete cardinality value, or nothing. A
  concrete cardinality value |v| is converted into a freshly invented variable |delta|, together with a special equality
  constraint |v === delta|. We can represent the |===| constraint internally with two |=>=| constraints without coercions. Specifying
  no annotation has the same meaning as the concrete cardinality annotation |(Card(*)(*))|.

  To construct the desired constraint set |S|, the type given by the programmer is extended with cardinality variables for the concrete or
  missing annotations, and the constraint set of the user updated accordingly. By overlaying the type of the programmer on the type
  inferred for the identifier, the cardinality variables in the constraints are renamed to the actual cardinality variables, instead of
  the symbols of the programmer. The result is |S|.

\subsection{The problem with sacred cardinality variables}

  There is a problem with |S|. Binding groups are not independent. Sacred cardinality variables occur all over the place, and
  create links between binding groups. Unfortunately, the programmer cannot specify the effects on the sacred cardinality variables, since
  they do not have a name which the programmer can refer to. Conventional type signatures have this problem as well, as the following example
  shows:

%%[[wrap=code
let f :: forall alpha . alpha -> alpha
    f x =  let  y :: ???
                y = x
           in y
%%]

  In Haskell, we are unable to specify the type for |y| in this case. A solution is to give a name to the skolemnised
  version of type variable |alpha|, such that this name can be used in the signature of |y|. Unfortunately, this requires
  naming all sacred variables that are affected by the expression for which we want to specify the constraint set. I.e. this
  requires that we name the annotations on the type of each toplevel function that is used by the expression. That is infeasible,
  just because of the amount of annotations.

  On the other hand, if we assume that we only specify replacement signatures for top-level functions, then we only have dependencies
  on annotations of other toplevel definitions. These annotations are on the spine of toplevel functions or values. Since the
  idea is that toplevel functions and values are shared, we can suffice by automatically enforcing a |(Card(*)(*))| cardinality
  value for these annotations (by means of specifying some additional constraints during the constraint gathering process). This way,
  we assume that all toplevel values are fully shared and toplevel functions have a shared spine. As a result, it is impossible for any
  other definition to influence these annotations (they already have the highest value in the lattice). So, the programmer does not need
  to mention these annotations in the constraint set, which allows us to bypass the problem at the loss of precision for toplevel
  functions. This is not a very bad assumption to make, since from a design perspective, a function should only be
  toplevel if it is intended to be shared by multiple functions or modules. With this approach, constraint sets specified by the
  programmer are feasible.

\subsection{Entailment check}

  Unfortunately, there is another problem: Suppose we have such an |S|. How do we know that the programmer specified the right constraint
  set? For example, how can we be sure that the constraint set of the programmer does not forget a constraint between some cardinality
  variables? For that, the compiler has to prove that |S| \emph{entails} |S'|, where |S'| is the constraint set resulting from
  constraint gathering on the expression. This means that a substitution on the sacred uniqueness variables that is valid for
  |S|, is also valid for |S'|.

  As a side note, the word \emph{substitution} is a bit ambiguous here, since there are two solve phases. Normally, we talk about the final
  substitution, so the substitution resulting from the cardinality inference phase. In this case, however, it is enough that
  we can prove that |S| entails |S'|, for substitutions resulting from the lower and upper bound inference phase, since if |S'| is more
  restrictive in terms of bounds than |S|, it also is more restrictive in the cardinality values (ignoring soft coercion constraints).
  
  One attempt to verify this condition is to check the paths between sacred vertices. After reducing (Section~\ref{sect.Rewriting}) the graph 
  |S| to |R| and |S'| to |R'|, then |S| entails |S'| if each path in |R| is also a path in |R'| (assuming that hyper edges are reduced to
  canonical form). However, a constraint set specified by the user typically contains concrete annotations; these concrete annotations can
  hide a complete graph, such that the graphs are incomparable. This approach is not going to work.
  
  One way to prove that the constraint set is valid, is to enumerate each combination of upper/lower bound values and check the implication |S => S'|,
  but this is not doable in practice. For propositional logic, there are many satisfiability checkers that can help us out here, but our
  final constraint set (which only has coercion and aggregation constraints) is an expression in a three-valued logic for the upper bound. We can
  rewrite the upper bounds to propositional logic, but that results into many propositions.
    
  For each cardinality variable |delta|, we require two propositional symbols for the upper bound, |P| and |Q|:

  \begin{tabular}{l||ll}
  |delta| & |P| & |Q| \\
  \hline
  0 & 0 & 0 \\
  1 & 0 & 1 \\
  * & 1 & 0 \\
  - & 1 & 1 \\
  \end{tabular}

  With this scheme, we can construct truth-tables for the constraints, and construct a proposition from it:

  \begin{tabular}{ll||l||llll||l}
  |p| & |q| & |p =>= q| & |(Sub(p)(1))| & |(Sub(p)(2))| & |(Sub(q)(1))| & |(Sub(q)(2))| \\
  \hline
  0 & 0 & 1 & 0 & 0 & 0 & 0 & $\neg{p_1} \wedge \neg{p_2} \wedge \neg{q_1} \wedge \neg{q_2}$ \\
  0 & 1 & 1 & 0 & 0 & 0 & 1 & $\neg{p_1} \wedge \neg{p_2} \wedge \neg{q_1} \wedge q_2$ \\
  0 & * & 1 & 0 & 0 & 1 & 0 & $\neg{p_1} \wedge \neg{p_2} \wedge q_1 \wedge \neg{q_2}$ \\
  1 & 0 & 0 & 0 & 1 & 0 & 0 & $-$   \\
  1 & 1 & 1 & 0 & 1 & 0 & 1 & $\neg{p_1} \wedge p_2 \wedge \neg{q_1} \wedge q_2$ \\
  1 & * & 1 & 0 & 1 & 1 & 0 & $\neg{p_1} \wedge p_2 \wedge q_1 \wedge \neg{q_2}$ \\
  * & 0 & 0 & 1 & 0 & 0 & 0 & $-$   \\
  * & 1 & 1 & 1 & 0 & 0 & 1 & $p_1 \wedge \neg{p_2} \wedge \neg{q_1} \wedge q_2$ \\
  * & * & 1 & 1 & 0 & 1 & 0 & $p_1 \wedge \neg{p_2} \wedge q_1 \wedge \neg{q_2}$ \\
  \end{tabular}

  The propositional variant of the constraint is the disjunction of the above
  propositions.

  Instead of translating an arbitrary aggregation constraint directly, we split it first up
  into multiple, but smaller, aggregation constraints. Since the |\*/| is commutative, and
  the |<=| transitive, we can introduce some fresh cardinality variables, and shape
  the aggregation constraints into a tree form (see Section~\ref{sect.ite}).

  Each aggregation constraint has at most three cardinality variables this way. The translation
  scheme is then similar to the scheme for |=>=|, only three times as big.

  After converting the entire constraint set into propositions, tools from propositional
  logic are used to verify that the proposition holds, or to come up with a counter example.
  Converting such a counter example into a piece of information understandable by the
  programmer is not trivial, and left as a subject for further study.

\subsection{Final remarks about helping the inferencer}
\label{sect.RemarksHelp}

  Helping the inferencer by specifying a replacement constraint set, gives some mixed feelings.
  Technically it is possible, but the effort it takes to get it implemented, is excessively
  huge, compared to the gains.


\section{Full polyvariant recursion}
\label{sect.FullRecursion}

  There is a way to support full polyvariant recursion. The |Inst| constraints are a specification: is there a finite graph
  |G|, such that when a graph |G'| is constructed using |G| at the places of the corresponding |Inst| constraints, that
  |G'| is equivalent to |G|? The set of sacred annotations is finite, so according to the graph reduction section
  (Section~\ref{sect.Rewriting}, we know that any graph for these annotations can be reduced to a finite graph. A graph that always
  fits the specification, is the graph that has an edge between each pair of sacred annotations, and a hyper edge between each
  subset of sacred annotations, including with and without coercions. Of course, a smaller graph is desired.

  To obtain a smallest graph that satisfies the specification, we perform a fixpoint computation on the constraint graph. Each
  binding group initially has an empty graph, and the insertion of graphs at the |Inst| constraints is replayed for each iteration.
  The graphs are reduced at the end of the iteration. This process proceeds until the graphs do not change anymore (see Figure~\ref{fig.graphfix}).
  The graph reduction ensures convergence: hyper edges cannot grow indefinitely, and if one coercion is in the constraint set, then it will
  continue to be in the constraint set the next iteration, which guarantees that the number of coercions cannot grow indefinitely.

\begin{CodeFigure}{}{Fixpoint iterating inferencer}{fig.graphfix}

%%[[wrap=code
inferSubst constrSets
  =  let  initial  =  [((outermostBndgId, []), emptygraph)]
          graphs   =  iterate (\current -> foldr iteration current
                   .  keys
                   $  topsortByInstDeps current) initial
          fixpnt   =  snd . last . takeWhile (\(g,g') -> g /= g')
                   .  zip graphs
                   $  tail graphs
     in   infer (outermost fixpnt)
  where
    iteration key grs
      =  update grs key (construct key grs)
  
    construct key@(bndgId, idMap) grs
      =  let  cs   =  bndgId `lookup` constrSets
              cs'  =  rename idMap cs
              gr   =  lookupWithDefault (csToGraph cs') key grs
              gr'  =  foldr (instantiate grs bndgId) gr (insts cs')
         in   reduce gr' `union` gr  -- ensure monotonicity

    instantiate grs to (Inst from idMap tups) gr
      =  let  key   =  (from, idMap)
              src   =  lookupWithDefault emptygraph key grs
              src'  =  fresh (sacred to) tups src
              dst   =  src `union` gr
         in   dst
%%]
\end{CodeFigure}

  This approach is less obvious than it appears. The constraint reduction is essential: without it, the constraint graph can
  grow forever. In fact, without reduction, the hyper edges increase in size with each iteration. But, graph reduction does
  not reduce to a canonical form. A result of this is that monotonicity is not guaranteed. We solve this by taking the union
  of the newly computed (and reduced) graph for the binding group (|gr'|) with the previous (reduced) graph of the binding group (|gr|). This
  graph has two important properties. It can only stay the same or grow between iterations (monotonically increasing). It
  has the same vertices as both |gr| and |gr'| (due to reduction). Only the number of edges can grow between iterations, and
  the number of edges is bounded (depending on the number of vertices). These two properties ensure that a fixpoint is reached.

  Implementation of this procedure in the inferencer has only a slight impact. It does not influence the other operations of
  the inferencer much, and given functions that reduce a graph, and functions for dealing with the |Inst| constraint, implementing
  functionality as discussed in Chapter~\ref{chapt.Polyvariant}, this procedure is not very difficult to implement.

  But there is a downside: even with the reduction strategies, the graphs for even toy programs are big. Fixpoint
  iterations on these graphs require at least the procedure to be performed twice, and depending on the nesting level and
  amount of recursion, several more times. For most programs, this fixpoint iteration process will be overkill. The question
  is whether this approach will be worth it.

  We give this choice to the programmer. This choice is possible due to the |Inst| constraint. The programmer
  specifies for which binding groups a fixpoint strategy is required. The inferencer uses the implementation of this section
  as main inference strategy. All binding groups are put in the worklist in the earlier descried topologically sorted order
  (Section~\ref{sect.TheInferencer}). When a recursive |Inst| constraint is encountered while processing a binding-group, then depending on
  the choice of the programmer, either the quick equational approach of Section~\ref{Sect.MonovariantRecursion} is taken, or the iterative approach
  (Section~\ref{sect.FullRecursion}) by reinserting this binding group and the binding groups dependent on it, back in the work list. We can thus have
  best of both worlds, without a performance penalty if the fixpoint approach is never selected.
  
  However, if we support the fixpoint approach, we pay a price: allowing full user-defined constraint sets (Section~\ref{sect.Helping}) becomes
  a problem. We can no longer verify the constraint set given by the programmer beforehand, since the full constraint set is
  only known at the use-site of an identifier. We can check that the constraint set is correct for each use of an identifier,
  just by checking if the substitution for the use-site is also a valid substitution for the constraints obtained by augmenting
  the programmer-supplied constraint set with the gathered constraint set. Unfortunately, there are many subtleties, due to dependencies
  on other binding-groups, and differences between use-site type and definition-site type. Perhaps most important, the problem
  is detected at a very late time, with probably no reasonable way to relate the error back to a location in the constraint
  set specified by the programmer. Letting the programmer specify additional constraints, is not a problem, however (Section~\ref{sect.Signatures}).


\section{Aliassing and cyclic definitions}

  A question that arises is: does our approach correctly deal with aliassing and cyclic definitions. The following example demonstrates
  a possible problem:

%%[[wrap=code
  let  x    =  1 + f 3
       f z  =  z + y
       y    =  x
  in   f 1
%%]

  The fact that |x| is a cyclic definition is not directly visible, since |x| only occurs one. Our analysis can detect this situation:
  the spine of |f| is discovered to be shared, because |f| is used in the |in| expression and in the body of |x|. As a result, |y| is
  discovered to be shared, and subsequently |x| is discovered to be shared.

  If there is a cyclic definition, then there is at least an identifier |v| that occurs twice in the abstract syntax tree that causes
  the definition to be cyclic. Since our approach assumes that identifiers hidden by a function or hidden by an alias, have the same
  usage counts as the identifier that hides it, we in the end discover that the occurrences of |v| are individually used and that
  |v| itself is shared. Subsequently, this propagates to the aliassed and hidden identifiers, which ensures that any cyclic definition
  is discovered to be shared.

\section{Conclusion}

  The |Inst| constraint plays an important role in dealing with recursion. At least two different ways of dealing with this
  constraint are possible. One way is fast, but less accurate. One way is accurate, but slow. The constraint creates an
  abstraction barrier between the two ways, allowing both to exist side by side. In practice, the fast approach
  is virtually always sufficient, but in case the programmer really needs the additional power, it is possible to turn on
  the slow approach for specific parts of the program.

  This chapter also showed the need for constraint graphs and the corresponding reductions of Chapter~\ref{chapt.Polyvariant}. Without the
  reductions, the fixpoint iteration process is not possible for all but the tiniest programs. That said, this almost concludes
  the story about constraint graphs. Only Chapter~\ref{chapt.Polymorphic} about polymorphism will mention constraint graphs again,
  since instantiation to a type with more structure requires another form of duplication of graphs.

%%]
