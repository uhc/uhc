%%[main

\chapter{Adding algebraic data types}
\label{chapt.DataTypes}

We almost have an industry-ready language from a typing point of view. The major type system feature that we
lack is that of algebraic data types. Most of the machinery to support data types is already in place; only
support for pattern matches is missing.

Support for data types involves changes at both the expression level and the type level. The expression level needs to deal with pattern matches
and case-expressions. At the type level, the types become more complicated. Types have more annotations, dictated
by the types in an algebraic data type definition. Although algebraic data types influence a lot of components of the inferencer, the
required changes are surprisingly isolated. The constraint gathering phase needs to deal with case expressions and pattern matches.
The solver is unaffected. The only place that is heavily affected, is the conversion
to annotation trees (Section~\ref{sect.AnnotationTrees}). In order to perform this conversion, it turns out that we
have to perform two analyses on the data type definitions (Chapter~\ref{chapt.Polarity}).

This chapter is organised as follows. We start with an example that illustrates what the problems are to
support algebraic data types. Each of these problems is discussed in a subsequent section in isolation. The
contents of these sections require a fair understanding of the previous chapters.


\section{Example}

The example data type that we take in this chapter, are lists:

%%[[wrap=code
data List a  =  Cons a (List a)
             |  Nil
%%]

The algebraic data type definition dictates how the representation of a |List Int| in memory is (see Figure~\ref{fig.ListRep}). In this case, a
list is a region of memory representing a |Cons| or a |Nil|. In the first case, the area of memory
has two pointers, one to a value of type |a|, and a pointer to the remainder of the list. Each area
of memory has a corresponding type in the algebraic data type definition. The other way around, each type in
a algebraic data type definition, corresponds to zero or more memory regions of a value of that data type.
Similar to Section~\ref{sect.TpAnnConstr}, annotations on the type constructors of the data type, classify
the uniqueness properties of a piece of memory. This example also shows why annotations on the types
are useful: a value can be infinite, but the representation of the type is finite.

With the annotation problem taken care of, the existing infrastructure knows how to deal with
expressions that have user-defined data types as types. But, in order to construct and deconstruct such
values we need a few alterations in the expression language, by means of value constructors,
pattern matching, and case-expressions. Value constructors are just functions with a type derived
from the corresponding algebraic data type definition. We already know how to deal with those. But
pattern matches and case-expressions deserve some attention.

The annotations are important in the case of pattern matching. Pattern matching touches the spine of
a value, and if it succeeds, can bring identifiers into scope. Constraints needs to be generated
in order to properly flow use-counts into the annotations on the spine of data types. Annotations
for the types of the identifiers introduced by the pattern match, need to be obtained from
the type of the pattern and the corresponding algebraic data type definition (Section~\ref{sect.PatternMatch}).
For example, if some value has annotated type |(Annot(utau)(0))|, where |tau = List Int|, and we perform a successful
pattern match on |Cons (x :: (Annot(Sub(tau)(1))(3)) (xs :: (Annot(Sub(tau)(2))(4)))|, then each use
of |x| and |xs| can potentially result in a use of the spine of the value towards the location where
|x| or |xs| is stored, so we generate the constraints: |(Delta(3)) (Sub(=>=)(s)) (Delta(0))| and |(Delta(4)) (Sub(=>=)(s)) (Delta(0))|.
For the strict parts in the pattern, in this case the |Cons| constructor alone (with |(Delta(0))| being the corresponding
annotation), we generate a constraint |delta (Sub(=>=)(s)) (Delta(0))|, where |delta| is the number of uses of the value of the expression
containing the pattern match.

Case expressions are a generalisation of the |if..then..else| expressions of Chapter~\ref{chapt.Parallel}. We are not going to
explain this subject again here: the generalisation is straightforward after we know how to deal with pattern matches.


\section{Exposed annotations}
\label{sect.ExposedAnnotations}

  Data types complicate the annotation process. Consider the following algebraic data type definition:

%%[[wrap=code
  data Tuple = T Int Int
%%]

  Only the |Tuple| type constructor gets an annotation |delta|. But what are the annotations to give to both integers? There are several ways to deal
  with this situation. A straightforward way is to annotate each |Int| with |delta|. That way we treat the components of a data type the same
  as the data type itself. Although possible, this does not provide the accuracy we aim for. A second way is to ask the programmer for the
  annotations on the components of the type, and assume that the annotation is |(Card(*)(*))| if that is not the case. This is the approach taken by
  Clean~\cite{barendsen96uniqueness}. The third way is to expose the annotations on inner types such that the use-site of a data type can choose
  the annotations~\cite{Wansbrough:PhDThesis}:

%%[[wrap=code
  data (Dot(Tuple)([(Sub(delta)(1)), (Sub(delta)(2))]))(delta)) = T (Dot(Int)([])(Delta(1))) (Dot(Int)([])(Delta(2)))
%%]

  This way, the use-site can choose if the |Tuple| contains two unique integers, two strict integers, or
  perhaps a unique integer and an arbitrarily used integer. The list subscripted on a type constructor is called
  the set of exposed annotations. This list contains all annotations occurring inside the
  algebraic data type definition.

  When referencing a type constructor from another binding group, the exposed annotations are instantiated to
  fresh versions:

%%[[wrap=code
  data (Dot(IntWrap)([(Delta(2))])(Delta(1))) = Wrap (Dot(Int)([])(Delta(2)))
  data (Dot(IntTup)([(Delta(4)),(Delta(5)),(Delta(6)),(Delta(7))])(Delta(3))) = Tup (Dot(IntWrap)([(Delta(5))])(Delta(4))) (Dot(IntWrap)([(Delta(7))])(Delta(6)))
%%]

  This approach is slightly complicated in case of (mutual) recursive data types, since what list of
  annotations should we pass to the recursive type constructor? This is a familiar problem, for which there is
  a common solution: in a first pass, pick fresh annotations for type constructors from another
  binding group, and take the empty list for annotations from the same binding group. Collect
  all the exposed annotations for each binding-group. In the second pass, set the exposed annotations
  for type constructors of the same binding-group to the list of the entire binding-group. Now, each
  annotation occurring inside the algebraic data type definition, occurs in the list of exposed annotations of
  the corresponding type constructor.

  For example:

%%[[wrap=code
  data (Dot(InfIntList)([(Delta(1)),(Delta(2))])(delta)) = Cell (Dot(Int)([])(Delta(1))) (Dot(InfIntList)([(Delta(1)),(Delta(2))])(Delta(2)))
%%]

  But what about named type variables in a algebraic data type definition? The problem with type variables is that the
  structure of a type variable can be substituted to virtually any structure at the use-site of a
  data type, and we cannot give the annotations beforehand. But that is not required either: the annotated
  types passed as arguments to a type constructor, contain exactly the annotations that we did not know yet.
  For example:

%%[[wrap=code
  data (Dot(Embed)([])(delta)) a = Emb a
%%]

  The annotations to choose for |a| come from the use-site. For example, the type |(Dot(Embed)([])(Delta(1))) (Dot(Int)([])(Delta(2)))|
  specifies that a value of this type has |(Delta(1))| as annotation on the Emb constructor, and |(Delta(2))| on the integer
  that it contains.

  \subsection{Implementation}

  After obtaining the set of annotations for each data type, the implementation of this approach is straightforward
  after the work that we did for annotation trees in Section~\ref{sect.AnnotationTrees}. When annotating a type constructor in a
  type, pick fresh annotations for the exposed annotations, and when processing a constraints with these type in them,
  construct an annotation tree with the list of annotations as attachment on the corresponding node in the annotation
  tree. The treatment of exposed annotations in the annotation tree is the same as the other annotations in the
  annotation trees. There is a complication, however, since we need to know the variance of annotations and the fact
  if they are below a function arrow or not. These two subjects are more involved in the presence of data types.
  Chapter~\ref{chapt.Polarity} provides a way to obtain the required information about the location of annotations.

  The described approach is flexible and the changes are fairly isolated. There is a downside to this approach, which is
  that types can contain a lot of annotations this way. We can control this number of annotations by giving up accuracy, for
  example by enforcing that the annotations of a certain data type are never taken freshly. This saves annotations when
  the data type is used a lot, but has as consequence that there is only one uniqueness variant of the data type. All values
  of such a data type then have the same cardinality values, which is less flexible.

\section{Expanded types}
\label{sect.ExpandedTypes}

  The exposed annotations approach has limitations. If we look at a list, then each |Cons| in the list is
  treated the same way. We cannot make a distinction between certain elements of a list. On the other
  hand, the reason for taking a list (which is of varying size), is to treat each element in the same way, so that
  is not a big issue. A bigger limitation is that a type variable can only be used in one fashion. For example:

%%[[wrap=code
  data Tuple a = T a a
%%]

  In this example, both occurrences of |a| will get the same cardinality annotation, since we take the annotations from
  the type passed as |a| to |Tuple|. But in the memory representation of such a data type, each |a| is a separate area
  and thus can have different cardinalities. So, can we change the system such that we can treat occurrences of the
  same type variable differently?

  We can achieve this by using an ad-hoc trick: by unfolding the algebraic data type definitions a couple of times. For
  example, unfolding |Tuple Int| results into the type |<T Int Int>|, and can be annotated with annotations in the
  conventional way.  The value-constructor name does not need an annotation, but is part of the type in order to
  determine of which constructor the fields are an expansion. Type expansion is dangerous for recursive types, since
  the expansion process will not terminate, and the representation can explode quickly, especially for nested
  data types.

  The expansion approach and the exposed-annotations approach can be combined. The exposed-annotations approach can
  be used on any type, and the expansion approach on any type of kind |star|. By default, due to the dangerous nature
  of expansions, we do not perform expansions and require the programmer to specify when an expansion is allowed. We
  start with an expanded annotated type. Assume that |utau| is a type annotated by the exposed-annotations approach. Consider
  that during a topdown visit a type |(Sub(utau)(0))| is encountered, with outermost type constructor |T| and parameters |(Sub(utau)(1))|, ..., |(Sub(utau)(n))|.
  If |(Sub(utau)(0))| is of kind |star| and the occurrence of |T| in |utau| is allowed for expansion, then the definition
  of |T| is unfolded with annotations taken from |(Sub(utau)(i))| at each occurrence of type variable |i|. This
  procedure thus works by performing one-step expansions. In the end, we replace each individual variable by a fresh one.

  For example, consider the algebraic data type definition of |List|:

%%[[wrap=code
  data (Dot(List)([(Delta(2))])(Delta(1))) a  =  Cons a ((Dot(List)([(Delta(2))])(Delta(2))) a)
                                              |  Nil
%%]

  Suppose that we annotate the type |List Int|. By means of exposed annotations, we obtain the
  type:

%%[[wrap=code
  (Dot(List)([(Delta(4))])(Delta(3))) (Dot(Int)([])(Delta(5)))
%%]

  A one step expansion gives:

%%[[wrap=code
(Annot(<Cons (Dot(Int)([])(Delta(5))) ((Dot(List)([(Delta(4))])(Delta(4))) (Dot(Int)([])(Delta(5)))), Nil>)(3))
%%]

  Note how the exposed annotation |(Delta(4))| is used on its corresponding occurrence on the
  data type during the expansion. Expanding yet another step further, gives:

%%[[wrap=code
(Annot(<Cons (Dot(Int)([])(Delta(5))) (Annot(<Cons (Dot(Int)([])(Delta(5))) ((Dot(List)([(Delta(4))])(Delta(4))) (Dot(Int)([])(Delta(5)))), Nil>)(4)), Nil>)(3))
%%]

  We now end up with a type that is just an expanded version of the exposed annotated type. This type reflects how
  exposed annotations are mapped to the structure of an algebraic data type. We use this type for pattern matches,
  because we can easily 'overlay' it over constructors. For freshly annotating a type we go one step further and
  replace each individual annotation variable with a fresh one:

%%[[wrap=code
(Annot(<Cons (Dot(Int)([])(Delta(6))) (Annot(<Cons (Dot(Int)([])(Delta(7))) ((Dot(List)([(Delta(8))])(Delta(9))) (Dot(Int)([])(Delta(10)))), Nil>)(11)), Nil>)(12))
%%]

  This type can be used as expanded annotated type of |List Int|. Note that this type allows more precise results than
  the exposed annotated type, because the expanded annotated type allows the first two constructors to be annotated
  differently than the remainder of the list.

  These one-step expansions are not only used for annotating the data types, but also for expanding a type for a
  certain constructor in pattern matches, and in constructing an equivalent annotation tree structure if one
  type has exposed type, and the other an expanded type. So, in a way, the infrastructure that is needed to support
  expanded types, is already needed for the system itself.

  So, how does a programmer specify when a type may be expanded? We use \emph{path expressions}, which is a
  sequence of constructor names with wildcards. A type constructor |T| is allowed to be expanded, when |T|
  occurs in some expanded type, which as parents |(Sub(T)(1)), ..., (Sub(T)(n))|, and there is a prefix of
  a constructor sequence equal to |(Sub(T)(1)), ..., (Sub(T)(n)), T|. It must explicitly mention |T|, but
  it is allowed to have a wildcard for some of the constructor names.

  A path expression is defined by:

%%[[wrap=code
  path  ::=  T path   -- constructor
        |    *T path  -- wildcard
        |    empty
%%]

  A wildcard stands for any number of constructors not equal to the constructor after the wildcard. For
  example, the sequence |* List * List * List| specifies that a list may be unfolded thrice.

  With expanded types, two things can be accomplished: type constructors of kind |*| can get different
  annotations for each occurrence in the body of a data type, and the head of a data type can be
  annotated differently than the (possibly infinite) tail. This power comes at a price: the types grow
  and so does the number of annotations. Therefore, expanded types are turned off by default, only to be used
  in special circumstances, specified by the programmer using path expressions.

\section{Pattern matches}
\label{sect.PatternMatch}

  Pattern matches bring identifiers into scope, and force evaluation of a certain part of a value to
  determine if a pattern matches or not. We assume that we have only pattern matches in a |case| expression,
  not in left-hand sides of functions.
  
  The structure of a pattern match dictates how to pattern match on a type to obtain the type of
  the subpatterns. An expanded type (Section~\ref{sect.ExpandedTypes}) can directly be overlayed on top
  of a pattern match. An exposed-annotated type can be overlayed on a pattern match after a
  one-step expansion for each pattern match on a constructor. For example, if we take the one-step
  expansion of Section~\ref{sect.ExpandedTypes} as example, then a match |Cons x xs|, with the type of
  the entire pattern match being |(Annot(<Cons (Dot(Int)([])(Delta(5))) ((Dot(List)([(Delta(4))])(Delta(4))) (Dot(Int)([])(Delta(5)))), Nil>)(3))|,
  gives |(Dot(Int)([])(Delta(5)))| for |x| and |((Dot(List)([(Delta(4))])(Delta(4))) (Dot(Int)([])(Delta(5))))| for
  |xs|. So, for pattern matches, the annotated type is pushed inwards, expanding it if needed for
  each match on a constructor. This ensures that we have a definition-site type for every part of
  a pattern.

  We now discuss which coercion and aggregation constraints are generated for a pattern match. Take the
  following pattern as example:

%%[[wrap=code
  Cons x (Cons 3 y@(Cons _ ~(Cons z zs)))
%%]

  This pattern shows some possibilities for a pattern, such as aliasing (|y@...|) and
  irrefutable patterns (|~(Cons ...)|) and an underscore (|_|).

  If a pattern has a topmost constructor (in our case the |Cons| on the left), then this constructor is special: it has the same
  annotation as the outermost annotation of the type of the pattern. There is guaranteed to be an
  evaluation of a value corresponding to the match if all patterns in branches above this constructor have topmost
  constructors. These constructors have one thing in common: the corresponding type constructor (and thus the annotation) are
  equal. So the case expression will always lead to a weak-head normal form evaluation of the value which we match. Thus
  we can safely set the annotation corresponding to such a special constructor to the upper and lower bound of the
  enclosing expression.

  For the non-special parts of the pattern, we proceed as follows. The lower bound of each annotation is forced
  to |0|, except for function arguments or parameters, since we do not have guarantees of evaluation. The upper
  bound depends on the specific part of a pattern.

  Suppose that the pattern is a constructor. A constructor is irrefutable if it is inside a irrefutable pattern (|~|),
  otherwise it is refutable. For a refutable pattern we force the upper bound to at least the upper bound of the
  enclosing expression, since such a match only needs to be performed at most once for each evaluation of the
  |case| expression. This is not the case for an irrefutable pattern. For an irrefutable pattern, we can force
  the upper bound to at least the minimum of the upper bound of the enclosing expression, and the maximum of
  the upper bounds of all the variables inside the same irrefutable pattern. The reason is that the irrefutable
  pattern is evaluated at most as often as the enclosing expression, but can be less if none of the identifiers
  it brings in scope are used. If there is an alias connected to the constructor, then we generate an aggregation
  constraint between the annotation of the constructor and the corresponding annotation on the use-sites of such
  an alias.
 
  Suppose that the pattern is a variable or underscore. We force, by means of an aggregation constraint, the upper
  bound to at least the upper bound of the sum of the corresponding upper bounds from all aliases and the aggregation result of
  the individual uses of the variable. For the above example, we create an aggregation constraint for all uses of |z| and combine
  this aggregation with the annotations from the corresponding part of the definition site of |y|.

  Pattern matches are not really complicated, but present some technical details due to forced evaluation and
  patterns that introduce aliasing.


\section{Case expressions}

  Constraint gathering for case expressions is fairly straightforward now we know how to deal with
  pattern matches. For the pattern matches and the subexpressions, we apply the constraint
  gathering rules as discussed before. A freshly annotated type |utau| is generated for the result
  type |tau| of the case-expression. Each branch |i|, with annotated type |(Sub(utau)(i))|, is coerced
  to the result type of the case-expressions by generating the constraint |utau (Sub(=>=)(q)) (Sub(utau)(i))|.
  We discussed this particular subject in the previous chapter when we dealt with |if..then..else| expressions.

\section{|beta| annotations and constructors}

  What about the |beta| annotations that were mentioned in Chapter~\ref{chapt.NoBindings}? These |beta|
  annotations are basically constraint set variables, and are required for higher-ranked uniqueness types.
  A constraint set can be associated with each parameter to a constructor, so with each field of a data type.

  We therefore annotate each data type with a |beta| annotation for each field in one of the
  constructors. So, a data type with |n| fields gets annotations |(Sub(beta)(1)) ... (Sub(beta)(n))|. These
  |beta| annotations participate with the general unification process (just assume they are additional
  phantom type variables).

  From the data type definition, we derive an annotated signature from the data type definition, using the
  conventional approach to get a signature from a data type definition, except that we take the annotated
  types of the fields.

  Consider the |List| data type. The algebraic data type definition of a |List| has two fields, so the
  |List| has two |beta| annotations. It also has one exposed annotation |delta|. This gives us the
  following annotated algebraic data type declaration (in a slightly different notation):

%%[[wrap=code
forall beta1 beta2 .
  forall delta1 delta2 .
    forall alpha .
      data (Dot(List)([delta1,delta2] [beta1,beta2])(delta1)) alpha  =  Cons (Sub(alpha)(beta1)) (Sub(((Dot(List)([delta1,delta2])(delta2)) alpha))(beta2))
                                                                     |  Nil
%%]

  From this definition, we generate the signature for |Cons| in the obvious way:

%%[[wrap=code
forall beta1 beta2 .
  forall delta1 delta2 .
    forall alpha .
      Cons :: alpha (Sub(->)(beta1)) ((Dot(List)([delta1,delta2] [beta1,beta2])(delta2)) alpha (Sub(->)(beta2)) ((Dot(List)([delta1,delta2] [beta1,beta2])(delta1)) alpha)
%%]

  This allows us to use constructors as arbitrary functions. Also note the interaction with the |beta| annotations.
  We can put functions into lists, but the |beta| annotations will ensure that we still know which constraints
  belong to a function:

%%[[wrap=code
xs = (Cons f (Cons g (Cons h Nil)))
%%]

  Since we assume that |beta| annotations are part of the underlying type system, unifications on |beta| annotations
  take place when constructing the list, so that |beta1| is the same for each |Cons|. This means that the
  constraint sets of |f|, |g|, and |h| are merged into a single constraint set. When pattern matching on a function
  in this |List|, we do not know which of the three functions it is. But the constraint set that we get is the
  merged version of the three constraint sets, which is at least as restrictive than any of the individual
  functions, thus correct.

\section{Conclusion}

  Algebraic data types are an interesting language feature in terms of what complications they give in order to
  support them. Good abstractions are required in the implementation, otherwise code soon becomes
  so interconnected that it is no longer feasible to grasp. Our approach with constraint gathering,
  constraint graph construction with annotation trees, and constraint solving, provides ways of splitting
  concerns in relatively small pieces.

  An important aspect of the approach taken in this chapter is that the part that is concerned with
  the analysis itself (constraint solving), is almost not affected. This means that algebraic data types do not
  require us to change our way of reasoning about uniqueness typing, and only complicate the analysis on
  a technical level, instead of a conceptual level. Perhaps the work on algebraic data types can be reused
  for other analyses as well.

  One aspect that we did not cover in this chapter, is how we determine the variance of an annotation,
  and whether or not the values it classifies are parameters or values of a function. We discuss this
  in Chapter~\ref{chapt.Polarity}.


\chapter{Polarity and Under-the-arrow analysis}
\label{chapt.Polarity}

Chapter~\ref{chapt.DataTypes} introduces data types. For annotations on the type constructors of these
data types, we need to know what their variance is and if they occur as function parameter or value. In
this chapter we discuss how to infer this information from the data type definitions and the types in
the program. For that, we reuse the entire analysis that we explained in this thesis, and apply it to
the type language instead of the expression language. Notice that this chapter is not about uniqueness
typing, but about a whole other analysis!

\section{Introduction}
\label{sect.VarIntro}

  In this section we introduce the concepts of covariance and below-the-function-arrow on simple types
  without data types.

  An annotation is \emph{covariant} if it only occurs on type constructors that are covariant. Likewise, an annotation
  is \emph{contra-variant} if it only occurs on type constructors that are contra-variant. When we do not know the
  difference, then it is called \emph{variant}. Without algebraic data types, a type is either an |Int|, or the function type |tau -> tau|.
  Suppose that we also have tuples |(tau, tau)|. We then define the variance as follows:

%%[[wrap=code
  variance t = var t CoVariant

  var (Sup(Int)(delta)) c         = [(delta, c)]
  var (t1 (Sup(->)(delta)) t2) c  = [(delta, c)] ++ var t1 (neg c) ++ var t2 c
  var (t1 (Sup(,)(delta)) t2) c   = [(delta, c)] ++ var t1 c ++ var t2 c

  neg  CoVariant      = ContraVariant
  neg  ContraVariant  = CoVariant
%%]

  Effectively, this definition means that the variance of a function argument is the inverse of a
  function result. Apply the |variance| function to |(((Annot(Int)(1)) (Annot(->)(4)) (Annot(Int)(2))) (Annot(->)(5)) (Annot(Int)(3)))|
  to discover that |(Delta(3))| and |(Delta(5))| are covariant, that |(Delta(2))| and |(Delta(4))| are contra variant (because of the negation),
  and that |(Delta(1))| is covariant again.

  We give a similar definition for below-the-function arrow. An annotation is \emph{below-the-function-arrow} if
  it only occurs on type constructors that are below-the-function-arrow. Likewise, an annotation
  is \emph{not-below-the-function-arrow} if it only occurs on type constructors that are not-below-the-function-arrow. If it is neither, then we
  assume that it is both. Again, we can give a definition in terms of types without algebraic data types:

%%[[wrap=code
  below t = bel t NotBelow
  bel (Sup(Int)(delta)) c         = [(delta, c)]
  bel (t1 (Sup(->)(delta)) t2) c  = [(delta, c)] ++ bel t1 Below ++ bel t2 Below
  bel (t1 (Sup(,)(delta)) t2) c   = [(delta, c)] ++ bel t1 c ++ bel t2 c
%%]

  For the example |(((Annot(Int)(1)) (Annot(->)(4)) (Annot(Int)(2))) (Annot(->)(5)) (Annot(Int)(3)))|, this definition means that
  |(Delta)(1)|, |(Delta(2))|, |(Delta(3))|, and |(Delta(4))| are below the function arrow, and |(Delta(5))| is not.

  But, these are just definitions for a type language without algebraic data types. Algebraic data types complicate these concepts because it depends
  on the definition of a data type what is done with the type arguments of a type constructor. We demonstrate this by an example
  in the following section.

\section{Example}

  Suppose we have a slight alteration of the commonly known |GRose| data type, which we will call |FRose|:

%%[[wrap=code
  data (Dot(FRose)(Delta(2))(Delta(1))) f a = FBranch (f a ((Dot(FRose)(Delta(1))(Delta(1))) f a))
  FRose :: forall a . (a -> * -> *) -> a -> *
%%]

  And consider the following types:

%%[[wrap=code
  (Dot(FRose)(Delta(4))(Delta(3))) (Annot((,))(5)) (Annot(Int)(6))
  (Dot(FRose)(Delta(8))(Delta(7))) (Annot((->))(9)) (Annot(Int)(10))
%%]

  We now ask ourself the following questions:

  \begin{itemize}
  \item What is the variance of |(Delta(6))| and |(Delta(10))|?
  \item If the type will be expanded indefinitely, will |(Delta(6))| and
        |(Delta(10))| always occur below a |(->)|? If that is the case, then |(Delta(10))|
        always occurs on type constructors of types of values that are function arguments or results.
  \end{itemize}

  An analysis on the types is required to answer these questions. For |(Delta(6))|
  we see that the type definition represents an right-infinitely deep tuple:

%%[[wrap=code
  ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ... )))))
%%]

  Applying the variance rules given in Section~\ref{sect.VarIntro} results in |(Delta(6))| in being covariant
  and not below a function arrow.

  The inverse is case for |(Delta(10))|. The type with |(Delta(10))| represents the type of
  a function with infinitely many parameters:

%%[[wrap=code
  ((Annot(Int)(10)) -> ((Annot(Int)(10)) -> ((Annot(Int)(10)) -> ((Annot(Int)(10)) -> ((Annot(Int)(10)) -> ... )))))
%%]

  Applying the techniques of Section~\ref{sect.VarIntro} again, we see that for the variance, each |(Delta(6))|
  is a multiple of two negations away from an other |(Delta(6))|, and that if there would be a final |(Delta(6))|,
  that it is the result of a function, thus all occurrences of |(Delta(6))| are covariant. As is visible in this
  example, all occurrences of |(Delta(6))| are below a function arrow.
  
  How are we going to infer these results? What we see here is that the analyses work by pushing some known information
  topdown into a definition. This is exactly what we did with the propagation of the upper bounds. Similarly, combining the
  results of each individual use-site to one single definition-site, is what we did with the aggregation constraint. So, by
  performing the analysis on the type level, instead of the expression level, on kinds instead of types, we can use the
  results of previous chapters to solve the problem of this chapter. For that, we only need different constraint gathering
  rules (although they look a lot like those for uniqueness typing) and a different interpretation in the solver.


\section{Constraint gathering}

  Constraint gathering is performed on the type level. A type expression does not differ much compared to a normal
  expression, except that there are no lambda abstractions. Instead of lambda abstractions, there are algebraic data type
  definitions in the declarations of a Let.
  
  The type of a type is called a kind, denoted by |kappa| (see Section~\ref{sect.PolymorphicKinds}). We allow
  a kind to be annotated, similarly to a type. An annotated kind is denoted by |ukappa|, following the same conventions
  as for types in Section~\ref{sect.TpAnnConstr}.

  Figure~\ref{RulerUniquenessExamples.DC.tp.base} lists the typing rules for constraint gathering on type expressions. We
  omit the cases for extensible records and expanded types, these cases clutter up the type rules, but do nothing more than
  collect the constraints for the type expressions occurring in them.
  
  \rulerCmdUse{RulerUniquenessExamples.DC.tp.base}

  Figure~\ref{RulerUniquenessExamples.UX.expr.base} has a lot of resemblance with Figure~\ref{RulerUniquenessExamples.DC.tp.base}.
  At each use of a type constructor, we generate an
  |Inst| constraint. Uses of type variables do not need constraints, similar to lambda-bound identifiers for
  expressions. Finally, at type applications, there are two coercion constraints generated to
  flow results from parameter to argument, and from type expression result to function result. This way, we connect
  the annotations on the kinds from the root of the type expression to the leaves.

  How we treat annotations that occur in the type, we make explicit with a separate case for annotations on a
  type constructor. This is a subtle rule in the sense that it connects two annotations that live in different
  worlds: one annotation lives in the kind world and one annotation lives in the type world. An annotated type constructor is encoded as the alternative |Ann| just above the alternative
  |Con|. For example |Ann delta (Con "Int" [])| represents |(Dot(Int)([])(delta))|. In the type rule, we connect
  the annotation on the type constructor to the topmost annotation on the annotated kind of the type constructor.
  We want to know the results of the analysis for the annotations occurring on types, but the analysis only finds
  results for the annotated kinds. With the constraint generated for the annotations, the results of kinds are
  propagated to the actual annotations. From an implementation point of view, this approach makes sure that
  an algebraic data type definition collects proper constraints for exposed annotations.

  We omit the type rules for binding groups and algebraic data type definitions and give only a textual description instead. For an
  algebraic data type definition,
  the constraints for all the types in the constructors are collected. The environment is enlarged with annotated kinds
  of the type variables, derived by means of pattern matching on the annotated kind of the data type. An aggregation
  constraint is generated for each type variable that combines each use-site of the type variable. These constraints
  are then collected in binding-groups and passed to the solver. Types of an expression outside an algebraic data type
  declaration, each get their own binding group.


\section{Constraint interpretation}

  To solve the constraints, we refer back to Chapter~\ref{chapt.Polyvariant}, and especially Section~\ref{sect.TheInferencer}. Instead
  of two solver phases, we only need a single solver phase.

  We consider variance first. There is a problem with variance in the sense that some of the propagation constraints are
  not negations, and negations are not represented by the constraint gathering in Section~\ref{Sect.ConstraintGathering}. But, the
  negations only occur in the constraint set of the function arrow, and thus we only have to put this new constraint
  into the initial constraint set:

%%[[wrap=code
  (->) :: (Annot(*)(1)) -> (Annot(*)(2)) -> (Annot(*)(3)), (Delta(3)) =>= (Delta(2)), (Delta(3)) (Sub(=>=)(/=)) (Delta(1))
  Int  :: (Annot(*)(4))
%%]

  This new constraint has some slight complications on the constraint graph, however, since care has to be taken that
  simplification of an odd number of negation edges, results still in an odd number of negation edges. However, this
  is not much more involving than determining soft paths, so it is only a minor complication, especially because we
  can eliminate hyper edges for the analysis in this chapter entirely (Section~\ref{sect.vargraphsimplification}).

  The infrastructure remains the same. This may raise the question how we deal with variance and below-the-function-arrow
  when converting the constraints of this chapter to constraint graphs. We do not need an extensive analysis for this:
  the kind language is simple enough that we can obtain variance information with a variation of the functions given
  in the introduction of this chapter. Values for belowness are fixed at |NotBelow| as the concept of belowness is not important
  for the analysis on algebraic data types.

  So, the only big change is in the interpretation of the constraints. We interpret the constraints to find a
  substitution. A substitution maps an annotation to some analysis result. In case of variance, we use for
  this the following lattice |leqV|:

%%[[wrap=code
  ?  leqV  +
  ?  leqV  -
  +  leqV  +/-
  -  leqV  +/-
%%]

  Then the interpretation is defined as follows:

%%[[wrap=code
variance (a =>= b)
  = a =>= (a `(Sub(join)(leqV))` b)
variance (a (Sub(=>=)(/=)) b)
  = a =>= (neg a `(Sub(join)(leqV))` b)
variance ((Sub(a)(1)) \*/ (Sub(a)(n)) <= a)
  =  let  z = (Sub(a)(1)) `(Sub(join)(leqV))` ... `(Sub(join)(leqV))` (Sub(a)(n)) `(Sub(join)(leqV))` a
     in   z \*/ ... \*/ z <= z

neg +  = -
neg -  = +
neg x  = x
%%]

  For the below-the-function-arrow analysis, no special constraints are needed. But, the initial substitution needs
  to be adapted for a special annotation |delta| that represents that it is below the arrow. Then the initial constraint
  set for below-the-function-arrow analysis is:

%%[[wrap=code
  (->) :: (Annot(*)(1)) -> (Annot(*)(2)) -> (Annot(*)(3)), delta =>= (Delta(1)), delta =>= (Delta(2))
  Int  :: (Annot(*)(4))
%%]

  The lattice |leqBL| that we use is similar to that of variance:

%%[[wrap=code
  ?          leqBL  below
  ?          leqBL  not_below
  below      leqBL  both
  not_below  leqBL  both
%%]

  Again, we formulate the interpretation function in a familiar way:

%%[[wrap=code
belowSolveF (a =>= b)
  = a =>= (a `(Sub(join)(leqBL))` b)
belowSolveF ((Sub(a)(1)) \*/ ... \*/ (Sub(a)(n)) <= a)
  =  let  z = (Sub(a)(1)) `(Sub(join)(leqBL))` ... `(Sub(join)(leqBL))` (Sub(a)(n)) `(Sub(join)(leqBL))` a
     in   z \*/ ... \*/ z <= z
%%]

  The result is that the constraints specify the traversal over the data types, and that the interpretation function
  perform the analysis that we did by hand in the introduction of this chapter.


\section{Graph simplification}
\label{sect.vargraphsimplification}

  As we mentioned before, the graph simplification can remain largely unaltered. However, we can convert an aggregation
  constraint for these analysis into coercion constraints. This has the advantage that the graph reducer does not need
  to deal with aggregation constraints. A constraint of the form |a \*/ b <= c| means |a == b == c| in the interpretation,
  thus we can replace it with |a =>= b =>= c| and |c =>= b =>= a|. The graphs for data types are even more sparse than those
  of expressions, which means that the resulting graphs are very small. This is important, because the above analysis is applied
  to all types occurring in constraints.

\section{Conclusion}

  This chapter shows that the results of previous chapters can not only be used for uniqueness typing, but for
  two `completely different' analyses as well, without much additional work. We did not compare the performance
  of the variance approach to a non-constraint based variance approach, but we think that due to the simplifications
  on the constraint graphs, that our approach is not much slower than a direct implementation.

%%]
