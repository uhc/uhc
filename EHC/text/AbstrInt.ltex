\documentclass[preprint,9pt]{sigplanconf}

%include lhs2tex.fmt

\def\spacecorrection{\;}
\def\isspacecorrection{\spacecorrection}
\def\allowforspacecorrection#1{%
  \gdef\temp{#1}%
  \ifx\isspacecorrection\temp
    \let\next=\empty
  \else
    \let\next=\temp
  \fi
  \next}



\newcounter{enumctr}
\newenvironment{enumate}{%
\begin{list}{\arabic{enumctr}}{
\usecounter{enumctr}
\parsep  = 0pt
\parskip = 0pt
\topsep  = 0pt
\itemsep = 0pt
}}{\end{list}}
\newenvironment{itize}%
{\begin{list}%
  {$\bullet$%
  }%
  {\parsep  = 0pt%
   \parskip = 0pt%
   \topsep  = 0pt%
   \itemsep = 0pt%
  }%
}%
{\end{list}%
}



%format GrModule  = "\mathit{Module}"
%format GrGlobalL = "\mathit{GlobalL}"
%format GrGlobal  = "\mathit{Global}"
%format GrBindL   = "\mathit{BindL}"
%format GrBind    = "\mathit{Bind}"
%format GrExpr    = "\mathit{Expr}"
%format GrAltL    = "\mathit{AltL}"
%format GrAlt     = "\mathit{Alt}"
%format GrTermL   = "\mathit{TermL}"
%format GrTerm    = "\mathit{Term}"
%format GrPatAlt  = "\mathit{PatAlt}"
%format GrPatLam  = "\mathit{PatLam}"
%format GrVarL    = "\mathit{VarL}"
%format GrVar     = "\mathit{Var}"
%format GrTag     = "\mathit{Tag}"
%format HsName    = "\mathit{Name}"
%format getNr     = "\mathit{nr}"
%format GrTag_Con     = "\mathit{Tag\_Con}"
%format GrTag_Fun     = "\mathit{Tag\_Fun}"
%format GrTag_PApp    = "\mathit{Tag\_PApp}"
%format GrTag_App     = "\mathit{Tag\_App}"
%format GrTag_Unboxed = "\mathit{Tag\_Unboxed}"
%format GrTag_Any     = "\mathit{Tag\_Any}"

%format Data = "\mathbf{data}"
%format DATA = "\mathbf{syntax}"
%format TYPE = "\mathbf{type}"
%format SET  = "\mathbf{set}"
%format ATTR = "\mathbf{attr}"
%format SEM  = "\mathbf{sem}"
%format USE  = "\mathbf{use}"
%format SYN  = "\mathbf{syn}"
%format INH  = "\mathbf{inh}"
%format .    = "."
%format ^    = " "
%format ^^    = "\;"
%format ^@    = "@"
%format LET  = "\mathbf{let}"
%format IN   = "\mathbf{in}"

%format @ = "\spacecorrection @"
%format [          = "[\mskip1.5mu\allowforspacecorrection "
%format (          = "(\allowforspacecorrection "
%subst fromto b e t     = "\fromto{" b "}{" e "}{{}\allowforspacecorrection " t "{}}'n"





\usepackage{amsmath}

\usepackage{natbib}
\bibpunct();A{},
\let\cite=\citep
\bibliographystyle{plainnat}



\begin{document}

\conferenceinfo{ICFP '07}{September 30, Freiburg.} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

%\titlebanner{Working copy v.1}        % These are ignored unless
%\preprintfooter{Working copy v.1}   % 'preprint' option specified.

\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt}


\title{Abstract Interpretation to Avoid Indirect Jumps}
\subtitle{A Complete Description based on Attribute Grammars}

% \authorinfo{Jeroen Fokker\and S.~Doaitse Swierstra}
%            {Utrecht University}
%            {\{jeroen,doaitse\}@@cs.uu.nl}

\authorinfo{XXXXXXXXXXXXX\and YYYYYYYYYYYYYY}
           {ZZZZZZZZZZZZ University}
           {\{xxxxxxx,yyyyyyy\}@@zzzzzzzz}


\maketitle

\begin{abstract}
Implementations of lazy functional languages often use 
closures, that is heap cells denoting postponed function 
calls. When a closure is forced to evaluation, the function 
code is called. The usual implementation involves an 
indirect jump to this code at evaluation points. Indirect 
jumps are expensive on pipelined architecture processors and 
should therefore be avoided. An alternative implementation 
evaluates closures by case analysis on the possible 
functions it represents. To keep the number of cases small, 
for each evaluation point the possible contents of the 
closure needs to be known. This paper describes the abstract 
interpretation algorithm that precisely determines 
the possible contents of all variables.

The algorithm does a tree walk over the program, collecting 
constraints on program variables. The set of constraints is 
then solved with fixpoint iteration. The description of the 
algorithm is a complete implementation, with no details left 
out. As such, it is a case study of the possibility to do so 
within the limits of a short paper. We use many abstraction 
mechanisms of Haskell to keep the description concise. On 
top of that, we use a preprocessor to be able to describe 
the algorithm in terms of catamorphisms and algebras.
A description of this attribute grammar based preprocessor 
is also included.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%keyword1, keyword2

\section{Introduction}

Early implementations of lazy functional languages
were usually based on graph rewriting techniques.
Substitution is done in evaluation of a $\beta$-redex
\[
   (\lambda x\;.\;b)\;a \;\;\Rightarrow \;\; b\;[\;x\;/\;a\;]
\]
either directly by walking through a copy of $b$ and
replacing occurences of $x$,
or indirectly by compilation to 
SKI-combinators \cite{turner} or super-combinators \cite{hughes}.

Later, other approaches were taken
which compile to an abstract machine model
based on stacks and continuations.
The instructions of this `STG-machine' can be mapped to
those of traditional hardware architectures \cite{stgmachine}.

Lazy evaluation is implemented by, instead of calling a function directly,
building `closures' of functions, 
i.e.\ heap records containing a reference to the function and to its arguments.
Such a closure can be forced to evaluation in situations where 
the result is actually needed,
viz.\ when it is used in a case-expression, or passed in a strict argument position.

In a naive implementation, the function reference would be a tag,
and there is a special evaluation function which does a case distinction on this tag.
Peyton Jones describes a better encoding, where the tag is actually 
a pointer to an information table, which in turn contains a pointer to the code of the function \cite{stgmachine}.
Evaluating a closure now amounts to just calling that code.
The double indirection in this encoding can be reduced to a single indirection
by having the `tag' point directly to the code, and putting the
rest of the information table just before that code in memory \cite{marlow}.

Either way, evaluation involves calling code through an indirection pointer.
On modern pipelined processors, this is a very costly operation, as
it stalls the pipeline.
Therefore, Boquist proposed to return to the naive encoding.
To avoid the overhead of calling the evaluation function which does the
case distinction between tags, the evaluation function is `inlined'
whenever used.
To prevent copying the large body of the evaluation function,
in each occurence the case analysis is pruned to contain only those cases
that can actually occur in that particular instance \cite{boquist1999}.
This way, evaluation amounts to a few tests and conditional jumps,
and indirect jumps are avoided completely.
Branch prediction schemes that are built in in pipelined processors
can deal with the conditional jumps efficiently.
Also, the conditional jumps are very local, and are likely to have their target
within the instruction cache.

To be able to do the pruning, 
it is necessary to know for each function closure what its possible tags are.
This can be determined by a global control flow analysis.
In an earlier paper, Boquist sketches an algorithm for
this abstract interpretation \cite{boquist1996}.
Unfortunately, the actual implementation of the algorithm
seems to be lost in history.


We re-engineered the abstract interpretation algorithm
to incorporate it in an experimental Haskell compiler \cite{ehc}
and to extend it to handle exceptions \cite{douma}.
As the algorithm is vital to this approach to avoid
indirect jumps, we carefully documented the implementation,
which is presented in this paper.

Algorithms are often described in some mathematical formalism.
A problem of mathematical notation is that it lacks sophisticated
data structures.
We feel it is paradoxical that for describing, 
e.g., the subtleties of the Haskell type system, 
one often uses mathematical notation which itself is almost untyped.
So, where a mathematical description ought to be more abstract
than an implementation, sometimes it is cluttered with low-level
encodings of data structures in terms of lists and tuples.

A way out of this paradox is to use Haskell itself as the description language.
We consider a paper like `Typing Haskell in Haskell' \cite{thih}
to be better readable than many a formal treatise on the same subject.
An added benefit is that the description is actually executable code,
which makes an error-prone translation from specification to implementation obsolete.

To be useful as an algorithm description intended for human readers,
an implementation should of course abstract from trivial details as much as possible.
Although Haskell has many mechanisms for abstraction, we think that for
tree processing algorithms it is helpful to use notions derived
from the realm of attribute grammars \cite{knuth}.
In order not to loose executability of our implementation,
we use a preprocessor that translates the attribute grammar notions to plain Haskell.
In order to be self-contained, we include a description of this preprocessor as well.

The aim of this paper is twofold:
\begin{enumate}
\item (technical) to give a concise, executable description of the 
      abstract interpretation algorithm that is needed to avoid indirect jumps
      when evaluating a closure in a lazy functional language;
\item (methodological) to provide a case study for the use of Haskell
      and attribute grammar related techniques for the description of an algorithm.
\end{enumate}
The reader is invited to judge whether the 7 pages we use for the actual algorithm
description in section~\ref{sec.ai} could have been much shorter
when using other formalisms.
Before that, we introduce the language to be analyzed in section~\ref{sec.lang},
and the attribute grammar preprocessor to Haskell in section~\ref{sec.ag}.



\section{Tree walk methodology}\label{sec.ag}

\subsection{Defining semantics}

% If a single phenomenon needed to be nominated as the
% most fundamental aspect of computer science, it would
% be the ability to generalize from specific instances
% through abstraction and parameterization.
% Every programming language has a way to define 
% subroutines, procedures, functions, methods, or other
% manifestations of abstraction, and to call them with
% appropriate parameters.

Functional languages are famous for their ability to 
parameterize functions not only with numbers and data structures,
but also with functions and operators.
The standard textbook example involves the functions |sum| and |product|,
which can be defined separately by tedious inductive definitions:
\begin{code}
sum      []      = 0
sum      (x:xs)  = x + sum xs
product  []      = 1
product  (x:xs)  = x * product xs
\end{code}
but, once this pattern has been generalized in a function |foldr|
that takes as additional parameters the base value and the operator to apply
in the inductive case:
\begin{code}
foldr op e []      = e
foldr op e (x:xs)  = x `op` foldr op e xs
\end{code}
could easily have been defined as specializations of the general case:
\begin{code}
sum      = foldr (+)  0
product  = foldr (*)  1
\end{code}
Indeed, good generalizations might have unexpected applications in other domains:
\begin{code}
concat     =  foldr (++) []
sort       =  foldr insert []
transpose  =  foldr (zipWith (:)) (repeat [])
\end{code}
The idea that underlies the definition of |foldr|, i.e.\ to capture the pattern
of an inductive definition by adding a function parameter for each constructor of
the data structure, can also be used for other data types, and even for
multiple mutually recursive data types.
Functions that can be expressed in this way were called {\em catamorphisms}
by Bird, and the collective extra parameters to |foldr|-like functions 
an {\em algebra} \cite{bird,birdmoor}. 
Thus, |((+),0)| is an algebra for lists, and |((++),[])| is another.
In fact, every algebra defines a {\em semantics} of the data structure.
When applying |foldr|-like functions to the algebra consisting of the original constructor functions,
such as |((:),[])| for lists, we have the identity function.
Such an algebra is said to define the `initial' semantics.

Outside circles of functional programmers and category theorists, an
algebra is simply known as a `tree walk'.
In compiler construction, algebras could be very useful to define
a semantics of a language, or bluntly said to define tree walks over the parse tree.
The fact that this is not widely done, is due to the following problems:

\begin{enumate}
\item Unlike lists, which have a standard function |foldr|, in a compiler we deal with
      (many) custom data structures to describe the abstract syntax of a language, 
      so we have to invest in writing a custom |fold|
      function first. Morover, whenever we change the abstract syntax,
      we need to change the |fold| function, and every algebra.
\item Generated code can be described as a semantics of the language, but often
      we need additional semantices: pretty-printed listings, warning messages,
      and various derived structures for internal use (symbol tables etc.).
      This can be done by having the semantic functions in the algebra return
      tuples, but this makes them hard to handle.
\item Data structures for abstract syntax tend to have many alternatives,
      so algebras end up to be clumsy tuples containing dozens of functions.
\item In practice, information not only flows bottom-up in the parse tree,
      but also top-down. E.g., symbol tables with global definitions need
      to be distributed to the leafs of the parse tree to be able to evaluate them.
      This can be done by making the semantic functions in the algebra
      higher order functions, but this pushes handling algebras beyond human control.
\item Much of the work is just passing values up and down the tree.
      The essense of a semantics is sparsely present in the algebra,
      and obscured by lots of boilerplate.
\end{enumate}

Many compiler writers thus end up writing ad hoc recursive functions
instead of defining the semantics by a algebra,
or even resort to non-functional techniques.
Others succeed in giving a concise definition of a semantics,
often using proof rules of some kind, but thereby loose the executability.
For the implementation they still need conventional techniques,
and the issue arises whether the program soundly implements
the specified semantics.

To save the nice idea of using an algebra for defining a semantics,
we use a preprocessor for Haskell that overcomes the abovementioned problems \cite{agsyst}.
It is not a separate language; we can still use Haskell for writing
auxiliary functions, and use all abstraction techniques and libraries available.
The preprocessor just allows a few additional constructs, which are translated
into a custom |fold| function and algebras.


\subsection{An Attribute Grammar based preprocessor for Haskell}

We describe the main features of the preprocessor here, and explain why they overcome
the five problems mentioned above.
To start with, the abstract syntax of the language is defined in a |DATA| declaration,
which is like a Haskell |Data| declaration with named fields.
The difference is that we don't have to write braces and commas,
and that constructor function names need not to be unique.
As an example, we define a fragment of a typical imperative language:
\begin{code}
DATA Stat
  =  Assign  dest   :: String  ^^  ^^  ^^  ^^  src   :: Expr
  |  While   cond   :: Expr    ^^  ^^  ^^  ^^  body  :: Stat
  |  Group   elems  :: [Stat]
DATA Expr
  =  Const   num   :: Int
  |  Var     name  :: String
  |  Add     left  :: Expr     ^^  ^^  ^^  ^^  right  :: Expr
  |  Call    name  :: String   ^^  ^^  ^^  ^^  args   :: [Expr]
\end{code}
The preprocessor generates corresponding |Data| declarations
(making the constructors unique by prepending the type name, like |Expr_Const|),
and more importantly, generates a custom |fold| function. This overcomes problem 1.

For any desired value we wish to compute over a tree, we can declare a `synthesized attribute'.
Attributes can be defined for one or more data types.
For example, we can define that both statements and expressions need to 
synthesize bytecode as well as pretty-printed listing, and that expressions
can be evaluated to an integer value:
\begin{code}
ATTR Expr Stat  SYN bytecode  :: [Instr]
                SYN listing   :: String
ATTR Expr       SYN value     :: Int
\end{code}
The preprocessor will ensure that the semantic functions will return appropriate
tuples, but in our program we can simply refer to the attributes by name.
This overcomes problem 2.

The value of each attribute needs to be defined for 
every constructor of every data type which has the attribute.
As this defines the semantics of the language, these definitions
are known as `semantic rules', and start with keyword |SEM|.
An example is:
\begin{code}
SEM Stat | Assign
  @lhs.listing =  @dest.listing ++ ":=" ++ 
                  @src.listing ++ ";"
\end{code}
This states that the synthesized |listing| attribute
of an assignment statement can be constructed
by combining the |listing| attributes of its |dest| and |src| children
and some fixed strings.
The |@| symbol in this context should be read as `attribute',
not to be confused with Haskell `as-patterns'.
At the left of the |=| symbol, the attribute to be defined is mentioned;
at the right, any Haskell expression can be given.
The |@| symbol may be omitted in the destination attribute,
as is done in the next example. 
This example shows that it is indeed useful that any Haskell expression,
with embedded occurrences of child attributes, can be used in the definition.
Also, it shows how to use the value of terminal symbols (|@num| in the example),
and how to group multiple semantic rules under a single |SEM| header:
\begin{code}
SEM Stat | While
  lhs.bytecode =  let  k = length @cond.bytecode
                       n = length @body.bytecode
                  in   @cond.bytecode
                       ++ [BEQ (n+1)]
                       ++ @body.bytecode
                       ++ [BRA (-(n+k+2))]
SEM Expr
  | Const  lhs.value = @num
  | Add    lhs.value = @left.value + @right.value
\end{code}
The preprocessor collects and orders all definitions into a single algebra,
replacing the attribute references by suitable selections from the results 
of the recursive tree walk on the children. 
This overcomes problem 3.

To be able to pass information downward during a tree walk,
we can define `inherited' attributes
(the terminology goes back to \cite{knuth}).
As an example, it can serve to pass an environment,
i.e.\ a lookup table that associates variables to values,
which is needed to evaluate expressions:
\begin{code}
TYPE Env = [(String,Int)]
ATTR Expr INH env::Env
SEM Expr | Var
  lhs.value = fromJust (lookup @lhs.env @name)
\end{code}
The value to use for the inherited attributes can be defined
in semantic rules higher up the tree:
\begin{code}
SEM Stat | Assign
  src.env = [ ("x",37), ("y",42) ]
\end{code}
The preprocessor translates inherited attributes into
extra parameters for the semantic functions in the algebra.
This overcomes problem 4.

In the example above, an environment with two variables was just made up.
In reality, a |Stat| construct probably inherited the environment
from even higher constructs, say a procedure declaration.
This means that the only thing that needs to be done at the |Stat| level,
is to pass the inherited environment down to the children.
This can be quite tedious to do:
\begin{code}
SEM Stat
  |  Assign  dest.env  = lhs.env
             src.env   = lhs.env
  |  While   cond.env  = lhs.env
             body.env  = lhs.env
\end{code}
Luckily, the preprocessor has a convention that, 
unless stated otherwise, attributes with the same name
are automatically copied. So, the attribute |env| that
a |Stat| inherited from its parent, is automatically copied
to the children which also inherit an |env|, and the tedious rules
above can be omitted.
A similar automated copying is done for synthesized attributes,
so if they need to be passed unchanged up the tree, this needs
not to be explicitly coded.

When more than one child offers a candidate to be copied,
normally the last one is taken.
But if we wish a combination of the copy candidates
to be used, we can specify so in the attribute declaration.
An example is:
\begin{code}
ATTR Expr Stat
  SYN listing USE (++) []
\end{code}
which specifies that by default, the synthesized
attribute |listing| is the concatenation of the |listing|s of
all children that have one, or the empty list if no child has one.
This defines a useful default rule, which can be overridden
when extra symbols need to be interspersed, as for example in
the definition of |listing| for assignment statements given earlier.

It is allowed to declare both an inherited and a synthesized attribute
with the same name. In combination with the copying mechanisms,
this enables us to silently thread a value through the entire
tree, updating it when necessary. See section~\ref{sec.collect}
which maintains, in attribute |location|,
a unique counter during the tree walk.
This captures a pattern for which often |Reader| and
|Writer| monads are introduced \cite{thih}.

The preprocessor automatically generates semantic rules
in the standard situations described, and this overcomes problem 5.




\section{The Grin language}\label{sec.lang}

The Grin language (Graph Reduction Intermediate Notation)
was proposed by Boquist as an intermediate language sitting
between the Core language (that in Haskell compilers describes a desugared program)
and an imperative backend \cite{boquist1999}.

We describe a slightly modified version here, which is
more explicit than Boquist's original description
about what constructs are allowed at various places.
Instead of the usual BNF description, we introduce the
language by means of Haskell data type declarations
(or rather |DATA| declarations for the AG preprocessor).
The advantage of this approach is that this explicitly mentions
the types and names of child constructs of each nonterminal symbol.
Also it is part of our endeavour to make the description to serve
both as the specification and as the implementation of the abstract semantics of the language.

The semantics/interpretation that we deal with in this paper
is an abstract interpretation needed for analysis of the program.
In the same style we are able to present other semantices.
In our compiler \cite{ehc} we implement a translation to bytecode that
is executable by a simple interpreter, and a translation
to a generic imperative language that can in turn be translated
to various backend languages.

In the presentation of the language we do not provide
a concrete syntax for the language, as normally is implicitly
done in a BNF description.
One reason for this is that a concrete syntax is unnecessary,
as Grin programs are only an intermediate representation
in the compilation process, and technically are merely data structures.
Another reason is that the mental parsing and unparsing involved
when reading the semantics description in later sections could
distract the reader from the algorithm proper, and cause confusion
between program fragments as data structures and their semantic values.

We start our description with a definition of toplevel constructs.
A program consists of a single module, which has a name,
a list of global variable definitions, and a list of function bindings.
Note that in our naming, we conventionally use suffix |L| for `list',
and prefix |mb| for `maybe'.
\begin{code}
DATA Program
  = Prog      mod          :: GrModule
DATA GrModule
  = Mod       nm           :: HsName
              globalL      :: GrGlobalL
              bindL        :: GrBindL
TYPE GrGlobalL  =   [GrGlobal]
TYPE GrBindL    =   [GrBind]
\end{code}
A global definition binds a name to a term,
whereas a lambda binding binds a parameterized name
to an expression.
\begin{code}
DATA GrGlobal
  = Global  nm              :: HsName
            val             :: GrTerm
DATA GrBind
  = Bind    nm              :: HsName
            argNmL          :: [HsName]
            expr            :: GrExpr
\end{code}
Grin programs manipulate five kinds of values:
integers, nodes with a known tag and a list of fields,
standalone tags, pointers to a node stored on the heap,
and the empty value.
The first three have a direct syntactic
representation as a |GrTerm|, pointers and the empty value have not.
Another possible |GrTerm| is a variable, which can refer to any of the five kinds of value.
\begin{code}
DATA GrTerm
  =  LitInt  int    :: Int
  |  Tag     tag    :: GrTag
  |  Node    tag    :: GrTag
             fldL   :: GrTermL
  |  Var     nm     :: HsName
TYPE GrTermL  =  [GrTerm]
\end{code}
Although the syntax above allows fields of a |Node| be any |GrTerm|,
we do not make use of nested nodes;
if they are desired, the field list should contain
variables that point to heap cells storing the inner nodes.

Tags label nodes.
Six different tags are possible.
Most tags are |Con| tags, which label nodes
that build up data structures in Grin.
They correspond to constructor functions in the Haskell source program,
but unlike constructor functions, nodes with a |Con| tag are always fully saturated.
For implementing lazy evaluation we have a |Fun| tag, which is needed
to construct function `thunks', i.e.\ function applications of which the evaluation is postponed.
Nodes with a |Fun| tag are always fully saturated functions.
For unsaturated functions (partial parameterization), a |PApp| tag is used,
which, apart from the function name, also specifies the number of 
parameters it still |needs| to become fully saturated.
\begin{code}
DATA GrTag
  |  Con   nm        :: HsName
  |  Fun   nm        :: HsName
  |  PApp  needs     :: Int
           nm        :: HsName
  |  App
  |  Unboxed
  |  Hole
\end{code}

The other three tags are an extension to those proposed by Boquist.
When a lazy call is needed to a function of which the name is not statically known,
a special thunk node is used. It has tag |App|;
the first field of the node represents the function,
the other fields the arguments to which the function is applied when the thunk is forced to evaluate.
The |Unboxed| tag is a mockup tag for constructs that conceptually are nodes,
but in reality are implemented as unboxed values.
Finally, the |Hole| tag is used in the implementation of recursive definitions,
but plays no special role in the analysis described in this paper.

The most ubiquitous construct in a Grin program is an expression, which
represents the body of function bindings.
An expression can be evaluated to a value, during which side effects on the heap may occur.
There are twelve cases in the expression syntax:
\begin{code}
DATA GrExpr
  =  Seq          expr            :: GrExpr
                  pat             :: GrPatLam
                  body            :: GrExpr
  |  Unit         val             :: GrTerm
  |  UpdateUnit   nm              :: HsName
                  val             :: GrTerm
  |  Case         val             :: GrTerm
                  altL            :: GrAltL
  |  FetchNode    nm              :: HsName
  |  FetchUpdate  src             :: HsName
                  dst             :: HsName
  |  FetchField   nm              :: HsName
                  offset          :: Int
                  mbTag           :: Maybe GrTag
  |  Store        val             :: GrTerm
  |  Call         nm              :: HsName
                  argL            :: GrTermL
  |  FFI          nm              :: String
                  argL            :: [HsName]
                  tagL            :: GrTagL
  |  Eval         nm              :: HsName
  |  Apply        nm              :: HsName
                  argL            :: GrTermL
\end{code}
We give an informal description of the semantics of these constructions,
that is their runtime evaluation result and side effects.
A formal description would be a Grin interpreter, which is not the focus of this paper.

An expression |Unit val| simply evaluates to a known value |val|.
Evaluation of expression |Seq expr pat body| first evaluates |expr|,
binds the result to |pat| and in the extended environment evaluates |body|.
Boquist uses a monadic style concrete syntax for this construct:
|expr ; \pat -> body|, which is why we declared |pat| to have type |GrPatLam|
(for `lambda pattern').
It can however just as well be thought of as |LET pat=expr IN body|
or even as an imperative style assignment |pat:=expr; body|.
Concrete syntax is immaterial; what is important is that |expr| and |body|
are evaluated sequentially.

Boquist proposes two constructs which have a side effect on the heap:
|Store|, which stores a node value in a new heap cell and returns a pointer to it,
and |Update|, which stores a node value in an existing heap cell and returns the empty value.
We do have a |Store| expression in our language, 
but instead of a separate |Update| expression we have |UpdateUnit|,
which combines the overwriting of an existing heap cell with returning the value.
This allows for a more efficient implementation of the combination.
Boquist uses a single construct |Fetch| for fetching either a complete node,
or a particular field of a node. Because these two variants behave quite differently,
we have separate constructs |FetchNode| and |FetchField|, and a |FetchUpdate|
which combines fetching a node and using it to update an existing heap cell.

Next, we have a construct |Call| for calling a Grin function,
and |FFI| for calling a foreign function.
Boquist proposes the use of two builtin functions |eval| and |apply|,
which can be called to force evaluation of a variable,
or to force application of an unknown function in a strict context, respectively.
As these functions behave quite different from ordinary functions,
we choose to include special constructs |Eval| and |Apply| for these cases.

Finally, there is a |Case| construct which selects from a list
of alternatives the one that has a pattern that matches 
the value of the variable in the |Case| header (the `scrutinee').
Each alternative consists of a pattern and a corresponding expression:
\begin{code}
DATA GrAlt
  | Alt  pat   :: GrPatAlt
         expr  :: GrExpr
TYPE GrAltL = [GrAlt]
\end{code}
Patterns in a case alternative normally consist of a node
with a known tag, and variables as arguments.
Stand-alone tags and literal integers are also possible patterns:
\begin{code}
DATA GrPatAlt
  =  LitInt      int             :: Int
  |  Tag         tag             :: GrTag
  |  Node        tag             :: GrTag
                 fldL            :: [HsName]
\end{code}
A pattern in a case alternative is quite different from
a lambda pattern in a |Seq| expression.
A lambda pattern is often just a variable name.
Two other possibilities are |Empty|, to be able to match
for the empty result value of the |FetchUpdate| expression that
only has a side effect, and a node denotation where
the tag can, but needs not be, known:
\begin{code}
DATA GrPatLam
  =  Empty
  |  Var         nm              :: HsName
  |  VarNode     fldL            :: GrVarL
DATA GrVar
  =  Var         nm              :: HsName
  |  KnownTag    tag             :: GrTag
TYPE GrVarL     =   [GrVar]
\end{code}
We assume the existence of a special name
\begin{code}
wildcard :: HsName
\end{code}
which can serve as a
`don't care variable' in a lambda pattern.

To complete our exposition of the Grin language, we
define abbreviations for some groups of nonterminal symbols,
which will facilitate the definition
of attributes that are needed for all of them:
\begin{code}
SET AllDef    =  GrGlobal GrGlobalL GrBind GrBindL
SET AllTerm   =  GrTerm GrTermL
SET AllExpr   =  GrExpr GrAlt GrAltL
                 GrPatAlt GrPatLam GrVar GrVarL
\end{code}



\section{Abstract interpretation}\label{sec.ai}

In this section we describe an abstract interpretation
algorithm for Grin programs.
The algorithm solves a set of constraints by
fixpoint iteration.
Constraints are first collected in a walk
over the tree that represents the Grin program.
We start with a description of an abstract domain,
and a language for specifying the constraints.

\subsection{An abstract domain}

Grin programs largely consist of bindings
from Grin expressions to function names.
Expressions in turn are built from terms,
of which a possible form is a single variable.
Although Grin is an untyped language,
in code generated from a correct Haskell program
variables always refer to values of the same kind:
basic values, nodes, tags, or heap pointers.
We will use abstract interpretation not only 
to infer these kinds, but also to collect more detailed 
information about the runtime structure of values.

When executed, a Grin program maintains a heap of
dynamically allocated nodes.
More specifically, execution of a |Store| expression
allocates a new heap cell, as do |Global| variable definitions.
Our abstract interpretation algorithm will
also determine, for each |Store| expression and
each |Global| definition, what type of node it can create.
The abstraction of all heap cells that a particular
|Store| or |Global| creates is known as a |Location|.
Thus, each |Location| corresponds uniquely to
a |Store| or |Global|. In our implementation we
identify locations simply by unique, consecutive numbers:
\begin{code}
type Location = Int
\end{code}
Grin variables refer to five different
kinds of value:
the empty value,
integers,
standalone tags,
pointers to a heap location,
or complete nodes.

We introduce a data type |AbstractValue| to describe
the abstract domain in the abstract interpretation.
It distinguishes four cases for the five different kinds of value (both the empty value and integers are regarded as `basic'), 
with added bottom and error cases to form a complete lattice
suitable for fixpoint iteration.
\begin{code}
data AbstractValue
  =  AbsBottom
  |  AbsBasic
  |  AbsTags   (Set GrTag)
  |  AbsLocs   (Set Location)
  |  AbsNodes  (Map GrTag [AbstractValue])
  |  AbsError  String
\end{code}
In the |AbsTags| case, abstract interpretation reveals
to which subset of all possible tags a variable can refer.
Similarly, for |AbsLocs| we determine to which locations
a pointer can point.
In the |AbsNodes| case, we not only determine the possible
tags of the nodes, but for each of these also a list of the abstract values of their parameters.
In section~\ref{sec.lang} we stipulated that nested nodes are only allowed
by letting the fields be variables which refer to pointers to heap cells storing the inner nodes.
This invariant propagates to |AbsNodes|: the elements of the fields of a node are
never |AbsNodes| themselves, but can be |AbsLocs| pointing to locations which store inner nodes.
  
The fact that |AbstractValue| indeed forms a lattice
is expressed by the following definition,
which specifies how two abstract values can be merged into one.
We state that |AbsBottom| is the identity of a |Monoid|
\begin{code}  
instance Monoid AbstractValue where
    mempty  =  AbsBottom
\end{code}
That is, any abstract value remains unchanged when merging it with |AbsBottom|
\begin{code}
   ^^  mappend  a          AbsBottom   =  a
       mappend  AbsBottom  b           =  b
\end{code}
Abstract values of each of the four types can be merged with others of the same type:
\begin{code}
   ^^  mappend AbsBasic AbsBasic     
         =  AbsBasic
       mappend (AbsTags  at) (AbsTags  bt) 
         =  AbsTags (Set.union at bt)
       mappend (AbsLocs  al) (AbsLocs  bl) 
         =  AbsLocs (Set.union al bl)
       mappend (AbsNodes an) (AbsNodes bn) 
         =  AbsNodes  (Map.unionWith 
                        (zipWith mappend) an bn)
\end{code}
Errors remain errors even when merged with other values:
\begin{code}
     mappend a^@(AbsError _ ) _  =  a
     mappend _ b^@(AbsError _ )  =  b
\end{code}
New errors originate from merging abstract values from incompatible types:
\begin{code}
     mappend a b  
       =  AbsError (show a ++ " conflicts " ++ show b)
\end{code}  
  
  
  
  
  
The goal of the abstract interpretation algorithm is
to determine the abstract value of each variable
in the program, and likewise for each abstract heap |Location|.
For efficiency reasons we represent these mappings by arrays:
\begin{code}
type AbstractEnv s  
  =  STArray s Variable AbstractValue
type AbstractHeap s 
  =  STArray s Location AbstractValue
\end{code}
Like a |Location|, each |Variable| is also represented by a number:
\begin{code}
type Variable = Int
\end{code}
A preprocessing stage uniquely numbers all variable names in a program
(taking care of scoping where necessary),
and makes the sequence number available through a function
\begin{code}
getNr :: HsName -> Variable
\end{code}


\subsection{A constraint language}\label{sec.constraintlang}

By observing a Grin program, we can deduce equations
which constrain variables and locations.
Before doing so, we need a language to specify such constraints.
We introduce the type |Equation| for describing six different
types of constraints for the abstract value of variables.
Likewise, we have |HeapEquation| for specifying constraints
on the abstract values of abstract heap locations.
\begin{code}
data Equation
  =  IsKnown          Variable  AbstractValue
  |  IsSuperset       Variable  Variable
  |  IsSelection      Variable  Variable Int GrTag
  |  IsConstruction   Variable  GrTag [Maybe Variable]
  |  IsEvaluation     Variable  Variable
  |  IsApplication    (Maybe Variable) [Variable]
\end{code}
Five out of the six equation types constrain a variable to fulfil certain properties.
Only in the case of an |IsApplication| equation, 
the variable that is constrained appears |Maybe|, i.e.\ is optional.

A variable may be constrained by more than one equation.
These equations are cumulative. 
If for example one constraint specifies that a variable `is known'
to have a particular abstract value, 
and another constraint specifies that it is known to have
another value, the abstract interpretation algorithm will
conclude that this variable can refer to either value.


Below we informally describe the semantics of the six equation types.
A formal description is given in figure~\ref{fig.envChanges},
which is discussed in section~\ref{sec.solution}.
\begin{enumate}
\item 
An equation
|IsKnown v a| means that variable |v| can have abstract value |a|.
\item
The meaning of |IsSuperset v w| is that variable |v| can have all 
values that variable |w| has. 
\item
The equation |IsSelection v n i t| expresses that |v| can be the 
selection of the |i|th component of any node tagged by |t|
which can be the value of variable |n|.
\item
The meaning of |IsConstruction v t as| is that |v| can be
a node with tag |t| and arguments |as|. Not all arguments need to be known.
\item
The meaning of |IsEvaluation v w| is that |v| can refer to the
evaluation result of any possible value of |w|.
\item
The meaning of |IsApplication v (f:as)| is that 
|f| is a variable that refers to a function which is applied to
values referred to by variables |as|,
and that the result is a possible value of |v|.
For this type of constraint, mentioning a variable |v| is optional.
If it is lacking, the equation still bears information
on the possible values of parameters of |f|.
\end{enumate}

For heap equations, we have only one constraint type:
\begin{code}
data HeapEquation
  =  WillStore Location GrTag [Maybe Variable]
\end{code}
The meaning of |WillStore p t as| is that location |p|
stores a node with tag |t| and arguments |as|.
A heap cell always stores a complete node,
not an isolated value of other type 
(basic value, tag or pointer to another heap cell).

The sets of constraints for variables and locations, respectively,
are collected in lists, for which we define the following types:
\begin{code}  
type Equations      = [Equation]
type HeapEquations  = [HeapEquation]
\end{code}



\subsection{Collecting constraints in a tree walk}\label{sec.collect}

In this subsection we describe a tree walk over a Grin program
that collects constraints on the program variables.
The tree walk is implemented using the attribute grammar (AG) based
language described in section~\ref{sec.ag}.

The goal of the tree walk is to synthesize |equations| stating
the constraints for program variables, and |heapEqs| stating
the constraints for locations (abstract results of store expressions
and global definitions).
\begin{code}
ATTR Program GrModule AllDef AllExpr
  SYN  equations  USE (++) []  :: Equations
  SYN  heapEqs    USE (++) []  :: HeapEquations
\end{code}
The declarations above specify that both type of equations
are not only synthesized for the whole program, but also for
the intermediate levels of the program tree that have to do
with definitions and expressions. No equations are synthesized
on the levels that have to do with values and variables.
The |USE| clause in the declaration of the attributes
expresses that the default way to synthesize
equations is just to concatenate the equations synthesized on
underlying levels. We will redefine the |equations| and
|heapEqs| attributes later for the tree positions where
equations are introduced.

First, we introduce some auxiliary attributes.
We need to uniquely number all abstract locations,
as we represent locations by integers.
For this purpose we have both a synthesized and
an inherited attribute |location| for all relevant 
positions in the tree.
With a semantic rule, value 0 is inserted for this
attribute at the top of the tree.
\begin{code}
ATTR Program GrModule AllDef AllExpr
  INH SYN  location  ::  Int
SEM Program | Prog
  mod.location = 0
\end{code}
The AG preprocessor ensures that the inherited attributes are
passed unchanged down the tree, and the synthesized values are passed up,
unless there is a semantic rule which specifies that a modified value should
be passed.
Indeed, in figure~\ref{fig.equations} we have rules that increment
the location counter when locations need to be numbered,
viz.\ at |Store| expressions and |Global| definitions.

Before we explain the rest of the rules in figure~\ref{fig.equations},
we define an auxiliary data structure needed
as the type of some attributes to come.
Nodes sometimes are indirectly referred to by a variable, 
sometimes they are directly enumerated in full.
The following data type distinguishes these two cases, 
where the polymorphic type variable |a| is the type of additional
information that we may want to express for the parameters of the node.
Function |fromInVar| retrieves the variable from
a |NodeInfo| that is known to be a |InVar| case.
\begin{code}
data NodeInfo a 
  =  InVar   Variable 
  |  InNode  GrTag [a]
 
fromInVar :: NodeInfo a -> Variable
fromInVar (InVar v)  = v
\end{code}
This data type is the type of attributes |termInfo| and |patInfo|
that summarize whether terms and patterns are denoted indirectly through 
a variable, or directly as a node with tag and fields:
\begin{code}
ATTR  GrTerm    SYN termInfo  :: NodeInfo (Maybe Variable)
ATTR  GrPatAlt 
      GrPatLam  SYN patInfo   :: NodeInfo Variable
\end{code}
Some auxiliary attributes are necessary to make the summary:
\begin{code}
ATTR  GrTerm   SYN  var      :: Maybe Variable
ATTR  GrTermL  SYN  vars     :: [Maybe Variable]
ATTR  GrVar    SYN  tag      :: GrTag
               SYN  var      :: Variable
ATTR  GrVarL   SYN  hdTag    :: GrTag 
               SYN  vars     :: [Variable]
\end{code}
The semantic rules for these attributes are straightforward:
\begin{code}
SEM GrTerm
| Tag        lhs.termInfo  =  InNode  @tag []
| Var        lhs.termInfo  =  InVar   (getNr @nm)
| Node       lhs.termInfo  =  InNode  @tag @fldL.vars
SEM GrPatAlt
| Tag        lhs.patInfo   =  InNode  @tag []
| Node       lhs.patInfo   =  InNode  @tag (map getNr @fldL)
SEM GrPatLam
| Empty      lhs.patInfo   =  InVar   wildcard
| Var        lhs.patInfo   =  InVar   (getNr @nm)
| VarNode    lhs.patInfo   =  InNode  (@fldL.hdTag)
                                      (tail @fldL.vars)
SEM GrTerm
| Var        lhs.var       =  Just    (getNr @nm)
| * - Var    lhs.var       =  Nothing
SEM GrTermL
| Cons       lhs.vars      =  @hd.var : @tl.vars
| Nil        lhs.vars      =  []
SEM GrVarL  
| Cons       lhs.hdTag     =  @hd.tag
SEM GrVarL
| Cons       lhs.vars      =  @hd.var : @tl.vars  
| Nil        lhs.vars      =  []
SEM GrVar
| KnownTag   lhs.tag       =  @tag
| Var        lhs.var       =  getNr @nm
\end{code}
The |patInfo| attribute defined above 
determines the target of each expression.
For most expressions, the target is the next
pattern in the sequence. For the last expression in a
sequence that is the body of a function, the target
is the function name bound in a |Bind| binding,
and passed all the way through the |Seq| spine.
This is expressed in the following semantic rule:
\begin{code}
ATTR AllExpr 
  INH targetInfo :: NodeInfo Variable
SEM GrBind | Bind  
  expr.targetInfo  =  InVar (getNr @nm)
SEM GrExpr | Seq    
  expr.targetInfo  =  @pat.patInfo
  body.targetInfo  =  @lhs.targetInfo
\end{code}
The |termInfo| attribute defined earlier occurs in the semantics rules 
for various expression forms in figure~\ref{fig.equations}.
The |termInfo| attribute value synthesized by the scrutinee term
of a |Case| expression is also needed in the alternatives
of that |Case| expression.
It is therefore passed down as an inherited attribute
to the alternatives: 
\begin{code}
ATTR GrAlt GrAltL 
  INH termInfo :: NodeInfo (Maybe Variable)
\end{code}
No explicit semantic rules are needed here, as the AG system automatically
routes the value synthesized by the first child of
a |Case| expression (the scrutinee) as the value
of the inherited attribute with the same name of its second 
child (the list of alternatives).

\begin{figure*}
\begin{code}
SEM GrExpr | Unit UpdateUnit
  loc.equations1   = case (@lhs.targetInfo, @val.termInfo) of
                       (InVar tvar        ,  InVar svar        )  -> [IsSuperset tvar svar]
                       (InVar tvar        ,  InNode stag snms  )  -> [IsConstruction tvar stag snms Nothing]
                       (InNode ttag tnms  ,  InVar svar        )  -> buildSelectEquations svar ttag tnms
                       (InNode ttag tnms  ,  InNode stag snms  )  -> buildUnifyEquations  snms tnms
SEM GrExpr | UpdateUnit
  loc.equations2   =  [ IsSuperset (getNr @nm) (getNr @val.getName) ]
SEM GrExpr | Unit
  lhs.equations    =  @loc.equations1
SEM GrExpr | UpdateUnit
  lhs.equations    =  @loc.equations2 ++ @loc.equations1

SEM GrAlt | Alt  
  lhs.equations    =  case (@pat.patInfo, @lhs.termInfo) of
                        (InNode ttag tnms, InVar svar)  -> buildSelectEquations svar ttag tnms

SEM GrExpr | FetchNode
  lhs.equations    =  case @lhs.targetInfo of
                        InVar tvar  ->  [ IsSuperset  tvar          (getNr @nm)   ]
SEM GrExpr | FetchUpdate                ^             ^             ^             ^
  lhs.equations    =                    [ IsSuperset  (getNr @dst)  (getNr @src)  ]
SEM GrExpr | FetchField
  lhs.equations    =  case @lhs.targetInfo of
                        InVar tvar  ->  [ IsSelection tvar (getNr @nm) @offset (fromJust @mbTag) ]
  
SEM GrExpr | Store  
  lhs.location     =  @lhs.location + 1  
  lhs.heapEqs      =  case @val.termInfo of
                        InNode stag snms  -> [ WillStore @lhs.location stag snms ]
  lhs.equations    =  case @lhs.targetInfo of
                        InVar tvar        -> [ IsKnown tvar (AbsLocs (Set.singleton @lhs.location)) ] 
SEM GrGlobal | Global 
  lhs.location     =  @lhs.location + 1
  lhs.heapEqs      =  case @val.termInfo of
                        InNode stag snms  ->  [ WillStore @lhs.location stag snms ]
  lhs.equations    =                          [ IsKnown (getNr @nm) (AbsLocs (Set.singleton @lhs.location)) ]
    
SEM GrExpr | Call  
  lhs.equations    =  case @lhs.targetInfo of
                        InVar  tvar       -> [ IsSuperset tvar (getNr @nm) ]
                        InNode ttag tnms  -> buildSelectEquations (getNr @nm) ttag tnms

SEM GrExpr | FFI
  loc.nodemap      =  Map.fromList ( [ (con, [ AbsBasic | con==GrTag_Unboxed ] ) | con <- @tagL ] )
  lhs.equations    =  case @lhs.targetInfo of
                        InVar tvar        -> [ IsKnown tvar (AbsNodes @loc.nodemap) ]
                        InNode ttag tnms  -> zipWith IsKnown tnms (fromJust (Map.lookup ttag @loc.nodemap))

SEM GrExpr | Eval
  lhs.equations    =  case @lhs.targetInfo of
                        InVar tvar  -> [ IsEvaluation tvar (getNr @nm) ]

SEM GrExpr | Apply  
  lhs.equations    =  case @lhs.targetInfo of
                        InVar tvar ->  [ IsApplication (Just tvar) (getNr @nm : map fromInVar @argL.valsInfo) ]
\end{code}
\caption{Definition of constraint equations for various expression types (discussed in section~\ref{sec.collect})}
\label{fig.equations}
\end{figure*}



We are now ready to discuss the twelve syntactic positions where
equations originate, as defined in figure~\ref{fig.equations}.
In the case of a |Unit| or |UpdateUnit| we distinguish
the four combinations of target pattern (variable or node)
and source term (variable or node). When both are variables, the
target is constrained to hold the same value as the source;
when the target is a variable and the source is a node, 
the target should be able to hold that node. If, on the contrary, the
target is a node with explicit arguments, and the source is a
variable, all the arguments of the node that are not wildcards should be projections of the source
variable.
These constraints are generated by the following auxiliary function:
\begin{code}
buildSelectEquations 
  :: Variable -> GrTag -> [Variable] -> Equations
buildSelectEquations svar ttag tnms
  = [  IsSelection tvar svar i ttag
    |  (tvar,i) <- zip tnms [0..]
    ,  tvar /= wildcard
    ]
\end{code}
Finally, when both target and source are full nodes,
corresponding arguments should unify.
This is handled by another auxiliary function:
\begin{code}
buildUnifyEquations 
  :: [Maybe Variable] -> [Variable] -> Equations
buildUnifyEquations snms tnms
  = [  case mbSvar of
         Nothing    -> IsKnown     tvar AbsBasic
         Just svar  -> IsSuperset  tvar svar
    |  (tvar,mbSvar) <- zip tnms snms
    ,  tvar /= wildcard
    ]
\end{code}
In the case of an |UpdateUnit| expression there is one
more constraint, setting the destination variable of the
update equal to that of the source variable.
In the semantic rules, AG keyword |loc| is used to 
define a local attribute common to |Unit| and |UpdateUnit|.

The situation arising from an alternative |Alt| in a |Case| expression
is very much like the third subcase of a |Unit| expression:
the fields of the target node (which come from the pattern in
each alternative) are projections of the value of the
scrutinee, that we so carefully passed down in
the tree before.

We now turn to the three variants of |Fetch| expressions.
When a complete node is fetched, the target variable should be
equal to the value fetched.
For a |FetchNode| the target is the inherited target
(i.e., the next |Seq| pattern or result of a function |Bind|ing),
for a |FetchUpdate| the target is specified in the expression.
In case of a |FetchField| of a single field, that field should
be a projection from the source.

The next semantic rule, still in figure~\ref{fig.equations},
states that for a |Store| expression we need a new uniquely
numbered location.
A heap equation is generated that states that this location indeed
stores the value, and a normal equation is generated
that states that the target variable is a pointer to this location.

The situation for a |Global| variable definition is quite the same,
which is why we define these situations adjacently in
figure~\ref{fig.equations} (the AG preprocessor allows to handle
the cases |Expr| non-contiguously, which we
happily use here to group similar rules).

In the case of a |Call| to a Grin function or an |FFI| call
to a foreign function we distinguish the cases 
that the target is a variable or a complete node.
The final two cases in figure~\ref{fig.equations}
state that |Eval| and |Apply| expressions give rise to
corresponding constraints.

What is not handled in the cases discussed above,
is that actual parameters should agree to formal parameters.
The |Call| expression handled in figure~\ref{fig.equations} only
matched the result, not the arguments.
Function calls can either occur directly in a |Call| expression,
or be postponed by way of a thunk node.
Thunk nodes are recognizible by their tag, which is either
|Fun|, |PApp| or |App| (but not |Con| or one of the
other special tags).


We define a tree walk that collects the relevant calls and
tagged nodes. Conceptually this is a separate tree walk,
but it is merged by the AG preprocessor with the tree walk
defined earlier.
We declare synthesized attributes to collect |allCalls| and
|fpaNodes| (nodes with tags that indicate a thunk)
for nearly all syntactic positions,
because nodes are introduced at many places,
and the collections need to be passed up the tree:
\begin{code}
ATTR AllTerm AllExpr AllDef GrModule 
  SYN allCalls  USE (++) [] :: [(Variable, [Maybe Variable])]
  SYN fpaNodes  USE (++) [] :: [NodeInfo (Maybe Variable)]

\end{code}
Thanks to the |USE| clause, we only need to specify the
locations where calls and nodes are actually introduced:
\begin{code}  
SEM GrExpr  | Call    
  lhs.allCalls  =  [ (getNr @nm, @argL.vars) ]
SEM GrTerm  |  Node   
  lhs.fpaNodes  =  if    @tag.isfpa 
                   then  [ InNode @tag @fldL.vars ] 
                   else  []
\end{code}
An auxiliary attribute decides which nodes are relevant
to collect:
\begin{code}
ATTR GrTag SYN isfpa :: Bool
SEM GrTag
  | Fun PApp App      lhs.isfpa  = True
  | Con Hole Unboxed  lhs.isfpa  = False
\end{code}
Now the final set of equations is the combination of
constraints that were gathered in the tree walk
(that is, the synthesized |equations| from the entire module |mod|),
and those that arise from direct calls, |Fun|, |PApp| and |App| thunk nodes:
\begin{code}
SEM Program  |  Prog
  lhs.equations 
   = @mod.equations
     ++
     [  IsSuperset x y
     |  (funnr, args) <- @mod.allCalls
     ,  (x, Just y) <- zip [funnr + 1 ..] args
     ]
     ++
     [  IsSuperset x y
     |  (InNode (GrTag_Fun nm) args) 
            <- @mod.fpaNodes
     ,  (x, Just y) <- zip [getNr nm + 1 ..] args
     ]
     ++
     [  IsSuperset x y
     |  (InNode (GrTag_PApp needs nm) args) 
            <- @mod.fpaNodes
     ,  (x, Just y) <- zip [getNr nm + 1 ..] args
     ]
     ++ 
     [  IsApplication Nothing (map fromJust args)
     |  (InNode GrTag_App args) 
            <- @mod.fpaNodes
     ]
\end{code}
Note that we exploit the fact that the function and its arguments
are numbered consecutively: the arguments are numbered from
one more than the function number onwards.
Without this convention, the correspondence between the
number of a function and those of its parameters
could have been established as a mapping 
that could have been defined as yet another synthesized
attribute of bindings.

The trickiest equations are generated in the fifth concatenated list:
it states that the arguments of an |App| node represent an 
application, although it is not statically known where the
result is stored.







\subsection{Solving the constraint equations}\label{sec.solution}

Now we've collected all equations,
we can proceed to solve them.
The solution process is wrapped in function |solveEquations|.
It takes the two lists of equations that were collected in the tree walk,
and two integers: the number of |Variable|s and |Location|s.
These were determined in an earlier stage where
variables are numbered (trivial, not shown in this paper),
and as synthesized attribute |location| in the tree walk.
\begin{figure*}
\begin{code}
envChanges :: Equation -> AbstractEnv s -> AbstractHeap s -> ApplyMap -> ST s [(Variable,AbstractValue)]
envChanges equat env heap applyMap
  = case equat of
      IsKnown         d av         ->  return [(d, av)]

      IsSuperset      d v          ->  do  {  av <- readArray env v
                                           ;  return [(d, av)]
                                           }
      IsSelection     d v i t      ->  do  {  av <- readArray env v
                                           ;  let res = absSelect av i t
                                           ;  return [(d,res)]
                                           }
      IsConstruction  d t as       ->  do  {  vars <- mapM (maybe (return AbsBasic) (readArray env)) as
                                           ;  let res = AbsNodes (Map.singleton t vars)
                                           ;  return [(d,res)]
                                           }
      IsEvaluation    d v          ->  do  {  av   <- readArray env v
                                           ;  res  <- absDeref av
                                           ;  return [(d,res)]
                                           }
      IsApplication mbd (f:as)     ->  do  {  av         <-  readArray env f
                                           ;  absFun     <-  case mbd of
                                                               Nothing  -> absDeref av
                                                               Just _   -> return av
                                           ;  absArgs    <-  mapM (readArray env) as
                                           ;  (sfx,res)  <-  absCall absFun absArgs
                                           ;  return $ (maybe id (\d->((d,res):)) mbd) sfx
                                           }
\end{code}
\begin{code}
    where
    absSelect av i t   =  case av of
                            AbsNodes  ns  -> maybe AbsBottom (!!i) (Map.lookup t ns)
                            AbsBottom     -> av
                            AbsError _    -> av
    absDeref av        =  case av of
                            AbsLocs ls    ->  do  { vs <- mapM (readArray heap) (Set.toList ls)
                                                  ; return (mconcat (map (filterNodes isFinalTag) vs))
                                                  }
                            AbsBottom     ->  return av
                            AbsError _    ->  return av
    absCall f args     =  do {  ts <- mapM addArgs (getNodes (filterNodes isPAppTag f))
                             ;  let (sfxs,avs) = unzip ts
                             ;  return (concat sfxs, mconcat avs)
      	                     }
      where  addArgs (tag@(GrTag_PApp needs nm) , oldArgs) 
               =  do  {  let  n         = length args
                              newtag    = GrTag_PApp (needs-n) nm
                              funnr     = getNr nm
                              sfx       = zip  [funnr+1+length oldArgs ..] args
                      ;  res <-  if    n<needs
                                 then  return $ AbsNodes (Map.singleton newtag (oldArgs++args))
                                 else  readArray env funnr
                      ;  return (sfx, res)
                      }
             getNodes av  =  case av of  
                               AbsNodes n  -> Map.toAscList n
                               AbsBottom   -> []
\end{code}
\caption{Selection of change candidates for the abstract environment during fixpoint iteration (discussed in section~\ref{sec.solution})}
\label{fig.envChanges}
\end{figure*}

\begin{code}
solveEquations ::  Int -> Int 
                   -> Equations -> HeapEquations 
                   -> (AbsEnv,AbsHeap,Int)
\end{code}
The |solveQuations| function starts with creating two arrays,
initially holding only |AbsBottom| values, to store the
abstract values of all variables and locations, respectively.
Then a fixpoint iteration is done, processing in each step
all constraints from both sets of equations.
The fixpoint function is parameterized not only by the two
sets of equations, but also by two procedures that process
an equation.
These procedures call function |envChanges| or |heapChange|
respectively, to obtain the changes on the variables or locations
that need to be made.
In the processing procedures, the change candidates obtained
(exactly one in the case of an |heapEquation|, 
possibly more in the case of an |Equation|)
are fed into function |procChange| to apply the change.
\begin{code}                   
solveEquations lenEnv lenHeap eqs1 eqs2
=  runST $
   do  { env   <- newArray (0, lenEnv   - 1) AbsBottom
       ; heap  <- newArray (0, lenHeap  - 1) AbsBottom
       ; let procEnv equat
             = do  { cs  <- envChanges equat env heap
                   ; bs  <- mapM (procChange env) cs
                   ; return (or bs)
                   }
             procHeap equat
             = do  { cs  <- heapChange equat env
                   ; b   <- procChange heap cs
                   ; return b
                   }
       ; count <- fixpoint eqs1 eqs2 procEnv procHeap
       ; return (env, heap, count)
       }
\end{code}
Function |procChange| can be generically used for either an environment variable
or a heap location.
This function only changes the array (environment or heap)
when an element (variable or location) is actually changed,
and returns a boolean that indicates whether there was a change:
\begin{code}
procChange arr (i,e1) =
   do { e0 <- readArray arr i
      ; let   e2       =  e0 `mappend` e1
              changed  =  e0 /= e2
      ; when changed (writeArray arr i e2)
      ; return changed
      }
\end{code}
The fixpoint function uses these booleans to decide whether to
stop or continue processing all equations again:
as long as one of the equations results in a change, the
iteration is continued.
\begin{code}
fixpoint eqs1 eqs2 proc1 proc2 
=  fix 0
   where  fix count 
          =  do
             {  let step1  b i  = proc1  i >>= return . (b||)
             ;  let step2  b i  = proc2  i >>= return . (b||)
             ;  changes1  <- foldM step1  False eqs1
             ;  changes2  <- foldM step2  False eqs2
             ;  if    changes1 || changes2
                then  fix (count+1)
                else  return count
             }
\end{code}
What remains to be done is to describe how change candidates
are selected for each equation.
This is implemented in function |heapChange| below and
function |envChanges| in figure~\ref{fig.envChanges}.



We start with the changes for heap locations.
Function |heapChange| dissects an |HeapEquation|,
that states that at some location a node with given tag
and argument variables is stored.
If the node is a function thunk, i.e.\ the tag is |GrTag_Fun|,
the location can later be updated with the function result.
Possible nodes that this location can point to are thus
all function results for this function.
We therefore consult the environment to obtain the `abstract result'
for this function.
Regardless of the value of the tag, the location mentioned in the equation
certainly points initially to the node that is constructed.
An `abstract node' is therefore constructed by creating a singleton
map from the node to the abstractly evaluated arguments.
\begin{code}
heapChange ::  HeapEquation -> AbstractEnv s 
               -> ST s (Location,AbstractValue)
heapChange (WillStore locat tag args) env 
 = do  { let mbres       =   tagFun tag
       ; absArgs         <-  mapM getEnv args
       ; absRes          <-  getEnv mbres
       ; let absNode     =   AbsNodes 
                               (Map.singleton tag absArgs)
       ; return (locat, absNode `mappend` absRes)
       }
       where
       tagFun (GrTag_Fun nm)  =  Just (getNr nm)
       tagFun _               =  Nothing
       getEnv Nothing         =  return AbsBottom
       getEnv (Just v)        =  readArray env v
\end{code}
The changes of the abstract variables that arise from
processing an |Equation| are determined by function |envChanges|
in figure~\ref{fig.envChanges}, which we will now discuss.
First, note that this function returns a list of changes,
unlike function |heapChange| above, which returns only a single change.
For five out of six possible equation types this list 
is a singleton, however.
Only for the |IsApplication| case of an equation, multiple changes
may arise from one equation.

For the first equation type |IsKnown|, where a variable is known 
to (possibly) have some abstract value, 
the variable is simply paired with that abstract value to indicate
a necessary change.
For the second equation type |IsSuperset d v|, 
the current approximation of |v| is looked up in the abstract environment,
and designated as a needed change for |d| as well.
For an |IsSelection| equation, the variable |v| is abstractly evaluated
to obtain an abstract node. From that abstract node the desired field
is abstractly selected.
The case of an |IsConstruction| equation is similar to 
the |WillStore| heap equation discussed above, in that 
an abstract node is created from the known tag and the abstractly 
evaluated argument variables.

The fifth equation type is |IsEvaluation d v|, 
which states that |d| may hold the evaluation result of 
thunk nodes pointed to by |v|.
Here, we first abstractly evaluate |v| to obtain the abstract pointers.
These pointers are then abstractly dereferenced, 
that is looked up in the abstract heap.
This results in all abstract nodes the locations can point to.
By the design of the processing of heap equations, 
this is not only the thunk node, but also the possible
evaluation results of it.
As the |IsEvaluation| equation is supposed to obtain the evaluation
results only, the list of all abstract nodes the locations can point
to is filtered such that only those with a final tag (like |GrTag_Con|)
remain, and those with thunk tag (like |GrTag_Fun|) are discarded.
The filtering is done by an auxiliary function:
\begin{code}
filterNodes ::  (GrTag->Bool) 
                -> AbstractValue -> AbstractValue
filterNodes p (AbsNodes nodes) 
  = AbsNodes (Map.filterWithKey (const . p) nodes)
filterNodes p av
  = av
isFinalTag, isPAppTag :: GrTag -> Bool
isFinalTag  (GrTag_Fun _)      = False
isFinalTag   GrTag_App         = False
isFinalTag  _                  = True
isPAppTag   (GrTag_PApp _ _)   = True
isPAppTag   _                  = False
\end{code}
The last equation type in figure~\ref{fig.envChanges}, |IsApplication|, is the trickiest.
It was introduced in section~\ref{sec.collect} in two situations:
(1) at every |App| expression in the Grin program, 
where the |Maybe Variable| destination is |Just| a variable name,
and 
(2) at every constructed node in the Grin program with |App| tag,
where the destination is |Nothing|.
Remember from section~\ref{sec.constraintlang} 
that |IsApplication mbv (f:as)| means that |f| is a variable
which refers to a function which is applied to
values referred to by variables |as|
(and the result may be stored in variable |v| if |mbv| is |Just v|).

The first thing that needs to be done is therefore to
evaluate |f| and |as| abstractly.
If the equation was introduced from situation (2),
the function variable also needs to be dereferenced abstractly.
This gives us an abstract function |absFun| and abstract arguments |absArgs|.
Function |absCall| now can abstractly apply the former to the latter.

Doing an abstract call amounts to filtering the partial-application thunk nodes
from the possible nodes that can represent the function, 
and adding the extra arguments by way of function |addArgs|.
If, after adding the new parameters, the function is still not fully saturated,
a new abstract node is constructed, having a |PApp| tag with lower |needs|
than the original one. 
If the function happens to be fully saturated, the possible results are read from the environment.
The resulting nodes (either the newly constructed, or those read)
is tupled with the destination variable to indicate a necessary change,
at least in situation (1) where such a variable exists.

But there are other changes that need to be taken into account as well.
In the abstract call, new associations are made between arguments and
formal parameters, that are not otherwise detected.
This is why the |absCall| and |addArgs| functions, in addition to the function result,
also return changes that take care of new possible abstract values for argument variables.
It is because of these `side effects' (designated |sfx| in figure~\ref{fig.envChanges})
that function |envChanges| sometimes returns more than one change.






\section{Discussion and future/related work}


Our algorithm determines, through static analysis,
for each variable the possible contents it can have at run time.
In particular, this reveals the possible functions a closure can
represent.
This information can be used to replace the indirect jump
in a closure evaluation by a small case analysis.
It is vital that the back end which translates the case distinction to machine code
does not use a jump table to implement it, as that
would waste all our efforts to avoid indirect jumps.
Instead, the case analysis should be translated to 
repeated tests and conditional jumps.

Although branch prediction makes conditional jumps with
known target less expensive than indirect jumps, it is still
important to keep the number of jumps as low as possible.
By using a binary search, $2^n$ cases can be distinguished
with $n$ comparisons and jumps.
Another approach would be to test for the most probable case first.
If the occurence probabilities are known, we can organize the
binary search in a Huffman tree, minimizing the expected number of
comparisons.
Determining which cases occur most frequently is hard to detect 
statically, as this depends on the dynamic behaviour of a program.
A interesting optimization opportunity is to gather such information
during a test run, and to use the profiling information in
subsequent compilations.

Extensively used higher-order library functions, like |map|,
can lead to a large number of cases to be distinguished.
In these situations it can be worthwhile to compile multiple
copies of the function, each with a limited number of different callers.

Although our algorithm makes it possible to avoid indirect jumps
on closure evaluation, there is another cause of indirect jumps:
returning from a function.
An idea worth investigating is to try and prevent these as well
using control flow analysis: if the number of possible callers
is limited, instead of returning by indirectly jumping to the
adress popped from the stack, we can do a direct jump based 
on a case analysis of possible callers.


We end with a few remarks on the attribute grammar based methodology we used.
We think that describing a tree walk algorithm explicitly in terms
of inherited and synthesized attributes helps a lot in 
clarifying the structure of the algorithm.
As tree walks are abundant in compiler construction
(the algorithm described is just one of many in our compiler),
we think it is worthwhile to consistently use attribute grammar based tools.

A debate remains on whether this should be organized as a preprocessor allowing for new constructs,
as we used in this work,
or as a library offering a set of combinators manipulating attributes.
\cite{cata,moor}












\acks

The authors thank Christof Douma for
writing an initial version of the implementation
described in this work.



\begin{thebibliography}{}




\bibitem[Bird 1984]{bird}
Richard S.\ Bird.
Using circular programs to eliminate multiple traversals of data.
{\em Acta Informatica} {\bf 21}: 239--250.

\bibitem[Bird and de Moor 1996]{birdmoor}
R.\ Bird and O.\ de Moor.
{\em The algebra of programming}.
Prentice Hall, 1996.


\bibitem[Boquist and Johnsson 1996]{boquist1996}
Urban Boquist and Thomas Johnsson.
The GRIN project: A highly optimising back end for lazy functional languages.
In {\em Workshop on Implementation of Functional Languages} IFL 1996.
Springer LNCS 1268. 

\bibitem[Boquist 1999]{boquist1999}
Urban Boquist.
{\em Code optimisation techniques for lazy functional languages}.
PhD Thesis Chalmers University, G\"oteborg March 1999.

% \bibitem[Dijkstra 2005]{ehc}
% Atze Dijkstra.
% {\em Stepping through Haskell}.
% PhD Thesis Utrecht University, November 2005.

\bibitem[Douma 2006]{douma}
Christof Douma.
{\em Exceptional GRIN}.
% Master's Thesis Utrecht University, April 2006.
Master's Thesis Zzzzzzz University, April 2006.

\bibitem[Hughes 1982]{hughes}
R.J.M.\ Hughes.
Super-combinators, a new implementation method for applicative languages.
In {\em Proc.\ ACM conference on Lisp and functional programming} 1982.


\bibitem[Jones 1995]{cata}
Mark. P.\ Jones.
Functional programming with overloading and higher-order polymorphism.
In: {\em Advanced Functional Programming} AFP'95, pp.~97--136.
Springer LNCS 925.


\bibitem[Jones 1999]{thih}
Mark P.\ Jones.
Typing Haskell in Haskell.
In: {\em Haskell Workshop} 1999.


\bibitem[Knuth 1968]{knuth}
D.E.\ Knuth.
Semantics of context-free languages.
{\em Mathematical Systems Theory} {\bf 2}: 127--145.


\bibitem[Marlow and Peyton Jones 2006]{marlow}
Simon Marlow and Simon Peyton Jones.
Making a fast curry: push/enter vs.\ eval/apply for higher-order languages.
{\em J.\ Functional Progr.} {\bf 16}: 415--449.


\bibitem[de Moor et al. 1999]{moor}
Oege de Moor, Simon Peyton Jones and Eric van Wyk,
Aspect-oriented compilers.
In: {\em Generative and Component-Based Software Engineering} GCSE'99, pp.~121-133.
Springer LNCS 1799.


\bibitem[Peyton Jones 1992]{stgmachine}
Simon L.\ Peyton Jones.
Implementing lazy functional languages on stock hardware: the Spineless Tagless G-machine.
{\em J.~Functional Programming} {\bf 2}: 127--202.
 
\bibitem[Swierstra et al. 1998]{agsyst}
S.\ Doaitse Swierstra,
Pablo R.\ Azero Alocer,
and Jo\~ao Saraiva.
Designing and implementing combinator languages.
In: {\em Advanced functional programming} AFP'98.
Springer LNCS 1608.

\bibitem[Turner 1979]{turner}
D.A.\ Turner.
A new implementation technique for ap\-plicative languages.
{\em Softw.\ practice and experience} {\bf 9}: 31--49.

\bibitem[We 200x]{ehc}
(Details of reference to a description of our compiler left out for blind review)

\end{thebibliography}

\end{document}
