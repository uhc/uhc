\chapter{Conclusions and future work}

\section{Conclusions}
\label{conclusions}

On of the goals of this thesis was the implementation of a back-end for EHC
based on GRIN. This goal is met for a big subset of the EH language. The
language constructs which cannot be compiled yet are the |update| statement
outside the |eval| function in GRIN, which is merely a practical problem with
HPT, and support for (extensible) records in GRIN. Extensible records support
was not included in the thesis goals and is thus not implemented. Non
extensible records are not supported because EHC uses the same GRIN constructs
as with the extensible records, which makes them, from the GRIN perspective,
equal to extensible records.

The current implementation is still in the 'toy' phase: it does not generate
efficient code, because only a few optimizing transformations are implemented,
and an accurate garbage collector is absent, which makes big programs choke on
the amount of memory they need. However, the current implementation is a good
basis to create a full featured GRIN compiler.

The research goals of this thesis are the description of HPT and the extension
of GRIN with support for exceptions. We have described the working of HPT and
have shown how HPT can be adapted, without difficulty, to support exceptions in
GRIN. Furthermore, we have shown that with the added exception support, the
imprecise exceptions extension of Haskell can be expressed in GRIN.


\subsection{Implementation experience}

The complete implementation, except the solving of the equations of HPT, is
created with the AG system uuagc \citep{uuag}. The various transformations
could be expressed quickly in AG.

There is a downsite to the used AG system. The used AG system cannot expresses
references to other nodes in the tree, a feature which is found in reference AG
systems, like JastAdd \citep{jastadd}. Since we cannot reference other nodes,
we must collect the contextual information needed by the transformations
ourself. Contextual information might be absent, so we must pack the
information in a datatype which can represent the existence or absence of the
expected context, for example usening Haskell's \emph{Maybe} datatype.
Equations based on these attributes must be aware that the needed context is
absent, and, the more context is needed, the more unreadable those equations
become.

Another downside of the AG system is the constant rebuilding of the AST. Some
transformations work very local in the AST and subtrees of the AST are
guarenteed not to change. The current implementation of the AG system is not
aware of the unchanged subtrees and will simply deconstruct and rebuild the
whole AST, even if this result in an identity transformation.


\section{Future work}
\label{futurework}

To create a full featured EHC back-end based on GRIN, more research and
implementation work is necessary. This section describes possibilities for
further research and lists several remaining implementation issues.

\subsection{Thread support and asynchrone exceptions}

A challenging part of GRIN is the modelling of thread support and asynchrone
exceptions. Especially the question if locking and the communication between
threads can be analysed and optimized with GRIN.

\subsection{Separate compilation}

One problem of whole program compilation is the compile time; a small change in
a function means recompilation of the complete program. To prevent complete
recompilation of the program, the front-end can use separate compilation and
emit GRIN code for each Haskell module seperately. The GRIN compiler is invoked
with a set GRIN modules and joins those modules together to create the complete
GRIN program.

While this is better than complete recompilation, the GRIN compiler still
recompiles the whole program for each change. A valid question would be if the
GRIN compiler can cache some information to speed-up the recompilation process.

HPT is a good candidate for caching. While HPT gives only a valid estimation if
the whole program is available, it is possible to run HPT on a part of the
program. The result from such a run is not complete and cannot be used for
transformations, but it can be used as the start of HPT on a complete program,
saving some iterations needed to reach a fixpoint.

One step further would be to make the HPT analyse the distribution of unknown
values. The HPT can be modified to have a notion of pointers to external heap
locations, and unknown tags. Some help from the front-end is needed to analyse
the evaluation of an unknown closure. The type information availible by the
front-end is a good source to approximate the evaluation of unknown closures. 

HPT results based on a module are worse than an analysis on a complete program,
but some transformations, like the vectorisation transformation, can already be
applied to part of a GRIN module. Results of this modified HPT can also be used
as a starting point for HPT on a complete program, provided that all
information based on an external heap location can be removed from the result
of HPT on a module.


\subsection{GRIN vs STG-machine}

It would be interesting to do some study on the differences and similarities
between GRIN and the STG-machine \citep{jones92implementing} as used by GHC.

One difference, the tags in GRIN vs the tagless STG-machine, is not so big as
it might seem. Tags in GRIN are identifiers of nodes. Currently, we use an
integer as the runtime representation of a tags, but using an unique pointer as
as the representation of a tag is also possible. Such a tag can point to a
record containing pointers to code for evaluation, garbage collection, etc.
This comes very close to the tagless part of the STG-machine.

One possible use of pointers as tags would be to prevent very big eval case
statements. If HPT gives many possible nodes for an eval call, the case
statements which are created by inlining the eval calls are huge. When using
pointers as tags, we have the option to use the tag as a pointer and jump to
the code for evaluation instead of inserting a very big case statement, while
for small case statements a jump table can still be used, although it is a bit
more inefficient to calculate the jump offset based on pointers instead of
integers.


\subsection{Extensible records}

EHC supports extensible records as described by Simon Peyton Jones
\citep{jones99lightweight}. For this purpose EHC emits some special GRIN
constructs. This section gives a start on how these GRIN constructs can be
compiled. Throughout this section we use the construct |case a > b of ...| as a
shorthand for:

\begin{code}
ffi primGtInt a b;
\c -> case c of ...
\end{code}

Extensible records as described by Simon Peyton Jones have the property that
all possible sizes of the records are finite and known at compile time. Thus,
it is possible to give each record size an unqiue tag in GRIN. E.g., a record
with zero elements gets the name |#R0|, a record with one element |#R1|, and so
on.  At runtime these tags are represented by a continuous interval of
integers.  E.g., |#R0| is assigned the value |n|, |#R1| the value |n+1|, and so
on. If the size of a record is needed at runtime this can be calculate by
substracting the number assigned to tag |#R0| from the number assigned to the
tag representing the record.

To express a selection from a record in GRIN, EHC uses a special case
alternative:

\begin{code}
case r of
  (#R_ n|f=o) -> ...
\end{code}

This construct selects a field at offset |o| from record |r| and binds it to
|f|. The variable |n| represents a record of one size less than the record |r|,
e.g. |n| contains all the fields of |r| minus |f|. The tag |#R_| in the
alternative pattern indicates that the size of the record, and thus the tag of
the node, is not known.

Once the GRIN compiler starts, the whole program is available, and since all
record sizes are known at compile time all possible record tags are available
in the program. HPT analysis already collects which record tags, and thus which
record sizes, might be bound to a record selection statement. When the possible
tags are estimated, the record selection statement can be rewritten to a normal
case statement. The variable name of the selected field is found by a binary
search based on the offset. Offsets are basic (unboxed) integers. The offset 1
represents the offset of the first field:

\begin{code}
case r of
    { (#R2 x1 x2)        ->  case o > 1 of
                               #CTrue   ->  unit (#R1  x2  x1)
                               #CFalse  ->  unit (#R1  x2  x1)
    ; (#R4 y1 y2 y3 y4)  ->  case o > 2 of
                               #CTrue   ->  case o > 3 of
                                              #CTrue    ->  unit (#R3  y4  y1  y2  y3)
                                              #CFalse   ->  unit (#R3  y3  y1  y2  y4)
                               #CFalse  ->  case o > 1 of
                                              #CTrue    ->  unit (#R3  y2  y1  y3  y4)
                                              #CFalse   ->  unit (#R3  y1  y2  y3  y4)
    }; \(v0, f, v1, v2, v3) -> unit (v0, v1, v2, v3); \n ->
...
\end{code}

The first field of the node returned by the case statement is the selected
field. This makes the node one field bigger than it should be according to its
tag. But as long as we don't analyse the code again this is safe. Immediately
after the case statement we destruct the returned node. |f| is bound to the
first field and |n| is bound to the tag and other fields, which represent the
record without the selected field.

HPT can also be used to collect a set of possible offsets for each variable
holding a record offset. To discriminate offset constants from other constants,
the front-end must annotate the offset constants. If all offset constants are
known, HPT can find a set of integers for offset variable instead of ``\avBAS'',
which it currently does.

In the special case that only a single record size is found, and this record is
stored in memory we might perform much better by emitting a fetch statement
with the offset of the record field to select as fetch offset, e.g. |fetch p
o|, in which |p| is a pointer to record |r|. However, in most cases a thunk of
a function which returns a record is stored in the same memory location as the
record itself, which makes this situation hard to detect.

A unsolved problem however is how to estimate the result of |f|. This variable
can hold any value stored in the possible records. To get a good estimation,
the relation between the record sizes and the offsets must be taken into
account. E.g., HPT must analyse which tags and offsets combinations occur in
the selection statement.

%{
%format += = "+\!\!="
To extend records, EHC uses the statement |unit (r||o += f)|. This extends a
record |r| with field |f| at offset |o|. Each field with an offset greater than
or equal to |o| is shifted to the right. The record extension statement can be
rewritten in the same way as the record selection. After the possible record
tags are estimated by HPT, a record extension statement can be rewritten to a
normal case statement:

\begin{code}
case r of
    {  (#R1 x1)        ->  case o > 1 of
                             #CTrue   ->  unit (#R2  x1 f   )
                             #CFalse  ->  unit (#R3  f  x1  )
    ;  (#R3 y1 y2 y3)  ->  case o > 2 of
                             #CTrue   ->  case o > 3 of
                                            #CTrue    ->  unit (#R4  y1  y2  y3  f   )
                                            #CFalse   ->  unit (#R4  y1  y2  f   y3  )
                             #CFalse  ->  case o > 1 of
                                            #CTrue    ->  unit (#R4  y1  f   y2  y3  )
                                            #CFalse   ->  unit (#R4  f   y1  y2  y3  )
    }
\end{code}


As with the selection satement, the extension statement HPT makes a bad
estimation on the result of this statment. Again HPT must analyse which tags
and offsets can occur at the same time.
%}

\subsection{Update specialisation}

The update statement has currently two problems:

\begin{itemize}
\item the update statement cannot differ in the layout of the node in memory;
\item the update statement might write uninitialized data to memory.
\end{itemize}

The current implementation uses a single layout of nodes in memory. This layout
is shown in Figure \ref{c--:node-layout} on page \pageref{c--:node-layout}. A
problem with this layout is explained by the following example: a node with 4
fields is stored in memory. Following the current layout, this node is split
into a base part, which hold the tag, 3 fields, and a pointer to the extended
part, and an extended part, which holds the 4th field. A better layout would be
to store the 4th field also in the base part, but this is only possible if the
update, store, and fetch statements know that the node has 4 fields.

The second problem is shown in the following example: we define a function
|upto|, which returns either a node with 2 fields or a node without fields, and
a closure to this function. The code responsable for the evaluation of this
closure is as follows:

\begin{code}
fetch p; \(nt n1 n2) ->
case nt of
  {  #CCons  ->  unit (nt n1 n2)
  ;  #CNil   ->  unit (nt)
  ;  #Fupto  ->  upto n1 n2; \(t x1 x2) ->
                 update p (t x1 x2); \() ->
                 unit (t x1 x2)
  }
\end{code}

The update statement in this code always writes |x1| and |x2| to memory, while
these variables might be uninitialized. To prevent writing of unitinialized
variables to memory, the update statement must know the size of the node.

Boquist describes how fetch and update can be annotated with the size, or
actually the tag, of the stored node.\footnote{the store statement has always a
tag constant of the node it stores.} The annotation of the fetch statement is
already implemented by the current implementation (as part of the ``right hoist
fetch transformation''). To annotate, or specialize, the update statement,
Boquist uses a different |eval| implementation. An example of this version of
|eval|, extended with our exception support, is as follows:

\begin{code}
fetch p; \n ->
try {
  case n of
    {  (#CCons x1 x2)  ->  unit n
    ;  (#CNil)         ->  unit n
    ;  (#Fupto a1 a2)  ->  upto a1 a2
    ;  (#Fthrow e)     ->  throw e
    }
} catch(e) {
    update p (#Fthrow e); \() ->
    throw e
}; \r ->
  update p r; \() ->
  unit r
\end{code}

Compared to the version used in our implemenation (shown in Figure
\ref{eval_with_exceptions} on page \pageref{eval_with_exceptions}) this version
of eval is less efficient since it might update a node with itself. The update
statement which updates the thunk with its result is put after the try-catch
statement. This is possible because an update statement never throws an
exception\footnote{it might throw an out of memory exception, but that
exception is treated as asynchrone.} and the last update statement of eval is
never reached if the exception handler is executed.

This version is more suitable for specializing the update statement using
Boquists approach of right hoisting the update statement until the tag of the
node in the update statement is known. However, the HPT can also be used to
insert a case statement which will descirbes the tag:

\begin{code}
fetch p; \n ->
  case n of
    {  (#CCons x1 x2)  ->  unit n
    ;  (#CNil)         ->  unit n
    ;  (#Fupto a1 a2)  ->  {  try {
                                upto a1 a2 ;\r ->
                                case r of 
                                  {  (#CCons x1 x2)  ->  update #CCons p r
                                  ;  (#CNil)         ->  update #CNil p r
                                  } \() ->
                                unit r
                              }
                           }
    ;  (#Fthrow e)     ->  throw e
    }
\end{code}

This way, the unnecessary updates are avoided. Which version leads to the
fastest running time is an open question.


\subsection{Haskell's builtin monad support}

The IO and the ST monad in Haskell do not necessarily need special support in
GRIN. To keep the sequential composition of these monads in tact in the
presents of optimisations, a sequence number (e.g. an integer) can be used as
representation of the state of the monad. Before each site effect, encoded by
an |ffi| statement, a sequence number must first be increased.

With this trick, code moving transformations must still be aware not to break
the ordening of ffi statements and the code that updates the sequence number.
To make this tracking easier to implement, we can use a special \emph{marker}
value instead of sequence number and a \emph{touch} statement instead of the
increment of the sequence number. A touch statement takes a marker variable and
returns a changed marker value. Instead the hard to detect code block of the
increment of a sequence number, we have now a single touch statement. Any code
moving transformation must not break the ordening of ffi statements and touch
statements.

The marker value and the touch statements are only used to make the sequencing
of the IO and ST monad explicit in GRIN. When all code moving transformations
are finished, any reference to a marker value and the touch statements can be
removed from the GRIN program.


\subsection{To do list}
\label{todo}

We list here some tasks which must be implemented sooner or later.

\begin{itemize} 

\item define a correlation between the key and the value of the eval- and
applymap, such that those maps become obsolete;

\item add support for blackholing in |eval|;

\item HPT support for update statements anywhere in the code, or at least support
for self referencing values;

\item add sharing analysis to HPT. The current implementation has already parts
of HPT modified to support the sharing analysis;

\item use dependency information of the equations in HPT to reach a fixpoint
faster using a worklist algorithm;

\item implement more transformations. Good starters are: Unboxing, constants
propagation, late inlining;

\item allow experimentations of the ordening of (optimizing) transformations
from the command line;

\item allow ffi statements to return boxed values;

\item optimize tag number assignment;

\item implement accurate garbage collector.

\end{itemize}
